prometheus:
  enabled: false
  # Default values for prometheus.
  # This is a YAML-formatted file.
  # Declare variables to be passed into your templates.

  rbac:
    create: true

  podSecurityPolicy:
    enabled: false

  imagePullSecrets: []
  # - name: "image-pull-secret"

  ## Define serviceAccount names for components. Defaults to component's fully qualified name.
  ##
  serviceAccounts:
    server:
      create: true
      name: ""
      annotations: {}

      ## Opt out of automounting Kubernetes API credentials.
      ## It will be overriden by server.automountServiceAccountToken value, if set.
      # automountServiceAccountToken: false

  ## Additional labels to attach to all resources
  commonMetaLabels: {}

  ## Monitors ConfigMap changes and POSTs to a URL
  ## Ref: https://github.com/prometheus-operator/prometheus-operator/tree/main/cmd/prometheus-config-reloader
  ##
  configmapReload:
    ## URL for configmap-reload to use for reloads
    ##
    reloadUrl: ""

    ## env sets environment variables to pass to the container. Can be set as name/value pairs,
    ## read from secrets or configmaps.
    env: []
      # - name: SOMEVAR
      #   value: somevalue
      # - name: PASSWORD
      #   valueFrom:
      #     secretKeyRef:
      #       name: mysecret
      #       key: password
      #       optional: false

    prometheus:
      ## If false, the configmap-reload container will not be deployed
      ##
      enabled: true

      ## configmap-reload container name
      ##
      name: configmap-reload

      ## configmap-reload container image
      ##
      image:
        repository: quay.io/prometheus-operator/prometheus-config-reloader
        tag: v0.78.2
        # When digest is set to a non-empty value, images will be pulled by digest (regardless of tag value).
        digest: ""
        pullPolicy: IfNotPresent

      ## config-reloader's container port and port name for probes and metrics
      containerPort: 8080
      containerPortName: metrics

      ## Additional configmap-reload container arguments
      ## Set to null for argumentless flags
      ##
      extraArgs: {}

      ## Additional configmap-reload volume directories
      ##
      extraVolumeDirs: []

      ## Additional configmap-reload volume mounts
      ##
      extraVolumeMounts: []

      ## Additional configmap-reload mounts
      ##
      extraConfigmapMounts: []
        # - name: prometheus-alerts
        #   mountPath: /etc/alerts.d
        #   subPath: ""
        #   configMap: prometheus-alerts
        #   readOnly: true

      ## Security context to be added to configmap-reload container
      containerSecurityContext: {}

      ## Settings for Prometheus reloader's readiness, liveness and startup probes
      ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
      ##

      livenessProbe:
        httpGet:
          path: /healthz
          port: metrics
          scheme: HTTP
        periodSeconds: 10
        initialDelaySeconds: 2

      readinessProbe:
        httpGet:
          path: /healthz
          port: metrics
          scheme: HTTP
        periodSeconds: 10

      startupProbe:
        enabled: false
        httpGet:
          path: /healthz
          port: metrics
          scheme: HTTP
        periodSeconds: 10

      ## configmap-reload resource requests and limits
      ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
      ##
      resources: {}

  server:
    ## Prometheus server container name
    ##
    name: server

    ## Opt out of automounting Kubernetes API credentials.
    ## If set it will override serviceAccounts.server.automountServiceAccountToken value for ServiceAccount.
    # automountServiceAccountToken: false

    ## Use a ClusterRole (and ClusterRoleBinding)
    ## - If set to false - we define a RoleBinding in the defined namespaces ONLY
    ##
    ## NB: because we need a Role with nonResourceURL's ("/metrics") - you must get someone with Cluster-admin privileges to define this role for you, before running with this setting enabled.
    ##     This makes prometheus work - for users who do not have ClusterAdmin privs, but wants prometheus to operate on their own namespaces, instead of clusterwide.
    ##
    ## You MUST also set namespaces to the ones you have access to and want monitored by Prometheus.
    ##
    # useExistingClusterRoleName: nameofclusterrole

    ## If set it will override prometheus.server.fullname value for ClusterRole and ClusterRoleBinding
    ##
    clusterRoleNameOverride: ""

    # Enable only the release namespace for monitoring. By default all namespaces are monitored.
    # If releaseNamespace and namespaces are both set a merged list will be monitored.
    releaseNamespace: false

    ## namespaces to monitor (instead of monitoring all - clusterwide). Needed if you want to run without Cluster-admin privileges.
    # namespaces:
    #   - yournamespace

    # sidecarContainers - add more containers to prometheus server
    # Key/Value where Key is the sidecar `- name: <Key>`
    # Example:
    #   sidecarContainers:
    #      webserver:
    #        image: nginx
    # OR for adding OAuth authentication to Prometheus
    #   sidecarContainers:
    #     oauth-proxy:
    #       image: quay.io/oauth2-proxy/oauth2-proxy:v7.1.2
    #       args:
    #       - --upstream=http://127.0.0.1:9090
    #       - --http-address=0.0.0.0:8081
    #       - ...
    #       ports:
    #       - containerPort: 8081
    #         name: oauth-proxy
    #         protocol: TCP
    #       resources: {}
    sidecarContainers: {}

    # sidecarTemplateValues - context to be used in template for sidecarContainers
    # Example:
    #   sidecarTemplateValues: *your-custom-globals
    #   sidecarContainers:
    #     webserver: |-
    #       {{ include "webserver-container-template" . }}
    # Template for `webserver-container-template` might looks like this:
    #   image: "{{ .Values.server.sidecarTemplateValues.repository }}:{{ .Values.server.sidecarTemplateValues.tag }}"
    #   ...
    #
    sidecarTemplateValues: {}

    ## Prometheus server container image
    ##
    image:
      repository: quay.io/prometheus/prometheus
      # if not set appVersion field from Chart.yaml is used
      tag: ""
      # When digest is set to a non-empty value, images will be pulled by digest (regardless of tag value).
      digest: ""
      pullPolicy: IfNotPresent

    ## Prometheus server command
    ##
    command: []

    ## prometheus server priorityClassName
    ##
    priorityClassName: ""

    ## EnableServiceLinks indicates whether information about services should be injected
    ## into pod's environment variables, matching the syntax of Docker links.
    ## WARNING: the field is unsupported and will be skipped in K8s prior to v1.13.0.
    ##
    enableServiceLinks: true

    ## The URL prefix at which the container can be accessed. Useful in the case the '-web.external-url' includes a slug
    ## so that the various internal URLs are still able to access as they are in the default case.
    ## (Optional)
    prefixURL: ""

    ## External URL which can access prometheus
    ## Maybe same with Ingress host name
    baseURL: ""

    ## Additional server container environment variables
    ##
    ## You specify this manually like you would a raw deployment manifest.
    ## This means you can bind in environment variables from secrets.
    ##
    ## e.g. static environment variable:
    ##  - name: DEMO_GREETING
    ##    value: "Hello from the environment"
    ##
    ## e.g. secret environment variable:
    ## - name: USERNAME
    ##   valueFrom:
    ##     secretKeyRef:
    ##       name: mysecret
    ##       key: username
    env: []

    # List of flags to override default parameters, e.g:
    # - --enable-feature=agent
    # - --storage.agent.retention.max-time=30m
    # - --config.file=/etc/config/prometheus.yml
    defaultFlagsOverride: []

    extraFlags:
      - web.enable-lifecycle
      ## web.enable-admin-api flag controls access to the administrative HTTP API which includes functionality such as
      ## deleting time series. This is disabled by default.
      # - web.enable-admin-api
      ##
      ## storage.tsdb.no-lockfile flag controls BD locking
      # - storage.tsdb.no-lockfile
      ##
      ## storage.tsdb.wal-compression flag enables compression of the write-ahead log (WAL)
      # - storage.tsdb.wal-compression

    ## Path to a configuration file on prometheus server container FS
    configPath: /etc/config/prometheus.yml

    ### The data directory used by prometheus to set --storage.tsdb.path
    ### When empty server.persistentVolume.mountPath is used instead
    storagePath: ""

    global:
      ## How frequently to scrape targets by default
      ##
      scrape_interval: 1m
      ## How long until a scrape request times out
      ##
      scrape_timeout: 10s
      ## How frequently to evaluate rules
      ##
      evaluation_interval: 1m
    ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#remote_write
    ##
    remoteWrite: []
    ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#remote_read
    ##
    remoteRead: []

    ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#tsdb
    ##
    tsdb: {}
      # out_of_order_time_window: 0s

    ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#exemplars
    ## Must be enabled via --enable-feature=exemplar-storage
    ##
    exemplars: {}
      # max_exemplars: 100000

    ## Custom HTTP headers for Liveness/Readiness/Startup Probe
    ##
    ## Useful for providing HTTP Basic Auth to healthchecks
    probeHeaders: []
      # - name: "Authorization"
      #   value: "Bearer ABCDEabcde12345"

    ## Additional Prometheus server container arguments
    ## Set to null for argumentless flags
    ##
    extraArgs: {}
      # web.enable-remote-write-receiver: null

    ## Additional InitContainers to initialize the pod
    ##
    extraInitContainers: []

    ## Additional Prometheus server Volume mounts
    ##
    extraVolumeMounts: []

    ## Additional Prometheus server Volumes
    ##
    extraVolumes: []

    ## Additional Prometheus server hostPath mounts
    ##
    extraHostPathMounts: []
      # - name: certs-dir
      #   mountPath: /etc/kubernetes/certs
      #   subPath: ""
      #   hostPath: /etc/kubernetes/certs
      #   readOnly: true

    extraConfigmapMounts: []
      # - name: certs-configmap
      #   mountPath: /prometheus
      #   subPath: ""
      #   configMap: certs-configmap
      #   readOnly: true

    ## Additional Prometheus server Secret mounts
    # Defines additional mounts with secrets. Secrets must be manually created in the namespace.
    extraSecretMounts: []
      # - name: secret-files
      #   mountPath: /etc/secrets
      #   subPath: ""
      #   secretName: prom-secret-files
      #   readOnly: true

    ## ConfigMap override where fullname is {{.Release.Name}}-{{.Values.server.configMapOverrideName}}
    ## Defining configMapOverrideName will cause templates/server-configmap.yaml
    ## to NOT generate a ConfigMap resource
    ##
    configMapOverrideName: ""

    ## Extra labels for Prometheus server ConfigMap (ConfigMap that holds serverFiles)
    extraConfigmapLabels: {}

    ## Override the prometheus.server.fullname for all objects related to the Prometheus server
    fullnameOverride: ""

    ingress:
      ## If true, Prometheus server Ingress will be created
      ##
      enabled: false

      # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
      # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
      # ingressClassName: nginx

      ## Prometheus server Ingress annotations
      ##
      annotations: {}
      #   kubernetes.io/ingress.class: nginx
      #   kubernetes.io/tls-acme: 'true'

      ## Prometheus server Ingress additional labels
      ##
      extraLabels: {}

      ## Redirect ingress to an additional defined port on the service
      # servicePort: 8081

      ## Prometheus server Ingress hostnames with optional path (passed through tpl)
      ## Must be provided if Ingress is enabled
      ##
      hosts: []
      #   - prometheus.domain.com
      #   - domain.com/prometheus

      path: /

      # pathType is only for k8s >= 1.18
      pathType: Prefix

      ## Extra paths to prepend to every host configuration. This is useful when working with annotation based services. (passed through tpl)
      extraPaths: []
      # - path: /*
      #   backend:
      #     serviceName: ssl-redirect
      #     servicePort: use-annotation

      ## Prometheus server Ingress TLS configuration (hosts passed through tpl)
      ## Secrets must be manually created in the namespace
      ##
      tls: []
      #   - secretName: prometheus-server-tls
      #     hosts:
      #       - prometheus.domain.com

    ## Server Deployment Strategy type
    strategy:
      type: Recreate

    ## hostAliases allows adding entries to /etc/hosts inside the containers
    hostAliases: []
    #   - ip: "127.0.0.1"
    #     hostnames:
    #       - "example.com"

    ## Node tolerations for server scheduling to nodes with taints
    ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
    ##
    tolerations: []
      # - key: "key"
      #   operator: "Equal|Exists"
      #   value: "value"
      #   effect: "NoSchedule|PreferNoSchedule|NoExecute(1.6 only)"

    ## Node labels for Prometheus server pod assignment
    ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
    ##
    nodeSelector: {}

    ## Pod affinity
    ##
    affinity: {}

    ## Pod anti-affinity can prevent the scheduler from placing Prometheus server replicas on the same node.
    ## The value "soft" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided.
    ## The value "hard" means that the scheduler is *required* to not schedule two replica pods onto the same node.
    ## The default value "" will disable pod anti-affinity so that no anti-affinity rules will be configured (unless set in `server.affinity`).
    ##
    podAntiAffinity: ""

    ## If anti-affinity is enabled sets the topologyKey to use for anti-affinity.
    ## This can be changed to, for example, failure-domain.beta.kubernetes.io/zone
    ##
    podAntiAffinityTopologyKey: kubernetes.io/hostname

    ## Pod topology spread constraints
    ## ref. https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/
    topologySpreadConstraints: []

    ## PodDisruptionBudget settings
    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/
    ##
    podDisruptionBudget:
      enabled: false
      # maxUnavailable: 1
      # minAvailable: 1
      ## unhealthyPodEvictionPolicy is available since 1.27.0 (beta)
      ## https://kubernetes.io/docs/tasks/run-application/configure-pdb/#unhealthy-pod-eviction-policy
      # unhealthyPodEvictionPolicy: IfHealthyBudget

    ## Use an alternate scheduler, e.g. "stork".
    ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
    ##
    # schedulerName:

    persistentVolume:
      ## If true, Prometheus server will create/use a Persistent Volume Claim
      ## If false, use emptyDir
      ##
      enabled: true

      ## If set it will override the name of the created persistent volume claim
      ## generated by the stateful set.
      ##
      statefulSetNameOverride: ""

      ## Prometheus server data Persistent Volume access modes
      ## Must match those of existing PV or dynamic provisioner
      ## Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
      ##
      accessModes:
        - ReadWriteOnce

      ## Prometheus server data Persistent Volume labels
      ##
      labels: {}

      ## Prometheus server data Persistent Volume annotations
      ##
      annotations: {}

      ## Prometheus server data Persistent Volume existing claim name
      ## Requires server.persistentVolume.enabled: true
      ## If defined, PVC must be created manually before volume will be bound
      existingClaim: ""

      ## Prometheus server data Persistent Volume mount root path
      ##
      mountPath: /data

      ## Prometheus server data Persistent Volume size
      ##
      size: 8Gi

      ## Prometheus server data Persistent Volume Storage Class
      ## If defined, storageClassName: <storageClass>
      ## If set to "-", storageClassName: "", which disables dynamic provisioning
      ## If undefined (the default) or set to null, no storageClassName spec is
      ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
      ##   GKE, AWS & OpenStack)
      ##
      # storageClass: "-"

      ## Prometheus server data Persistent Volume Binding Mode
      ## If defined, volumeBindingMode: <volumeBindingMode>
      ## If undefined (the default) or set to null, no volumeBindingMode spec is
      ##   set, choosing the default mode.
      ##
      # volumeBindingMode: ""

      ## Subdirectory of Prometheus server data Persistent Volume to mount
      ## Useful if the volume's root directory is not empty
      ##
      subPath: ""

      ## Persistent Volume Claim Selector
      ## Useful if Persistent Volumes have been provisioned in advance
      ## Ref: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#selector
      ##
      # selector:
      #  matchLabels:
      #    release: "stable"
      #  matchExpressions:
      #    - { key: environment, operator: In, values: [ dev ] }

      ## Persistent Volume Name
      ## Useful if Persistent Volumes have been provisioned in advance and you want to use a specific one
      ##
      # volumeName: ""

    emptyDir:
      ## Prometheus server emptyDir volume size limit
      ##
      sizeLimit: ""

    ## Annotations to be added to Prometheus server pods
    ##
    podAnnotations: {}
      # iam.amazonaws.com/role: prometheus

    ## Labels to be added to Prometheus server pods
    ##
    podLabels: {}

    ## Prometheus AlertManager configuration
    ##
    alertmanagers: []

    ## Specify if a Pod Security Policy for node-exporter must be created
    ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/
    ##
    podSecurityPolicy:
      annotations: {}
        ## Specify pod annotations
        ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#apparmor
        ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp
        ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#sysctl
        ##
        # seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'
        # seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default'
        # apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default'

    ## Use a StatefulSet if replicaCount needs to be greater than 1 (see below)
    ##
    replicaCount: 1

    ## Number of old history to retain to allow rollback
    ## Default Kubernetes value is set to 10
    ##
    revisionHistoryLimit: 10

    ## Annotations to be added to ConfigMap
    ##
    configMapAnnotations: {}

    ## Annotations to be added to deployment
    ##
    deploymentAnnotations: {}

    statefulSet:
      ## If true, use a statefulset instead of a deployment for pod management.
      ## This allows to scale replicas to more than 1 pod
      ##
      enabled: false

      annotations: {}
      labels: {}
      podManagementPolicy: OrderedReady

      ## Alertmanager headless service to use for the statefulset
      ##
      headless:
        annotations: {}
        labels: {}
        servicePort: 80
        ## Enable gRPC port on service to allow auto discovery with thanos-querier
        gRPC:
          enabled: false
          servicePort: 10901
          # nodePort: 10901

      ## Statefulset's persistent volume claim retention policy
      ## pvcDeleteOnStsDelete and pvcDeleteOnStsScale determine whether
      ## statefulset's PVCs are deleted (true) or retained (false) on scaling down
      ## and deleting statefulset, respectively. Requires 1.27.0+.
      ## Ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#persistentvolumeclaim-retention
      ##
      pvcDeleteOnStsDelete: false
      pvcDeleteOnStsScale: false

    ## Prometheus server readiness and liveness probe initial delay and timeout
    ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
    ##
    tcpSocketProbeEnabled: false
    probeScheme: HTTP
    readinessProbeInitialDelay: 30
    readinessProbePeriodSeconds: 5
    readinessProbeTimeout: 4
    readinessProbeFailureThreshold: 3
    readinessProbeSuccessThreshold: 1
    livenessProbeInitialDelay: 30
    livenessProbePeriodSeconds: 15
    livenessProbeTimeout: 10
    livenessProbeFailureThreshold: 3
    livenessProbeSuccessThreshold: 1
    startupProbe:
      enabled: false
      periodSeconds: 5
      failureThreshold: 30
      timeoutSeconds: 10

    ## Prometheus server resource requests and limits
    ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
    ##
    resources: {}
      # limits:
      #   cpu: 500m
      #   memory: 512Mi
      # requests:
      #   cpu: 500m
      #   memory: 512Mi

    # Required for use in managed kubernetes clusters (such as AWS EKS) with custom CNI (such as calico),
    # because control-plane managed by AWS cannot communicate with pods' IP CIDR and admission webhooks are not working
    ##
    hostNetwork: false

    # When hostNetwork is enabled, this will set to ClusterFirstWithHostNet automatically
    dnsPolicy: ClusterFirst

    # Use hostPort
    # hostPort: 9090

    # Use portName
    portName: ""

    ## Vertical Pod Autoscaler config
    ## Ref: https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler
    verticalAutoscaler:
      ## If true a VPA object will be created for the controller (either StatefulSet or Deployemnt, based on above configs)
      enabled: false
      # updateMode: "Auto"
      # containerPolicies:
      # - containerName: 'prometheus-server'

    # Custom DNS configuration to be added to prometheus server pods
    dnsConfig: {}
      # nameservers:
      #   - 1.2.3.4
      # searches:
      #   - ns1.svc.cluster-domain.example
      #   - my.dns.search.suffix
      # options:
      #   - name: ndots
      #     value: "2"
    #   - name: edns0

    ## Security context to be added to server pods
    ##
    securityContext:
      runAsUser: 65534
      runAsNonRoot: true
      runAsGroup: 65534
      fsGroup: 65534

    ## Security context to be added to server container
    ##
    containerSecurityContext: {}

    service:
      ## If false, no Service will be created for the Prometheus server
      ##
      enabled: true

      annotations: {}
      labels: {}
      clusterIP: ""

      ## List of IP addresses at which the Prometheus server service is available
      ## Ref: https://kubernetes.io/docs/concepts/services-networking/service/#external-ips
      ##
      externalIPs: []

      loadBalancerIP: ""
      loadBalancerSourceRanges: []
      servicePort: 80
      sessionAffinity: None
      type: ClusterIP

      ## Enable gRPC port on service to allow auto discovery with thanos-querier
      gRPC:
        enabled: false
        servicePort: 10901
        # nodePort: 10901

      ## If using a statefulSet (statefulSet.enabled=true), configure the
      ## service to connect to a specific replica to have a consistent view
      ## of the data.
      statefulsetReplica:
        enabled: false
        replica: 0

      ## Additional port to define in the Service
      additionalPorts: []
      # additionalPorts:
      # - name: authenticated
      #   port: 8081
      #   targetPort: 8081

    ## Prometheus server pod termination grace period
    ##
    terminationGracePeriodSeconds: 300

    ## Prometheus data retention period (default if not specified is 15 days)
    ##
    retention: "15d"

    ## Prometheus' data retention size. Supported units: B, KB, MB, GB, TB, PB, EB.
    ##
    retentionSize: ""

  ## Prometheus server ConfigMap entries for rule files (allow prometheus labels interpolation)
  ruleFiles: {}

  ## Prometheus server ConfigMap entries for scrape_config_files
  ## (allows scrape configs defined in additional files)
  ##
  scrapeConfigFiles: []

  ## Prometheus server ConfigMap entries
  ##
  serverFiles:
    ## Alerts configuration
    ## Ref: https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/
    alerting_rules.yml: {}
    # groups:
    #   - name: Instances
    #     rules:
    #       - alert: InstanceDown
    #         expr: up == 0
    #         for: 5m
    #         labels:
    #           severity: page
    #         annotations:
    #           description: '{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes.'
    #           summary: 'Instance {{ $labels.instance }} down'
    ## DEPRECATED DEFAULT VALUE, unless explicitly naming your files, please use alerting_rules.yml
    alerts: {}

    ## Records configuration
    ## Ref: https://prometheus.io/docs/prometheus/latest/configuration/recording_rules/
    recording_rules.yml: {}
    ## DEPRECATED DEFAULT VALUE, unless explicitly naming your files, please use recording_rules.yml
    rules: {}

    prometheus.yml:
      rule_files:
        - /etc/config/recording_rules.yml
        - /etc/config/alerting_rules.yml
      ## Below two files are DEPRECATED will be removed from this default values file
        - /etc/config/rules
        - /etc/config/alerts

      scrape_configs:
        - job_name: prometheus
          static_configs:
            - targets:
              - localhost:9090

        # A scrape configuration for running Prometheus on a Kubernetes cluster.
        # This uses separate scrape configs for cluster components (i.e. API server, node)
        # and services to allow each to use different authentication configs.
        #
        # Kubernetes labels will be added as Prometheus labels on metrics via the
        # `labelmap` relabeling action.

        # Scrape config for API servers.
        #
        # Kubernetes exposes API servers as endpoints to the default/kubernetes
        # service so this uses `endpoints` role and uses relabelling to only keep
        # the endpoints associated with the default/kubernetes service using the
        # default named port `https`. This works for single API server deployments as
        # well as HA API server deployments.
        - job_name: 'kubernetes-apiservers'

          kubernetes_sd_configs:
            - role: endpoints

          # Default to scraping over https. If required, just disable this or change to
          # `http`.
          scheme: https

          # This TLS & bearer token file config is used to connect to the actual scrape
          # endpoints for cluster components. This is separate to discovery auth
          # configuration because discovery & scraping are two separate concerns in
          # Prometheus. The discovery auth config is automatic if Prometheus runs inside
          # the cluster. Otherwise, more config options have to be provided within the
          # <kubernetes_sd_config>.
          tls_config:
            ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
            # If your node certificates are self-signed or use a different CA to the
            # master CA, then disable certificate verification below. Note that
            # certificate verification is an integral part of a secure infrastructure
            # so this should only be disabled in a controlled environment. You can
            # disable certificate verification by uncommenting the line below.
            #
            insecure_skip_verify: true
          bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

          # Keep only the default/kubernetes service endpoints for the https port. This
          # will add targets for each API server which Kubernetes adds an endpoint to
          # the default/kubernetes service.
          relabel_configs:
            - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
              action: keep
              regex: default;kubernetes;https

        - job_name: 'kubernetes-nodes'

          # Default to scraping over https. If required, just disable this or change to
          # `http`.
          scheme: https

          # This TLS & bearer token file config is used to connect to the actual scrape
          # endpoints for cluster components. This is separate to discovery auth
          # configuration because discovery & scraping are two separate concerns in
          # Prometheus. The discovery auth config is automatic if Prometheus runs inside
          # the cluster. Otherwise, more config options have to be provided within the
          # <kubernetes_sd_config>.
          tls_config:
            ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
            # If your node certificates are self-signed or use a different CA to the
            # master CA, then disable certificate verification below. Note that
            # certificate verification is an integral part of a secure infrastructure
            # so this should only be disabled in a controlled environment. You can
            # disable certificate verification by uncommenting the line below.
            #
            insecure_skip_verify: true
          bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

          kubernetes_sd_configs:
            - role: node

          relabel_configs:
            - action: labelmap
              regex: __meta_kubernetes_node_label_(.+)
            - target_label: __address__
              replacement: kubernetes.default.svc:443
            - source_labels: [__meta_kubernetes_node_name]
              regex: (.+)
              target_label: __metrics_path__
              replacement: /api/v1/nodes/$1/proxy/metrics


        - job_name: 'kubernetes-nodes-cadvisor'

          # Default to scraping over https. If required, just disable this or change to
          # `http`.
          scheme: https

          # This TLS & bearer token file config is used to connect to the actual scrape
          # endpoints for cluster components. This is separate to discovery auth
          # configuration because discovery & scraping are two separate concerns in
          # Prometheus. The discovery auth config is automatic if Prometheus runs inside
          # the cluster. Otherwise, more config options have to be provided within the
          # <kubernetes_sd_config>.
          tls_config:
            ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
            # If your node certificates are self-signed or use a different CA to the
            # master CA, then disable certificate verification below. Note that
            # certificate verification is an integral part of a secure infrastructure
            # so this should only be disabled in a controlled environment. You can
            # disable certificate verification by uncommenting the line below.
            #
            insecure_skip_verify: true
          bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

          kubernetes_sd_configs:
            - role: node

          # This configuration will work only on kubelet 1.7.3+
          # As the scrape endpoints for cAdvisor have changed
          # if you are using older version you need to change the replacement to
          # replacement: /api/v1/nodes/$1:4194/proxy/metrics
          # more info here https://github.com/coreos/prometheus-operator/issues/633
          relabel_configs:
            - action: labelmap
              regex: __meta_kubernetes_node_label_(.+)
            - target_label: __address__
              replacement: kubernetes.default.svc:443
            - source_labels: [__meta_kubernetes_node_name]
              regex: (.+)
              target_label: __metrics_path__
              replacement: /api/v1/nodes/$1/proxy/metrics/cadvisor

          # Metric relabel configs to apply to samples before ingestion.
          # [Metric Relabeling](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#metric_relabel_configs)
          # metric_relabel_configs:
          # - action: labeldrop
          #   regex: (kubernetes_io_hostname|failure_domain_beta_kubernetes_io_region|beta_kubernetes_io_os|beta_kubernetes_io_arch|beta_kubernetes_io_instance_type|failure_domain_beta_kubernetes_io_zone)

        # Scrape config for service endpoints.
        #
        # The relabeling allows the actual service scrape endpoint to be configured
        # via the following annotations:
        #
        # * `prometheus.io/scrape`: Only scrape services that have a value of
        # `true`, except if `prometheus.io/scrape-slow` is set to `true` as well.
        # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
        # to set this to `https` & most likely set the `tls_config` of the scrape config.
        # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
        # * `prometheus.io/port`: If the metrics are exposed on a different port to the
        # service then set this appropriately.
        # * `prometheus.io/param_<parameter>`: If the metrics endpoint uses parameters
        # then you can set any parameter
        - job_name: 'kubernetes-service-endpoints'
          honor_labels: true

          kubernetes_sd_configs:
            - role: endpoints

          relabel_configs:
            - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
              action: keep
              regex: true
            - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape_slow]
              action: drop
              regex: true
            - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
              action: replace
              target_label: __scheme__
              regex: (https?)
            - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
              action: replace
              target_label: __metrics_path__
              regex: (.+)
            - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
              action: replace
              target_label: __address__
              regex: (.+?)(?::\d+)?;(\d+)
              replacement: $1:$2
            - action: labelmap
              regex: __meta_kubernetes_service_annotation_prometheus_io_param_(.+)
              replacement: __param_$1
            - action: labelmap
              regex: __meta_kubernetes_service_label_(.+)
            - source_labels: [__meta_kubernetes_namespace]
              action: replace
              target_label: namespace
            - source_labels: [__meta_kubernetes_service_name]
              action: replace
              target_label: service
            - source_labels: [__meta_kubernetes_pod_node_name]
              action: replace
              target_label: node

        # Scrape config for slow service endpoints; same as above, but with a larger
        # timeout and a larger interval
        #
        # The relabeling allows the actual service scrape endpoint to be configured
        # via the following annotations:
        #
        # * `prometheus.io/scrape-slow`: Only scrape services that have a value of `true`
        # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
        # to set this to `https` & most likely set the `tls_config` of the scrape config.
        # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
        # * `prometheus.io/port`: If the metrics are exposed on a different port to the
        # service then set this appropriately.
        # * `prometheus.io/param_<parameter>`: If the metrics endpoint uses parameters
        # then you can set any parameter
        - job_name: 'kubernetes-service-endpoints-slow'
          honor_labels: true

          scrape_interval: 5m
          scrape_timeout: 30s

          kubernetes_sd_configs:
            - role: endpoints

          relabel_configs:
            - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape_slow]
              action: keep
              regex: true
            - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
              action: replace
              target_label: __scheme__
              regex: (https?)
            - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
              action: replace
              target_label: __metrics_path__
              regex: (.+)
            - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
              action: replace
              target_label: __address__
              regex: (.+?)(?::\d+)?;(\d+)
              replacement: $1:$2
            - action: labelmap
              regex: __meta_kubernetes_service_annotation_prometheus_io_param_(.+)
              replacement: __param_$1
            - action: labelmap
              regex: __meta_kubernetes_service_label_(.+)
            - source_labels: [__meta_kubernetes_namespace]
              action: replace
              target_label: namespace
            - source_labels: [__meta_kubernetes_service_name]
              action: replace
              target_label: service
            - source_labels: [__meta_kubernetes_pod_node_name]
              action: replace
              target_label: node

        - job_name: 'prometheus-pushgateway'
          honor_labels: true

          kubernetes_sd_configs:
            - role: service

          relabel_configs:
            - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]
              action: keep
              regex: pushgateway

        # Example scrape config for probing services via the Blackbox Exporter.
        #
        # The relabeling allows the actual service scrape endpoint to be configured
        # via the following annotations:
        #
        # * `prometheus.io/probe`: Only probe services that have a value of `true`
        - job_name: 'kubernetes-services'
          honor_labels: true

          metrics_path: /probe
          params:
            module: [http_2xx]

          kubernetes_sd_configs:
            - role: service

          relabel_configs:
            - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]
              action: keep
              regex: true
            - source_labels: [__address__]
              target_label: __param_target
            - target_label: __address__
              replacement: blackbox
            - source_labels: [__param_target]
              target_label: instance
            - action: labelmap
              regex: __meta_kubernetes_service_label_(.+)
            - source_labels: [__meta_kubernetes_namespace]
              target_label: namespace
            - source_labels: [__meta_kubernetes_service_name]
              target_label: service

        # Example scrape config for pods
        #
        # The relabeling allows the actual pod scrape endpoint to be configured via the
        # following annotations:
        #
        # * `prometheus.io/scrape`: Only scrape pods that have a value of `true`,
        # except if `prometheus.io/scrape-slow` is set to `true` as well.
        # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
        # to set this to `https` & most likely set the `tls_config` of the scrape config.
        # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
        # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`.
        - job_name: 'kubernetes-pods'
          honor_labels: true

          kubernetes_sd_configs:
            - role: pod

          relabel_configs:
            - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
              action: keep
              regex: true
            - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape_slow]
              action: drop
              regex: true
            - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scheme]
              action: replace
              regex: (https?)
              target_label: __scheme__
            - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
              action: replace
              target_label: __metrics_path__
              regex: (.+)
            - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_port, __meta_kubernetes_pod_ip]
              action: replace
              regex: (\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})
              replacement: '[$2]:$1'
              target_label: __address__
            - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_port, __meta_kubernetes_pod_ip]
              action: replace
              regex: (\d+);((([0-9]+?)(\.|$)){4})
              replacement: $2:$1
              target_label: __address__
            - action: labelmap
              regex: __meta_kubernetes_pod_annotation_prometheus_io_param_(.+)
              replacement: __param_$1
            - action: labelmap
              regex: __meta_kubernetes_pod_label_(.+)
            - source_labels: [__meta_kubernetes_namespace]
              action: replace
              target_label: namespace
            - source_labels: [__meta_kubernetes_pod_name]
              action: replace
              target_label: pod
            - source_labels: [__meta_kubernetes_pod_phase]
              regex: Pending|Succeeded|Failed|Completed
              action: drop
            - source_labels: [__meta_kubernetes_pod_node_name]
              action: replace
              target_label: node

        # Example Scrape config for pods which should be scraped slower. An useful example
        # would be stackriver-exporter which queries an API on every scrape of the pod
        #
        # The relabeling allows the actual pod scrape endpoint to be configured via the
        # following annotations:
        #
        # * `prometheus.io/scrape-slow`: Only scrape pods that have a value of `true`
        # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
        # to set this to `https` & most likely set the `tls_config` of the scrape config.
        # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
        # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`.
        - job_name: 'kubernetes-pods-slow'
          honor_labels: true

          scrape_interval: 5m
          scrape_timeout: 30s

          kubernetes_sd_configs:
            - role: pod

          relabel_configs:
            - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape_slow]
              action: keep
              regex: true
            - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scheme]
              action: replace
              regex: (https?)
              target_label: __scheme__
            - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
              action: replace
              target_label: __metrics_path__
              regex: (.+)
            - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_port, __meta_kubernetes_pod_ip]
              action: replace
              regex: (\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})
              replacement: '[$2]:$1'
              target_label: __address__
            - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_port, __meta_kubernetes_pod_ip]
              action: replace
              regex: (\d+);((([0-9]+?)(\.|$)){4})
              replacement: $2:$1
              target_label: __address__
            - action: labelmap
              regex: __meta_kubernetes_pod_annotation_prometheus_io_param_(.+)
              replacement: __param_$1
            - action: labelmap
              regex: __meta_kubernetes_pod_label_(.+)
            - source_labels: [__meta_kubernetes_namespace]
              action: replace
              target_label: namespace
            - source_labels: [__meta_kubernetes_pod_name]
              action: replace
              target_label: pod
            - source_labels: [__meta_kubernetes_pod_phase]
              regex: Pending|Succeeded|Failed|Completed
              action: drop
            - source_labels: [__meta_kubernetes_pod_node_name]
              action: replace
              target_label: node

  # adds additional scrape configs to prometheus.yml
  # must be a string so you have to add a | after extraScrapeConfigs:
  # example adds prometheus-blackbox-exporter scrape config
  extraScrapeConfigs: ""
    # - job_name: 'prometheus-blackbox-exporter'
    #   metrics_path: /probe
    #   params:
    #     module: [http_2xx]
    #   static_configs:
    #     - targets:
    #       - https://example.com
    #   relabel_configs:
    #     - source_labels: [__address__]
    #       target_label: __param_target
    #     - source_labels: [__param_target]
    #       target_label: instance
    #     - target_label: __address__
    #       replacement: prometheus-blackbox-exporter:9115

  # Adds option to add alert_relabel_configs to avoid duplicate alerts in alertmanager
  # useful in H/A prometheus with different external labels but the same alerts
  alertRelabelConfigs: {}
    # alert_relabel_configs:
    # - source_labels: [dc]
    #   regex: (.+)\d+
    #   target_label: dc

  networkPolicy:
    ## Enable creation of NetworkPolicy resources.
    ##
    enabled: false

  # Force namespace of namespaced resources
  forceNamespace: ""

  # Extra manifests to deploy as an array
  extraManifests: []
    # - |
    #   apiVersion: v1
    #   kind: ConfigMap
    #   metadata:
    #   labels:
    #     name: prometheus-extra
    #   data:
    #     extra-data: "value"

  # Configuration of subcharts defined in Chart.yaml

  ## alertmanager sub-chart configurable values
  ## Please see https://github.com/prometheus-community/helm-charts/tree/main/charts/alertmanager
  ##
  alertmanager:
    ## If false, alertmanager will not be installed
    ##
    enabled: true

    persistence:
      size: 2Gi

    podSecurityContext:
      runAsUser: 65534
      runAsNonRoot: true
      runAsGroup: 65534
      fsGroup: 65534

  ## kube-state-metrics sub-chart configurable values
  ## Please see https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-state-metrics
  ##
  kube-state-metrics:
    ## If false, kube-state-metrics sub-chart will not be installed
    ##
    enabled: true

  ## prometheus-node-exporter sub-chart configurable values
  ## Please see https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-node-exporter
  ##
  prometheus-node-exporter:
    ## If false, node-exporter will not be installed
    ##
    enabled: true

    rbac:
      pspEnabled: false

    containerSecurityContext:
      allowPrivilegeEscalation: false

  ## prometheus-pushgateway sub-chart configurable values
  ## Please see https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-pushgateway
  ##
  prometheus-pushgateway:
    ## If false, pushgateway will not be installed
    ##
    enabled: true

    # Optional service annotations
    serviceAnnotations:
      prometheus.io/probe: pushgateway


grafana:
  enabled: false
  global:
    # -- Overrides the Docker registry globally for all images
    imageRegistry: null

    # To help compatibility with other charts which use global.imagePullSecrets.
    # Allow either an array of {name: pullSecret} maps (k8s-style), or an array of strings (more common helm-style).
    # Can be templated.
    # global:
    #   imagePullSecrets:
    #   - name: pullSecret1
    #   - name: pullSecret2
    # or
    # global:
    #   imagePullSecrets:
    #   - pullSecret1
    #   - pullSecret2
    imagePullSecrets: []

  rbac:
    create: true
    ## Use an existing ClusterRole/Role (depending on rbac.namespaced false/true)
    # useExistingRole: name-of-some-role
    # useExistingClusterRole: name-of-some-clusterRole
    pspEnabled: false
    pspUseAppArmor: false
    namespaced: false
    extraRoleRules: []
    # - apiGroups: []
    #   resources: []
    #   verbs: []
    extraClusterRoleRules: []
    # - apiGroups: []
    #   resources: []
    #   verbs: []
  serviceAccount:
    create: true
    name:
    nameTest:
    ## ServiceAccount labels.
    labels: {}
    ## Service account annotations. Can be templated.
    #  annotations:
    #    eks.amazonaws.com/role-arn: arn:aws:iam::123456789000:role/iam-role-name-here

    ## autoMount is deprecated in favor of automountServiceAccountToken
    # autoMount: false
    automountServiceAccountToken: false

  replicas: 1

  ## Create a headless service for the deployment
  headlessService: false

  ## Should the service account be auto mounted on the pod
  automountServiceAccountToken: true

  ## Create HorizontalPodAutoscaler object for deployment type
  #
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 5
    targetCPU: "60"
    targetMemory: ""
    behavior: {}

  ## See `kubectl explain poddisruptionbudget.spec` for more
  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/
  podDisruptionBudget: {}
  #  apiVersion: ""
  #  minAvailable: 1
  #  maxUnavailable: 1

  ## See `kubectl explain deployment.spec.strategy` for more
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy
  deploymentStrategy:
    type: RollingUpdate

  readinessProbe:
    httpGet:
      path: /api/health
      port: 3000

  livenessProbe:
    httpGet:
      path: /api/health
      port: 3000
    initialDelaySeconds: 60
    timeoutSeconds: 30
    failureThreshold: 10

  ## Use an alternate scheduler, e.g. "stork".
  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
  ##
  # schedulerName: "default-scheduler"

  image:
    # -- The Docker registry
    registry: docker.io
    # -- Docker image repository
    repository: grafana/grafana
    # Overrides the Grafana image tag whose default is the chart appVersion
    tag: ""
    sha: ""
    pullPolicy: IfNotPresent

    ## Optionally specify an array of imagePullSecrets.
    ## Secrets must be manually created in the namespace.
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
    ## Can be templated.
    ##
    pullSecrets: []
    #   - myRegistrKeySecretName

  testFramework:
    enabled: true
    ## The type of Helm hook used to run this test. Defaults to test.
    ## ref: https://helm.sh/docs/topics/charts_hooks/#the-available-hooks
    ##
    # hookType: test
    image:
      # -- The Docker registry
      registry: docker.io
      repository: bats/bats
      tag: "v1.4.1"
    imagePullPolicy: IfNotPresent
    securityContext: {}
    resources: {}
    #  limits:
    #    cpu: 100m
    #    memory: 128Mi
    #  requests:
    #    cpu: 100m
    #    memory: 128Mi

  # dns configuration for pod
  dnsPolicy: ~
  dnsConfig: {}
    # nameservers:
    #   - 8.8.8.8
    #   options:
    #   - name: ndots
    #     value: "2"
    #   - name: edns0

  securityContext:
    runAsNonRoot: true
    runAsUser: 472
    runAsGroup: 472
    fsGroup: 472

  containerSecurityContext:
    allowPrivilegeEscalation: false
    capabilities:
      drop:
      - ALL
    seccompProfile:
      type: RuntimeDefault

  # Enable creating the grafana configmap
  createConfigmap: true

  # Extra configmaps to mount in grafana pods
  # Values are templated.
  extraConfigmapMounts: []
    # - name: certs-configmap
    #   mountPath: /etc/grafana/ssl/
    #   subPath: certificates.crt # (optional)
    #   configMap: certs-configmap
    #   readOnly: true
    #   optional: false


  extraEmptyDirMounts: []
    # - name: provisioning-notifiers
    #   mountPath: /etc/grafana/provisioning/notifiers


  # Apply extra labels to common labels.
  extraLabels: {}

  ## Assign a PriorityClassName to pods if set
  # priorityClassName:

  downloadDashboardsImage:
    # -- The Docker registry
    registry: docker.io
    repository: curlimages/curl
    tag: 7.85.0
    sha: ""
    pullPolicy: IfNotPresent

  downloadDashboards:
    env: {}
    envFromSecret: ""
    resources: {}
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      seccompProfile:
        type: RuntimeDefault
    envValueFrom: {}
    #  ENV_NAME:
    #    configMapKeyRef:
    #      name: configmap-name
    #      key: value_key

  ## Pod Annotations
  # podAnnotations: {}

  ## ConfigMap Annotations
  # configMapAnnotations: {}
    # argocd.argoproj.io/sync-options: Replace=true

  ## Pod Labels
  # podLabels: {}

  podPortName: grafana
  gossipPortName: gossip
  ## Deployment annotations
  # annotations: {}

  ## Expose the grafana service to be accessed from outside the cluster (LoadBalancer service).
  ## or access it from within the cluster (ClusterIP service). Set the service type and the port to serve it.
  ## ref: http://kubernetes.io/docs/user-guide/services/
  ##
  service:
    enabled: true
    type: ClusterIP
    # Set the ip family policy to configure dual-stack see [Configure dual-stack](https://kubernetes.io/docs/concepts/services-networking/dual-stack/#services)
    ipFamilyPolicy: ""
    # Sets the families that should be supported and the order in which they should be applied to ClusterIP as well. Can be IPv4 and/or IPv6.
    ipFamilies: []
    loadBalancerIP: ""
    loadBalancerClass: ""
    loadBalancerSourceRanges: []
    port: 80
    targetPort: 3000
      # targetPort: 4181 To be used with a proxy extraContainer
    ## Service annotations. Can be templated.
    annotations: {}
    labels: {}
    portName: service
    # Adds the appProtocol field to the service. This allows to work with istio protocol selection. Ex: "http" or "tcp"
    appProtocol: ""

  serviceMonitor:
    ## If true, a ServiceMonitor CR is created for a prometheus operator
    ## https://github.com/coreos/prometheus-operator
    ##
    enabled: false
    path: /metrics
    #  namespace: monitoring  (defaults to use the namespace this chart is deployed to)
    labels: {}
    interval: 30s
    scheme: http
    tlsConfig: {}
    scrapeTimeout: 30s
    relabelings: []
    metricRelabelings: []
    basicAuth: {}
    targetLabels: []

  extraExposePorts: []
  # - name: keycloak
  #   port: 8080
  #   targetPort: 8080

  # overrides pod.spec.hostAliases in the grafana deployment's pods
  hostAliases: []
    # - ip: "1.2.3.4"
    #   hostnames:
    #     - "my.host.com"

  ingress:
    enabled: false
    # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
    # ingressClassName: nginx
    # Values can be templated
    annotations: {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
    labels: {}
    path: /

    # pathType is only for k8s >= 1.1=
    pathType: Prefix

    hosts:
      - chart-example.local
    ## Extra paths to prepend to every host configuration. This is useful when working with annotation based services.
    extraPaths: []
    # - path: /*
    #   backend:
    #     serviceName: ssl-redirect
    #     servicePort: use-annotation
    ## Or for k8s > 1.19
    # - path: /*
    #   pathType: Prefix
    #   backend:
    #     service:
    #       name: ssl-redirect
    #       port:
    #         name: use-annotation


    tls: []
    #  - secretName: chart-example-tls
    #    hosts:
    #      - chart-example.local

  # -- BETA: Configure the gateway routes for the chart here.
  # More routes can be added by adding a dictionary key like the 'main' route.
  # Be aware that this is an early beta of this feature,
  # kube-prometheus-stack does not guarantee this works and is subject to change.
  # Being BETA this can/will change in the future without notice, do not use unless you want to take that risk
  # [[ref]](https://gateway-api.sigs.k8s.io/references/spec/#gateway.networking.k8s.io%2fv1alpha2)
  route:
    main:
      # -- Enables or disables the route
      enabled: false

      # -- Set the route apiVersion, e.g. gateway.networking.k8s.io/v1 or gateway.networking.k8s.io/v1alpha2
      apiVersion: gateway.networking.k8s.io/v1
      # -- Set the route kind
      # Valid options are GRPCRoute, HTTPRoute, TCPRoute, TLSRoute, UDPRoute
      kind: HTTPRoute

      annotations: {}
      labels: {}

      hostnames: []
      # - my-filter.example.com
      parentRefs: []
      # - name: acme-gw

      matches:
        - path:
            type: PathPrefix
            value: /

      ## Filters define the filters that are applied to requests that match this rule.
      filters: []

      ## Additional custom rules that can be added to the route
      additionalRules: []

  resources: {}
  #  limits:
  #    cpu: 100m
  #    memory: 128Mi
  #  requests:
  #    cpu: 100m
  #    memory: 128Mi

  ## Node labels for pod assignment
  ## ref: https://kubernetes.io/docs/user-guide/node-selection/
  #
  nodeSelector: {}

  ## Tolerations for pod assignment
  ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  ##
  tolerations: []

  ## Affinity for pod assignment (evaluated as template)
  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  ##
  affinity: {}

  ## Topology Spread Constraints
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/
  ##
  topologySpreadConstraints: []

  ## Additional init containers (evaluated as template)
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/
  ##
  extraInitContainers: []

  ## Enable an Specify container in extraContainers. This is meant to allow adding an authentication proxy to a grafana pod
  extraContainers: ""
  # extraContainers: |
  # - name: proxy
  #   image: quay.io/gambol99/keycloak-proxy:latest
  #   args:
  #   - -provider=github
  #   - -client-id=
  #   - -client-secret=
  #   - -github-org=<ORG_NAME>
  #   - -email-domain=*
  #   - -cookie-secret=
  #   - -http-address=http://0.0.0.0:4181
  #   - -upstream-url=http://127.0.0.1:3000
  #   ports:
  #     - name: proxy-web
  #       containerPort: 4181

  ## Volumes that can be used in init containers that will not be mounted to deployment pods
  extraContainerVolumes: []
  #  - name: volume-from-secret
  #    secret:
  #      secretName: secret-to-mount
  #  - name: empty-dir-volume
  #    emptyDir: {}

  ## Enable persistence using Persistent Volume Claims
  ## ref: https://kubernetes.io/docs/user-guide/persistent-volumes/
  ##
  persistence:
    type: pvc
    enabled: false
    # storageClassName: default
    accessModes:
      - ReadWriteOnce
    size: 10Gi
    # annotations: {}
    finalizers:
      - kubernetes.io/pvc-protection
    # selectorLabels: {}
    ## Sub-directory of the PV to mount. Can be templated.
    # subPath: ""
    ## Name of an existing PVC. Can be templated.
    # existingClaim:
    ## Extra labels to apply to a PVC.
    extraPvcLabels: {}
    disableWarning: false

    ## If persistence is not enabled, this allows to mount the
    ## local storage in-memory to improve performance
    ##
    inMemory:
      enabled: false
      ## The maximum usage on memory medium EmptyDir would be
      ## the minimum value between the SizeLimit specified
      ## here and the sum of memory limits of all containers in a pod
      ##
      # sizeLimit: 300Mi

    ## If 'lookupVolumeName' is set to true, Helm will attempt to retrieve
    ## the current value of 'spec.volumeName' and incorporate it into the template.
    lookupVolumeName: true

  initChownData:
    ## If false, data ownership will not be reset at startup
    ## This allows the grafana-server to be run with an arbitrary user
    ##
    enabled: true

    ## initChownData container image
    ##
    image:
      # -- The Docker registry
      registry: docker.io
      repository: library/busybox
      tag: "1.31.1"
      sha: ""
      pullPolicy: IfNotPresent

    ## initChownData resource requests and limits
    ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
    ##
    resources: {}
    #  limits:
    #    cpu: 100m
    #    memory: 128Mi
    #  requests:
    #    cpu: 100m
    #    memory: 128Mi
    securityContext:
      runAsNonRoot: false
      runAsUser: 0
      seccompProfile:
        type: RuntimeDefault
      capabilities:
        add:
          - CHOWN

  # Administrator credentials when not using an existing secret (see below)
  adminUser: admin
  # adminPassword: strongpassword

  # Use an existing secret for the admin user.
  admin:
    ## Name of the secret. Can be templated.
    existingSecret: ""
    userKey: admin-user
    passwordKey: admin-password

  ## Define command to be executed at startup by grafana container
  ## Needed if using `vault-env` to manage secrets (ref: https://banzaicloud.com/blog/inject-secrets-into-pods-vault/)
  ## Default is "run.sh" as defined in grafana's Dockerfile
  # command:
  # - "sh"
  # - "/run.sh"

  ## Optionally define args if command is used
  ## Needed if using `hashicorp/envconsul` to manage secrets
  ## By default no arguments are set
  # args:
  # - "-secret"
  # - "secret/grafana"
  # - "./grafana"

  ## Extra environment variables that will be pass onto deployment pods
  ##
  ## to provide grafana with access to CloudWatch on AWS EKS:
  ## 1. create an iam role of type "Web identity" with provider oidc.eks.* (note the provider for later)
  ## 2. edit the "Trust relationships" of the role, add a line inside the StringEquals clause using the
  ## same oidc eks provider as noted before (same as the existing line)
  ## also, replace NAMESPACE and prometheus-operator-grafana with the service account namespace and name
  ##
  ##  "oidc.eks.us-east-1.amazonaws.com/id/XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX:sub": "system:serviceaccount:NAMESPACE:prometheus-operator-grafana",
  ##
  ## 3. attach a policy to the role, you can use a built in policy called CloudWatchReadOnlyAccess
  ## 4. use the following env: (replace 123456789000 and iam-role-name-here with your aws account number and role name)
  ##
  ## env:
  ##   AWS_ROLE_ARN: arn:aws:iam::123456789000:role/iam-role-name-here
  ##   AWS_WEB_IDENTITY_TOKEN_FILE: /var/run/secrets/eks.amazonaws.com/serviceaccount/token
  ##   AWS_REGION: us-east-1
  ##
  ## 5. uncomment the EKS section in extraSecretMounts: below
  ## 6. uncomment the annotation section in the serviceAccount: above
  ## make sure to replace arn:aws:iam::123456789000:role/iam-role-name-here with your role arn

  env: {}

  ## "valueFrom" environment variable references that will be added to deployment pods. Name is templated.
  ## ref: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#envvarsource-v1-core
  ## Renders in container spec as:
  ##   env:
  ##     ...
  ##     - name: <key>
  ##       valueFrom:
  ##         <value rendered as YAML>
  envValueFrom: {}
    #  ENV_NAME:
    #    configMapKeyRef:
    #      name: configmap-name
    #      key: value_key

  ## The name of a secret in the same kubernetes namespace which contain values to be added to the environment
  ## This can be useful for auth tokens, etc. Value is templated.
  envFromSecret: ""

  ## Sensible environment variables that will be rendered as new secret object
  ## This can be useful for auth tokens, etc.
  ## If the secret values contains "{{", they'll need to be properly escaped so that they are not interpreted by Helm
  ## ref: https://helm.sh/docs/howto/charts_tips_and_tricks/#using-the-tpl-function
  envRenderSecret: {}

  ## The names of secrets in the same kubernetes namespace which contain values to be added to the environment
  ## Each entry should contain a name key, and can optionally specify whether the secret must be defined with an optional key.
  ## Name is templated.
  envFromSecrets: []
  ## - name: secret-name
  ##   prefix: prefix
  ##   optional: true

  ## The names of conifgmaps in the same kubernetes namespace which contain values to be added to the environment
  ## Each entry should contain a name key, and can optionally specify whether the configmap must be defined with an optional key.
  ## Name is templated.
  ## ref: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#configmapenvsource-v1-core
  envFromConfigMaps: []
  ## - name: configmap-name
  ##   prefix: prefix
  ##   optional: true

  # Inject Kubernetes services as environment variables.
  # See https://kubernetes.io/docs/concepts/services-networking/connect-applications-service/#environment-variables
  enableServiceLinks: true

  ## Additional grafana server secret mounts
  # Defines additional mounts with secrets. Secrets must be manually created in the namespace.
  extraSecretMounts: []
    # - name: secret-files
    #   mountPath: /etc/secrets
    #   secretName: grafana-secret-files
    #   readOnly: true
    #   optional: false
    #   subPath: ""
    #
    # for AWS EKS (cloudwatch) use the following (see also instruction in env: above)
    # - name: aws-iam-token
    #   mountPath: /var/run/secrets/eks.amazonaws.com/serviceaccount
    #   readOnly: true
    #   projected:
    #     defaultMode: 420
    #     sources:
    #       - serviceAccountToken:
    #           audience: sts.amazonaws.com
    #           expirationSeconds: 86400
    #           path: token
    #
    # for CSI e.g. Azure Key Vault use the following
    # - name: secrets-store-inline
    #  mountPath: /run/secrets
    #  readOnly: true
    #  csi:
    #    driver: secrets-store.csi.k8s.io
    #    readOnly: true
    #    volumeAttributes:
    #      secretProviderClass: "akv-grafana-spc"
    #    nodePublishSecretRef:                       # Only required when using service principal mode
    #       name: grafana-akv-creds                  # Only required when using service principal mode

  ## Additional grafana server volume mounts
  # Defines additional volume mounts.
  extraVolumeMounts: []
    # - name: extra-volume-0
    #   mountPath: /mnt/volume0
    #   readOnly: true
    # - name: extra-volume-1
    #   mountPath: /mnt/volume1
    #   readOnly: true
    # - name: grafana-secrets
    #   mountPath: /mnt/volume2

  ## Additional Grafana server volumes
  extraVolumes: []
    # - name: extra-volume-0
    #   existingClaim: volume-claim
    # - name: extra-volume-1
    #   hostPath:
    #     path: /usr/shared/
    #     type: ""
    # - name: grafana-secrets
    #   csi:
    #     driver: secrets-store.csi.k8s.io
    #     readOnly: true
    #     volumeAttributes:
    #       secretProviderClass: "grafana-env-spc"

  ## Container Lifecycle Hooks. Execute a specific bash command or make an HTTP request
  lifecycleHooks: {}
    # postStart:
    #   exec:
    #     command: []

  ## Pass the plugins you want installed as a list.
  ##
  plugins: []
    # - digrich-bubblechart-panel
    # - grafana-clock-panel
    ## You can also use other plugin download URL, as long as they are valid zip files,
    ## and specify the name of the plugin after the semicolon. Like this:
    # - https://grafana.com/api/plugins/marcusolsson-json-datasource/versions/1.3.2/download;marcusolsson-json-datasource

  ## Configure grafana datasources
  ## ref: http://docs.grafana.org/administration/provisioning/#datasources
  ##
  datasources: {}
  #  datasources.yaml:
  #    apiVersion: 1
  #    datasources:
  #    - name: Prometheus
  #      type: prometheus
  #      url: http://prometheus-prometheus-server
  #      access: proxy
  #      isDefault: true
  #    - name: CloudWatch
  #      type: cloudwatch
  #      access: proxy
  #      uid: cloudwatch
  #      editable: false
  #      jsonData:
  #        authType: default
  #        defaultRegion: us-east-1
  #    deleteDatasources: []
  #    - name: Prometheus

  ## Configure grafana alerting (can be templated)
  ## ref: https://docs.grafana.com/alerting/set-up/provision-alerting-resources/file-provisioning/
  ##
  alerting: {}
    # policies.yaml:
    #   apiVersion: 1
    #   policies:
    #     - orgId: 1
    #       receiver: first_uid
    #
    # rules.yaml:
    #   apiVersion: 1
    #   groups:
    #     - orgId: 1
    #       name: '{{ .Chart.Name }}_my_rule_group'
    #       folder: my_first_folder
    #       interval: 60s
    #       rules:
    #         - uid: my_id_1
    #           title: my_first_rule
    #           condition: A
    #           data:
    #             - refId: A
    #               datasourceUid: '-100'
    #               model:
    #                 conditions:
    #                   - evaluator:
    #                       params:
    #                         - 3
    #                       type: gt
    #                     operator:
    #                       type: and
    #                     query:
    #                       params:
    #                         - A
    #                     reducer:
    #                       type: last
    #                     type: query
    #                 datasource:
    #                   type: __expr__
    #                   uid: '-100'
    #                 expression: 1==0
    #                 intervalMs: 1000
    #                 maxDataPoints: 43200
    #                 refId: A
    #                 type: math
    #           dashboardUid: my_dashboard
    #           panelId: 123
    #           noDataState: Alerting
    #           for: 60s
    #           annotations:
    #             some_key: some_value
    #           labels:
    #             team: sre_team_1
    #
    # contactpoints.yaml:
    #   secret:
    #     apiVersion: 1
    #     contactPoints:
    #       - orgId: 1
    #         name: cp_1
    #         receivers:
    #           - uid: first_uid
    #             type: pagerduty
    #             settings:
    #               integrationKey: XXX
    #               severity: critical
    #               class: ping failure
    #               component: Grafana
    #               group: app-stack
    #               summary: |
    #                 {{ `{{ include "default.message" . }}` }}
    #
    # templates.yaml:
    #   apiVersion: 1
    #   templates:
    #     - orgId: 1
    #       name: my_first_template
    #       template: |
    #         {{ `
    #         {{ define "my_first_template" }}
    #         Custom notification message
    #         {{ end }}
    #         ` }}
    #
    # mutetimes.yaml
    #   apiVersion: 1
    #   muteTimes:
    #     - orgId: 1
    #       name: mti_1
    #       # refer to https://prometheus.io/docs/alerting/latest/configuration/#time_interval-0
    #       time_intervals: {}

  ## Configure notifiers
  ## ref: http://docs.grafana.org/administration/provisioning/#alert-notification-channels
  ##
  notifiers: {}
  #  notifiers.yaml:
  #    notifiers:
  #    - name: email-notifier
  #      type: email
  #      uid: email1
  #      # either:
  #      org_id: 1
  #      # or
  #      org_name: Main Org.
  #      is_default: true
  #      settings:
  #        addresses: an_email_address@example.com
  #    delete_notifiers:

  ## Configure grafana dashboard providers
  ## ref: http://docs.grafana.org/administration/provisioning/#dashboards
  ##
  ## `path` must be /var/lib/grafana/dashboards/<provider_name>
  ##
  dashboardProviders: {}
  #  dashboardproviders.yaml:
  #    apiVersion: 1
  #    providers:
  #    - name: 'default'
  #      orgId: 1
  #      folder: ''
  #      type: file
  #      disableDeletion: false
  #      editable: true
  #      options:
  #        path: /var/lib/grafana/dashboards/default

  ## Configure grafana dashboard to import
  ## NOTE: To use dashboards you must also enable/configure dashboardProviders
  ## ref: https://grafana.com/dashboards
  ##
  ## dashboards per provider, use provider name as key.
  ##
  dashboards: {}
    # default:
    #   some-dashboard:
    #     json: |
    #       $RAW_JSON
    #   custom-dashboard:
    #     file: dashboards/custom-dashboard.json
    #   prometheus-stats:
    #     gnetId: 2
    #     revision: 2
    #     datasource: Prometheus
    #   local-dashboard:
    #     url: https://example.com/repository/test.json
    #     token: ''
    #   local-dashboard-base64:
    #     url: https://example.com/repository/test-b64.json
    #     token: ''
    #     b64content: true
    #   local-dashboard-gitlab:
    #     url: https://example.com/repository/test-gitlab.json
    #     gitlabToken: ''
    #   local-dashboard-bitbucket:
    #     url: https://example.com/repository/test-bitbucket.json
    #     bearerToken: ''
    #   local-dashboard-azure:
    #     url: https://example.com/repository/test-azure.json
    #     basic: ''
    #     acceptHeader: '*/*'

  ## Reference to external ConfigMap per provider. Use provider name as key and ConfigMap name as value.
  ## A provider dashboards must be defined either by external ConfigMaps or in values.yaml, not in both.
  ## ConfigMap data example:
  ##
  ## data:
  ##   example-dashboard.json: |
  ##     RAW_JSON
  ##
  dashboardsConfigMaps: {}
  #  default: ""

  ## Grafana's primary configuration
  ## NOTE: values in map will be converted to ini format
  ## ref: http://docs.grafana.org/installation/configuration/
  ##
  grafana.ini:
    paths:
      data: /var/lib/grafana/
      logs: /var/log/grafana
      plugins: /var/lib/grafana/plugins
      provisioning: /etc/grafana/provisioning
    analytics:
      check_for_updates: true
    log:
      mode: console
    grafana_net:
      url: https://grafana.net
    server:
      domain: "{{ if (and .Values.ingress.enabled .Values.ingress.hosts) }}{{ tpl (.Values.ingress.hosts | first) . }}{{ else }}''{{ end }}"
  ## grafana Authentication can be enabled with the following values on grafana.ini
  # server:
        # The full public facing url you use in browser, used for redirects and emails
  #    root_url:
  # https://grafana.com/docs/grafana/latest/auth/github/#enable-github-in-grafana
  # auth.github:
  #    enabled: false
  #    allow_sign_up: false
  #    scopes: user:email,read:org
  #    auth_url: https://github.com/login/oauth/authorize
  #    token_url: https://github.com/login/oauth/access_token
  #    api_url: https://api.github.com/user
  #    team_ids:
  #    allowed_organizations:
  #    client_id:
  #    client_secret:
  ## LDAP Authentication can be enabled with the following values on grafana.ini
  ## NOTE: Grafana will fail to start if the value for ldap.toml is invalid
    # auth.ldap:
    #   enabled: true
    #   allow_sign_up: true
    #   config_file: /etc/grafana/ldap.toml

  ## Grafana's LDAP configuration
  ## Templated by the template in _helpers.tpl
  ## NOTE: To enable the grafana.ini must be configured with auth.ldap.enabled
  ## ref: http://docs.grafana.org/installation/configuration/#auth-ldap
  ## ref: http://docs.grafana.org/installation/ldap/#configuration
  ldap:
    enabled: false
    # `existingSecret` is a reference to an existing secret containing the ldap configuration
    # for Grafana in a key `ldap-toml`.
    existingSecret: ""
    # `config` is the content of `ldap.toml` that will be stored in the created secret
    config: ""
    # config: |-
    #   verbose_logging = true

    #   [[servers]]
    #   host = "my-ldap-server"
    #   port = 636
    #   use_ssl = true
    #   start_tls = false
    #   ssl_skip_verify = false
    #   bind_dn = "uid=%s,ou=users,dc=myorg,dc=com"

  ## Grafana's SMTP configuration
  ## NOTE: To enable, grafana.ini must be configured with smtp.enabled
  ## ref: http://docs.grafana.org/installation/configuration/#smtp
  smtp:
    # `existingSecret` is a reference to an existing secret containing the smtp configuration
    # for Grafana.
    existingSecret: ""
    userKey: "user"
    passwordKey: "password"

  ## Sidecars that collect the configmaps with specified label and stores the included files them into the respective folders
  ## Requires at least Grafana 5 to work and can't be used together with parameters dashboardProviders, datasources and dashboards
  sidecar:
    image:
      # -- The Docker registry
      registry: quay.io
      repository: kiwigrid/k8s-sidecar
      tag: 1.28.0
      sha: ""
    imagePullPolicy: IfNotPresent
    resources: {}
  #   limits:
  #     cpu: 100m
  #     memory: 100Mi
  #   requests:
  #     cpu: 50m
  #     memory: 50Mi
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      seccompProfile:
        type: RuntimeDefault
    # skipTlsVerify Set to true to skip tls verification for kube api calls
    # skipTlsVerify: true
    enableUniqueFilenames: false
    readinessProbe: {}
    livenessProbe: {}
    # Log level default for all sidecars. Can be one of: DEBUG, INFO, WARN, ERROR, CRITICAL. Defaults to INFO
    # logLevel: INFO
    alerts:
      enabled: false
      # Additional environment variables for the alerts sidecar
      env: {}
      # Do not reprocess already processed unchanged resources on k8s API reconnect.
      # ignoreAlreadyProcessed: true
      # label that the configmaps with alert are marked with
      label: grafana_alert
      # value of label that the configmaps with alert are set to
      labelValue: ""
      # Log level. Can be one of: DEBUG, INFO, WARN, ERROR, CRITICAL.
      # logLevel: INFO
      # If specified, the sidecar will search for alert config-maps inside this namespace.
      # Otherwise the namespace in which the sidecar is running will be used.
      # It's also possible to specify ALL to search in all namespaces
      searchNamespace: null
      # Method to use to detect ConfigMap changes. With WATCH the sidecar will do a WATCH requests, with SLEEP it will list all ConfigMaps, then sleep for 60 seconds.
      watchMethod: WATCH
      # search in configmap, secret or both
      resource: both
      # watchServerTimeout: request to the server, asking it to cleanly close the connection after that.
      # defaults to 60sec; much higher values like 3600 seconds (1h) are feasible for non-Azure K8S
      # watchServerTimeout: 3600
      #
      # watchClientTimeout: is a client-side timeout, configuring your local socket.
      # If you have a network outage dropping all packets with no RST/FIN,
      # this is how long your client waits before realizing & dropping the connection.
      # defaults to 66sec (sic!)
      # watchClientTimeout: 60
      #
      # maxTotalRetries: Total number of retries to allow for any http request.
      # Takes precedence over other counts. Applies to all requests to reloadURL and k8s api requests.
      # Set to 0 to fail on the first retry.
      # maxTotalRetries: 5
      #
      # maxConnectRetries: How many connection-related errors to retry on for any http request.
      # These are errors raised before the request is sent to the remote server, which we assume has not triggered the server to process the request.
      # Applies to all requests to reloadURL and k8s api requests.
      # Set to 0 to fail on the first retry of this type.
      # maxConnectRetries: 10
      #
      # maxReadRetries: How many times to retry on read errors for any http request
      # These errors are raised after the request was sent to the server, so the request may have side-effects.
      # Applies to all requests to reloadURL and k8s api requests.
      # Set to 0 to fail on the first retry of this type.
      # maxReadRetries: 5
      #
      # Endpoint to send request to reload alerts
      reloadURL: "http://localhost:3000/api/admin/provisioning/alerting/reload"
      # Absolute path to shell script to execute after a alert got reloaded
      script: null
      skipReload: false
      # This is needed if skipReload is true, to load any alerts defined at startup time.
      # Deploy the alert sidecar as an initContainer.
      initAlerts: false
      # Additional alerts sidecar volume mounts
      extraMounts: []
      # Sets the size limit of the alert sidecar emptyDir volume
      sizeLimit: {}
    dashboards:
      enabled: false
      # Additional environment variables for the dashboards sidecar
      env: {}
      ## "valueFrom" environment variable references that will be added to deployment pods. Name is templated.
      ## ref: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#envvarsource-v1-core
      ## Renders in container spec as:
      ##   env:
      ##     ...
      ##     - name: <key>
      ##       valueFrom:
      ##         <value rendered as YAML>
      envValueFrom: {}
      #  ENV_NAME:
      #    configMapKeyRef:
      #      name: configmap-name
      #      key: value_key
      # Do not reprocess already processed unchanged resources on k8s API reconnect.
      # ignoreAlreadyProcessed: true
      SCProvider: true
      # label that the configmaps with dashboards are marked with
      label: grafana_dashboard
      # value of label that the configmaps with dashboards are set to
      labelValue: ""
      # Log level. Can be one of: DEBUG, INFO, WARN, ERROR, CRITICAL.
      # logLevel: INFO
      # folder in the pod that should hold the collected dashboards (unless `defaultFolderName` is set)
      folder: /tmp/dashboards
      # The default folder name, it will create a subfolder under the `folder` and put dashboards in there instead
      defaultFolderName: null
      # Namespaces list. If specified, the sidecar will search for config-maps/secrets inside these namespaces.
      # Otherwise the namespace in which the sidecar is running will be used.
      # It's also possible to specify ALL to search in all namespaces.
      searchNamespace: null
      # Method to use to detect ConfigMap changes. With WATCH the sidecar will do a WATCH requests, with SLEEP it will list all ConfigMaps, then sleep for 60 seconds.
      watchMethod: WATCH
      # search in configmap, secret or both
      resource: both
      # If specified, the sidecar will look for annotation with this name to create folder and put graph here.
      # You can use this parameter together with `provider.foldersFromFilesStructure`to annotate configmaps and create folder structure.
      folderAnnotation: null
      #
      # maxTotalRetries: Total number of retries to allow for any http request.
      # Takes precedence over other counts. Applies to all requests to reloadURL and k8s api requests.
      # Set to 0 to fail on the first retry.
      # maxTotalRetries: 5
      #
      # maxConnectRetries: How many connection-related errors to retry on for any http request.
      # These are errors raised before the request is sent to the remote server, which we assume has not triggered the server to process the request.
      # Applies to all requests to reloadURL and k8s api requests.
      # Set to 0 to fail on the first retry of this type.
      # maxConnectRetries: 10
      #
      # maxReadRetries: How many times to retry on read errors for any http request
      # These errors are raised after the request was sent to the server, so the request may have side-effects.
      # Applies to all requests to reloadURL and k8s api requests.
      # Set to 0 to fail on the first retry of this type.
      # maxReadRetries: 5
      #
      # Endpoint to send request to reload alerts
      reloadURL: "http://localhost:3000/api/admin/provisioning/dashboards/reload"
      # Absolute path to shell script to execute after a configmap got reloaded
      script: null
      skipReload: false
      # watchServerTimeout: request to the server, asking it to cleanly close the connection after that.
      # defaults to 60sec; much higher values like 3600 seconds (1h) are feasible for non-Azure K8S
      # watchServerTimeout: 3600
      #
      # watchClientTimeout: is a client-side timeout, configuring your local socket.
      # If you have a network outage dropping all packets with no RST/FIN,
      # this is how long your client waits before realizing & dropping the connection.
      # defaults to 66sec (sic!)
      # watchClientTimeout: 60
      #
      # provider configuration that lets grafana manage the dashboards
      provider:
        # name of the provider, should be unique
        name: sidecarProvider
        # orgid as configured in grafana
        orgid: 1
        # folder in which the dashboards should be imported in grafana
        folder: ''
        # <string> folder UID. will be automatically generated if not specified
        folderUid: ''
        # type of the provider
        type: file
        # disableDelete to activate a import-only behaviour
        disableDelete: false
        # allow updating provisioned dashboards from the UI
        allowUiUpdates: false
        # allow Grafana to replicate dashboard structure from filesystem
        foldersFromFilesStructure: false
      # Additional dashboards sidecar volume mounts
      extraMounts: []
      # Sets the size limit of the dashboard sidecar emptyDir volume
      sizeLimit: {}
    datasources:
      enabled: false
      # Additional environment variables for the datasourcessidecar
      env: {}
      ## "valueFrom" environment variable references that will be added to deployment pods. Name is templated.
      ## ref: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#envvarsource-v1-core
      ## Renders in container spec as:
      ##   env:
      ##     ...
      ##     - name: <key>
      ##       valueFrom:
      ##         <value rendered as YAML>
      envValueFrom: {}
      #  ENV_NAME:
      #    configMapKeyRef:
      #      name: configmap-name
      #      key: value_key
      # Do not reprocess already processed unchanged resources on k8s API reconnect.
      # ignoreAlreadyProcessed: true
      # label that the configmaps with datasources are marked with
      label: grafana_datasource
      # value of label that the configmaps with datasources are set to
      labelValue: ""
      # Log level. Can be one of: DEBUG, INFO, WARN, ERROR, CRITICAL.
      # logLevel: INFO
      # If specified, the sidecar will search for datasource config-maps inside this namespace.
      # Otherwise the namespace in which the sidecar is running will be used.
      # It's also possible to specify ALL to search in all namespaces
      searchNamespace: null
      # Method to use to detect ConfigMap changes. With WATCH the sidecar will do a WATCH requests, with SLEEP it will list all ConfigMaps, then sleep for 60 seconds.
      watchMethod: WATCH
      # search in configmap, secret or both
      resource: both
      # watchServerTimeout: request to the server, asking it to cleanly close the connection after that.
      # defaults to 60sec; much higher values like 3600 seconds (1h) are feasible for non-Azure K8S
      # watchServerTimeout: 3600
      #
      # watchClientTimeout: is a client-side timeout, configuring your local socket.
      # If you have a network outage dropping all packets with no RST/FIN,
      # this is how long your client waits before realizing & dropping the connection.
      # defaults to 66sec (sic!)
      # watchClientTimeout: 60
      #
      # maxTotalRetries: Total number of retries to allow for any http request.
      # Takes precedence over other counts. Applies to all requests to reloadURL and k8s api requests.
      # Set to 0 to fail on the first retry.
      # maxTotalRetries: 5
      #
      # maxConnectRetries: How many connection-related errors to retry on for any http request.
      # These are errors raised before the request is sent to the remote server, which we assume has not triggered the server to process the request.
      # Applies to all requests to reloadURL and k8s api requests.
      # Set to 0 to fail on the first retry of this type.
      # maxConnectRetries: 10
      #
      # maxReadRetries: How many times to retry on read errors for any http request
      # These errors are raised after the request was sent to the server, so the request may have side-effects.
      # Applies to all requests to reloadURL and k8s api requests.
      # Set to 0 to fail on the first retry of this type.
      # maxReadRetries: 5
      #
      # Endpoint to send request to reload datasources
      reloadURL: "http://localhost:3000/api/admin/provisioning/datasources/reload"
      # Absolute path to shell script to execute after a datasource got reloaded
      script: null
      skipReload: false
      # This is needed if skipReload is true, to load any datasources defined at startup time.
      # Deploy the datasources sidecar as an initContainer.
      initDatasources: false
      # Additional datasources sidecar volume mounts
      extraMounts: []
      # Sets the size limit of the datasource sidecar emptyDir volume
      sizeLimit: {}
    plugins:
      enabled: false
      # Additional environment variables for the plugins sidecar
      env: {}
      # Do not reprocess already processed unchanged resources on k8s API reconnect.
      # ignoreAlreadyProcessed: true
      # label that the configmaps with plugins are marked with
      label: grafana_plugin
      # value of label that the configmaps with plugins are set to
      labelValue: ""
      # Log level. Can be one of: DEBUG, INFO, WARN, ERROR, CRITICAL.
      # logLevel: INFO
      # If specified, the sidecar will search for plugin config-maps inside this namespace.
      # Otherwise the namespace in which the sidecar is running will be used.
      # It's also possible to specify ALL to search in all namespaces
      searchNamespace: null
      # Method to use to detect ConfigMap changes. With WATCH the sidecar will do a WATCH requests, with SLEEP it will list all ConfigMaps, then sleep for 60 seconds.
      watchMethod: WATCH
      # search in configmap, secret or both
      resource: both
      # watchServerTimeout: request to the server, asking it to cleanly close the connection after that.
      # defaults to 60sec; much higher values like 3600 seconds (1h) are feasible for non-Azure K8S
      # watchServerTimeout: 3600
      #
      # watchClientTimeout: is a client-side timeout, configuring your local socket.
      # If you have a network outage dropping all packets with no RST/FIN,
      # this is how long your client waits before realizing & dropping the connection.
      # defaults to 66sec (sic!)
      # watchClientTimeout: 60
      #
      # maxTotalRetries: Total number of retries to allow for any http request.
      # Takes precedence over other counts. Applies to all requests to reloadURL and k8s api requests.
      # Set to 0 to fail on the first retry.
      # maxTotalRetries: 5
      #
      # maxConnectRetries: How many connection-related errors to retry on for any http request.
      # These are errors raised before the request is sent to the remote server, which we assume has not triggered the server to process the request.
      # Applies to all requests to reloadURL and k8s api requests.
      # Set to 0 to fail on the first retry of this type.
      # maxConnectRetries: 10
      #
      # maxReadRetries: How many times to retry on read errors for any http request
      # These errors are raised after the request was sent to the server, so the request may have side-effects.
      # Applies to all requests to reloadURL and k8s api requests.
      # Set to 0 to fail on the first retry of this type.
      # maxReadRetries: 5
      #
      # Endpoint to send request to reload plugins
      reloadURL: "http://localhost:3000/api/admin/provisioning/plugins/reload"
      # Absolute path to shell script to execute after a plugin got reloaded
      script: null
      skipReload: false
      # Deploy the datasource sidecar as an initContainer in addition to a container.
      # This is needed if skipReload is true, to load any plugins defined at startup time.
      initPlugins: false
      # Additional plugins sidecar volume mounts
      extraMounts: []
      # Sets the size limit of the plugin sidecar emptyDir volume
      sizeLimit: {}
    notifiers:
      enabled: false
      # Additional environment variables for the notifierssidecar
      env: {}
      # Do not reprocess already processed unchanged resources on k8s API reconnect.
      # ignoreAlreadyProcessed: true
      # label that the configmaps with notifiers are marked with
      label: grafana_notifier
      # value of label that the configmaps with notifiers are set to
      labelValue: ""
      # Log level. Can be one of: DEBUG, INFO, WARN, ERROR, CRITICAL.
      # logLevel: INFO
      # If specified, the sidecar will search for notifier config-maps inside this namespace.
      # Otherwise the namespace in which the sidecar is running will be used.
      # It's also possible to specify ALL to search in all namespaces
      searchNamespace: null
      # Method to use to detect ConfigMap changes. With WATCH the sidecar will do a WATCH requests, with SLEEP it will list all ConfigMaps, then sleep for 60 seconds.
      watchMethod: WATCH
      # search in configmap, secret or both
      resource: both
      # watchServerTimeout: request to the server, asking it to cleanly close the connection after that.
      # defaults to 60sec; much higher values like 3600 seconds (1h) are feasible for non-Azure K8S
      # watchServerTimeout: 3600
      #
      # watchClientTimeout: is a client-side timeout, configuring your local socket.
      # If you have a network outage dropping all packets with no RST/FIN,
      # this is how long your client waits before realizing & dropping the connection.
      # defaults to 66sec (sic!)
      # watchClientTimeout: 60
      #
      # maxTotalRetries: Total number of retries to allow for any http request.
      # Takes precedence over other counts. Applies to all requests to reloadURL and k8s api requests.
      # Set to 0 to fail on the first retry.
      # maxTotalRetries: 5
      #
      # maxConnectRetries: How many connection-related errors to retry on for any http request.
      # These are errors raised before the request is sent to the remote server, which we assume has not triggered the server to process the request.
      # Applies to all requests to reloadURL and k8s api requests.
      # Set to 0 to fail on the first retry of this type.
      # maxConnectRetries: 10
      #
      # maxReadRetries: How many times to retry on read errors for any http request
      # These errors are raised after the request was sent to the server, so the request may have side-effects.
      # Applies to all requests to reloadURL and k8s api requests.
      # Set to 0 to fail on the first retry of this type.
      # maxReadRetries: 5
      #
      # Endpoint to send request to reload notifiers
      reloadURL: "http://localhost:3000/api/admin/provisioning/notifications/reload"
      # Absolute path to shell script to execute after a notifier got reloaded
      script: null
      skipReload: false
      # Deploy the notifier sidecar as an initContainer in addition to a container.
      # This is needed if skipReload is true, to load any notifiers defined at startup time.
      initNotifiers: false
      # Additional notifiers sidecar volume mounts
      extraMounts: []
      # Sets the size limit of the notifier sidecar emptyDir volume
      sizeLimit: {}

  ## Override the deployment namespace
  ##
  namespaceOverride: ""

  ## Number of old ReplicaSets to retain
  ##
  revisionHistoryLimit: 10

  ## Add a seperate remote image renderer deployment/service
  imageRenderer:
    deploymentStrategy: {}
    # Enable the image-renderer deployment & service
    enabled: false
    replicas: 1
    autoscaling:
      enabled: false
      minReplicas: 1
      maxReplicas: 5
      targetCPU: "60"
      targetMemory: ""
      behavior: {}
    # The url of remote image renderer if it is not in the same namespace with the grafana instance
    serverURL: ""
    # The callback url of grafana instances if it is not in the same namespace with the remote image renderer
    renderingCallbackURL: ""
    image:
      # -- The Docker registry
      registry: docker.io
      # image-renderer Image repository
      repository: grafana/grafana-image-renderer
      # image-renderer Image tag
      tag: latest
      # image-renderer Image sha (optional)
      sha: ""
      # image-renderer ImagePullPolicy
      pullPolicy: Always
    # extra environment variables
    env:
      HTTP_HOST: "0.0.0.0"
      # Fixes "Error: Failed to launch the browser process!\nchrome_crashpad_handler: --database is required"
      XDG_CONFIG_HOME: /tmp/.chromium
      XDG_CACHE_HOME: /tmp/.chromium
      # RENDERING_ARGS: --no-sandbox,--disable-gpu,--window-size=1280x758
      # RENDERING_MODE: clustered
      # IGNORE_HTTPS_ERRORS: true

    ## "valueFrom" environment variable references that will be added to deployment pods. Name is templated.
    ## ref: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#envvarsource-v1-core
    ## Renders in container spec as:
    ##   env:
    ##     ...
    ##     - name: <key>
    ##       valueFrom:
    ##         <value rendered as YAML>
    envValueFrom: {}
      #  ENV_NAME:
      #    configMapKeyRef:
      #      name: configmap-name
      #      key: value_key

    # image-renderer deployment serviceAccount
    serviceAccountName: ""
    automountServiceAccountToken: false
    # image-renderer deployment securityContext
    securityContext: {}
    # image-renderer deployment container securityContext
    containerSecurityContext:
      seccompProfile:
        type: RuntimeDefault
      capabilities:
        drop: ['ALL']
      allowPrivilegeEscalation: false
      readOnlyRootFilesystem: true
    ## image-renderer pod annotation
    podAnnotations: {}
    # image-renderer deployment Host Aliases
    hostAliases: []
    # image-renderer deployment priority class
    priorityClassName: ''
    service:
      # Enable the image-renderer service
      enabled: true
      # image-renderer service port name
      portName: 'http'
      # image-renderer service port used by both service and deployment
      port: 8081
      targetPort: 8081
      # Adds the appProtocol field to the image-renderer service. This allows to work with istio protocol selection. Ex: "http" or "tcp"
      appProtocol: ""
    serviceMonitor:
      ## If true, a ServiceMonitor CRD is created for a prometheus operator
      ## https://github.com/coreos/prometheus-operator
      ##
      enabled: false
      path: /metrics
      #  namespace: monitoring  (defaults to use the namespace this chart is deployed to)
      labels: {}
      interval: 1m
      scheme: http
      tlsConfig: {}
      scrapeTimeout: 30s
      relabelings: []
      # See: https://doc.crds.dev/github.com/prometheus-operator/kube-prometheus/monitoring.coreos.com/ServiceMonitor/v1@v0.11.0#spec-targetLabels
      targetLabels: []
        # - targetLabel1
        # - targetLabel2
    # If https is enabled in Grafana, this needs to be set as 'https' to correctly configure the callback used in Grafana
    grafanaProtocol: http
    # In case a sub_path is used this needs to be added to the image renderer callback
    grafanaSubPath: ""
    # name of the image-renderer port on the pod
    podPortName: http
    # number of image-renderer replica sets to keep
    revisionHistoryLimit: 10
    networkPolicy:
      # Enable a NetworkPolicy to limit inbound traffic to only the created grafana pods
      limitIngress: true
      # Enable a NetworkPolicy to limit outbound traffic to only the created grafana pods
      limitEgress: false
      # Allow additional services to access image-renderer (eg. Prometheus operator when ServiceMonitor is enabled)
      extraIngressSelectors: []
    resources: {}
  #   limits:
  #     cpu: 100m
  #     memory: 100Mi
  #   requests:
  #     cpu: 50m
  #     memory: 50Mi
    ## Node labels for pod assignment
    ## ref: https://kubernetes.io/docs/user-guide/node-selection/
    #
    nodeSelector: {}

    ## Tolerations for pod assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
    ##
    tolerations: []

    ## Affinity for pod assignment (evaluated as template)
    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
    ##
    affinity: {}

    ## Use an alternate scheduler, e.g. "stork".
    ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
    ##
    # schedulerName: "default-scheduler"

    # Extra configmaps to mount in image-renderer pods
    extraConfigmapMounts: []

    # Extra secrets to mount in image-renderer pods
    extraSecretMounts: []

    # Extra volumes to mount in image-renderer pods
    extraVolumeMounts: []

    # Extra volumes for image-renderer pods
    extraVolumes: []

  networkPolicy:
    ## @param networkPolicy.enabled Enable creation of NetworkPolicy resources. Only Ingress traffic is filtered for now.
    ##
    enabled: false
    ## @param networkPolicy.allowExternal Don't require client label for connections
    ## The Policy model to apply. When set to false, only pods with the correct
    ## client label will have network access to  grafana port defined.
    ## When true, grafana will accept connections from any source
    ## (with the correct destination port).
    ##
    ingress: true
    ## @param networkPolicy.ingress When true enables the creation
    ## an ingress network policy
    ##
    allowExternal: true
    ## @param networkPolicy.explicitNamespacesSelector A Kubernetes LabelSelector to explicitly select namespaces from which traffic could be allowed
    ## If explicitNamespacesSelector is missing or set to {}, only client Pods that are in the networkPolicy's namespace
    ## and that match other criteria, the ones that have the good label, can reach the grafana.
    ## But sometimes, we want the grafana to be accessible to clients from other namespaces, in this case, we can use this
    ## LabelSelector to select these namespaces, note that the networkPolicy's namespace should also be explicitly added.
    ##
    ## Example:
    ## explicitNamespacesSelector:
    ##   matchLabels:
    ##     role: frontend
    ##   matchExpressions:
    ##    - {key: role, operator: In, values: [frontend]}
    ##
    explicitNamespacesSelector: {}
    ##
    ##
    ##
    ##
    ##
    ##
    egress:
      ## @param networkPolicy.egress.enabled When enabled, an egress network policy will be
      ## created allowing grafana to connect to external data sources from kubernetes cluster.
      enabled: false
      ##
      ## @param networkPolicy.egress.blockDNSResolution When enabled, DNS resolution will be blocked
      ## for all pods in the grafana namespace.
      blockDNSResolution: false
      ##
      ## @param networkPolicy.egress.ports Add individual ports to be allowed by the egress
      ports: []
      ## Add ports to the egress by specifying - port: <port number>
      ## E.X.
      ## - port: 80
      ## - port: 443
      ##
      ## @param networkPolicy.egress.to Allow egress traffic to specific destinations
      to: []
      ## Add destinations to the egress by specifying - ipBlock: <CIDR>
      ## E.X.
      ## to:
      ##  - namespaceSelector:
      ##    matchExpressions:
    ##    - {key: role, operator: In, values: [grafana]}
    ##
    ##
    ##
    ##
    ##

  # Enable backward compatibility of kubernetes where version below 1.13 doesn't have the enableServiceLinks option
  enableKubeBackwardCompatibility: false
  useStatefulSet: false
  # Create a dynamic manifests via values:
  extraObjects: []
    # - apiVersion: "kubernetes-client.io/v1"
    #   kind: ExternalSecret
    #   metadata:
    #     name: grafana-secrets
    #   spec:
    #     backendType: gcpSecretsManager
    #     data:
    #       - key: grafana-admin-password
    #         name: adminPassword

  # assertNoLeakedSecrets is a helper function defined in _helpers.tpl that checks if secret
  # values are not exposed in the rendered grafana.ini configmap. It is enabled by default.
  #
  # To pass values into grafana.ini without exposing them in a configmap, use variable expansion:
  # https://grafana.com/docs/grafana/latest/setup-grafana/configure-grafana/#variable-expansion
  #
  # Alternatively, if you wish to allow secret values to be exposed in the rendered grafana.ini configmap,
  # you can disable this check by setting assertNoLeakedSecrets to false.
  assertNoLeakedSecrets: true


loki:
  enabled: false
  # -- Overrides the version used to determine compatibility of resources with the target Kubernetes cluster.
  # This is useful when using `helm template`, because then helm will use the client version of kubectl as the Kubernetes version,
  # which may or may not match your cluster's server version. Example: 'v1.24.4'. Set to null to use the version that helm
  # devises.
  kubeVersionOverride: null

  global:
    image:
      # -- Overrides the Docker registry globally for all images
      registry: null
    # -- Overrides the priorityClassName for all pods
    priorityClassName: null
    # -- configures cluster domain ("cluster.local" by default)
    clusterDomain: "cluster.local"
    # -- configures DNS service name
    dnsService: "kube-dns"
    # -- configures DNS service namespace
    dnsNamespace: "kube-system"
  # -- Overrides the chart's name
  nameOverride: null
  # -- Overrides the chart's computed fullname
  fullnameOverride: null
  # -- Overrides the chart's cluster label
  clusterLabelOverride: null
  # -- Image pull secrets for Docker images
  imagePullSecrets: []
  # -- Deployment mode lets you specify how to deploy Loki.
  # There are 3 options:
  # - SingleBinary: Loki is deployed as a single binary, useful for small installs typically without HA, up to a few tens of GB/day.
  # - SimpleScalable: Loki is deployed as 3 targets: read, write, and backend. Useful for medium installs easier to manage than distributed, up to a about 1TB/day.
  # - Distributed: Loki is deployed as individual microservices. The most complicated but most capable, useful for large installs, typically over 1TB/day.
  # There are also 2 additional modes used for migrating between deployment modes:
  # - SingleBinary<->SimpleScalable: Migrate from SingleBinary to SimpleScalable (or vice versa)
  # - SimpleScalable<->Distributed: Migrate from SimpleScalable to Distributed (or vice versa)
  # Note: SimpleScalable and Distributed REQUIRE the use of object storage.
  deploymentMode: SimpleScalable
  ######################################################################################################################
  #
  # Base Loki Configs including kubernetes configurations and configurations for Loki itself,
  # see below for more specifics on Loki's configuration.
  #
  ######################################################################################################################
  # -- Configuration for running Loki
  # @default -- See values.yaml
  loki:
    # Configures the readiness probe for all of the Loki pods
    readinessProbe:
      httpGet:
        path: /ready
        port: http-metrics
      initialDelaySeconds: 30
      timeoutSeconds: 1
    image:
      # -- The Docker registry
      registry: docker.io
      # -- Docker image repository
      repository: grafana/loki
      # -- Overrides the image tag whose default is the chart's appVersion
      tag: 3.3.2
      # -- Overrides the image tag with an image digest
      digest: null
      # -- Docker image pull policy
      pullPolicy: IfNotPresent
    # -- Common annotations for all deployments/StatefulSets
    annotations: {}
    # -- Common annotations for all pods
    podAnnotations: {}
    # -- Common labels for all pods
    podLabels: {}
    # -- Common annotations for all services
    serviceAnnotations: {}
    # -- Common labels for all services
    serviceLabels: {}
    # -- The number of old ReplicaSets to retain to allow rollback
    revisionHistoryLimit: 10
    # -- The SecurityContext for Loki pods
    podSecurityContext:
      fsGroup: 10001
      runAsGroup: 10001
      runAsNonRoot: true
      runAsUser: 10001
    # -- The SecurityContext for Loki containers
    containerSecurityContext:
      readOnlyRootFilesystem: true
      capabilities:
        drop:
          - ALL
      allowPrivilegeEscalation: false
    # -- Should enableServiceLinks be enabled. Default to enable
    enableServiceLinks: true
    ######################################################################################################################
    #
    # Loki Configuration
    #
    # There are several ways to pass configuration to Loki, listing them here in order of our preference for how
    # you should use this chart.
    # 1. Use the templated value of loki.config below and the corresponding override sections which follow.
    #    This allows us to set a lot of important Loki configurations and defaults and also allows us to maintain them
    #    over time as Loki changes and evolves.
    # 2. Use the loki.structuredConfig section.
    #    This will completely override the templated value of loki.config, so you MUST provide the entire Loki config
    #    including any configuration that we set in loki.config unless you explicitly are trying to change one of those
    #    values and are not able to do so with the templated sections.
    #    If you choose this approach the burden is on you to maintain any changes we make to the templated config.
    # 3. Use an existing secret or configmap to provide the configuration.
    #    This option is mostly provided for folks who have external processes which provide or modify the configuration.
    #    When using this option you can specify a different name for loki.generatedConfigObjectName and configObjectName
    #    if you have a process which takes the generated config and modifies it, or you can stop the chart from generating
    #    a config entirely by setting loki.generatedConfigObjectName to
    #
    ######################################################################################################################

    # -- Defines what kind of object stores the configuration, a ConfigMap or a Secret.
    # In order to move sensitive information (such as credentials) from the ConfigMap/Secret to a more secure location (e.g. vault), it is possible to use [environment variables in the configuration](https://grafana.com/docs/loki/latest/configuration/#use-environment-variables-in-the-configuration).
    # Such environment variables can be then stored in a separate Secret and injected via the global.extraEnvFrom value. For details about environment injection from a Secret please see [Secrets](https://kubernetes.io/docs/concepts/configuration/secret/#use-case-as-container-environment-variables).
    configStorageType: ConfigMap
    # -- The name of the object which Loki will mount as a volume containing the config.
    # If the configStorageType is Secret, this will be the name of the Secret, if it is ConfigMap, this will be the name of the ConfigMap.
    # The value will be passed through tpl.
    configObjectName: '{{ include "loki.name" . }}'
    # -- The name of the Secret or ConfigMap that will be created by this chart.
    # If empty, no configmap or secret will be created.
    # The value will be passed through tpl.
    generatedConfigObjectName: '{{ include "loki.name" . }}'
    # -- Config file contents for Loki
    # @default -- See values.yaml
    config: |
      {{- if .Values.enterprise.enabled}}
      {{- tpl .Values.enterprise.config . }}
      {{- else }}
      auth_enabled: {{ .Values.loki.auth_enabled }}
      {{- end }}

      {{- with .Values.loki.server }}
      server:
        {{- toYaml . | nindent 2}}
      {{- end}}

      pattern_ingester:
        enabled: {{ .Values.loki.pattern_ingester.enabled }}

      memberlist:
      {{- if .Values.loki.memberlistConfig }}
        {{- toYaml .Values.loki.memberlistConfig | nindent 2 }}
      {{- else }}
      {{- if .Values.loki.extraMemberlistConfig}}
      {{- toYaml .Values.loki.extraMemberlistConfig | nindent 2}}
      {{- end }}
        join_members:
          - {{ include "loki.memberlist" . }}
          {{- with .Values.migrate.fromDistributed }}
          {{- if .enabled }}
          - {{ .memberlistService }}
          {{- end }}
          {{- end }}
      {{- end }}

      {{- with .Values.loki.ingester }}
      ingester:
        {{- tpl (. | toYaml) $ | nindent 4 }}
      {{- end }}

      {{- if .Values.loki.commonConfig}}
      common:
      {{- toYaml .Values.loki.commonConfig | nindent 2}}
        storage:
        {{- include "loki.commonStorageConfig" . | nindent 4}}
      {{- end}}

      {{- with .Values.loki.limits_config }}
      limits_config:
        {{- tpl (. | toYaml) $ | nindent 4 }}
      {{- end }}

      runtime_config:
        file: /etc/loki/runtime-config/runtime-config.yaml

      {{- with .Values.chunksCache }}
      {{- if .enabled }}
      chunk_store_config:
        chunk_cache_config:
          default_validity: {{ .defaultValidity }}
          background:
            writeback_goroutines: {{ .writebackParallelism }}
            writeback_buffer: {{ .writebackBuffer }}
            writeback_size_limit: {{ .writebackSizeLimit }}
          memcached:
            batch_size: {{ .batchSize }}
            parallelism: {{ .parallelism }}
          memcached_client:
            addresses: dnssrvnoa+_memcached-client._tcp.{{ template "loki.fullname" $ }}-chunks-cache.{{ $.Release.Namespace }}.svc
            consistent_hash: true
            timeout: {{ .timeout }}
            max_idle_conns: 72
      {{- end }}
      {{- end }}

      {{- if .Values.loki.schemaConfig }}
      schema_config:
      {{- toYaml .Values.loki.schemaConfig | nindent 2}}
      {{- end }}

      {{- if .Values.loki.useTestSchema }}
      schema_config:
      {{- toYaml .Values.loki.testSchemaConfig | nindent 2}}
      {{- end }}

      {{- if .Values.ruler.enabled }}
      {{ include "loki.rulerConfig" . }}
      {{- end }}

      {{- if or .Values.tableManager.retention_deletes_enabled .Values.tableManager.retention_period }}
      table_manager:
        retention_deletes_enabled: {{ .Values.tableManager.retention_deletes_enabled }}
        retention_period: {{ .Values.tableManager.retention_period }}
      {{- end }}

      query_range:
        align_queries_with_step: true
        {{- with .Values.loki.query_range }}
        {{- tpl (. | toYaml) $ | nindent 2 }}
        {{- end }}
        {{- if .Values.resultsCache.enabled }}
        {{- with .Values.resultsCache }}
        cache_results: true
        results_cache:
          cache:
            default_validity: {{ .defaultValidity }}
            background:
              writeback_goroutines: {{ .writebackParallelism }}
              writeback_buffer: {{ .writebackBuffer }}
              writeback_size_limit: {{ .writebackSizeLimit }}
            memcached_client:
              consistent_hash: true
              addresses: dnssrvnoa+_memcached-client._tcp.{{ template "loki.fullname" $ }}-results-cache.{{ $.Release.Namespace }}.svc
              timeout: {{ .timeout }}
              update_interval: 1m
        {{- end }}
        {{- end }}

      {{- with .Values.loki.storage_config }}
      storage_config:
        {{- tpl (. | toYaml) $ | nindent 4 }}
      {{- end }}

      {{- with .Values.loki.query_scheduler }}
      query_scheduler:
        {{- tpl (. | toYaml) $ | nindent 4 }}
      {{- end }}

      {{- with .Values.loki.compactor }}
      compactor:
        {{- tpl (. | toYaml) $ | nindent 4 }}
      {{- end }}

      {{- with .Values.loki.analytics }}
      analytics:
        {{- tpl (. | toYaml) $ | nindent 4 }}
      {{- end }}

      {{- with .Values.loki.querier }}
      querier:
        {{- tpl (. | toYaml) $ | nindent 4 }}
      {{- end }}

      {{- with .Values.loki.index_gateway }}
      index_gateway:
        {{- tpl (. | toYaml) $ | nindent 4 }}
      {{- end }}

      {{- with .Values.loki.frontend }}
      frontend:
        {{- tpl (. | toYaml) $ | nindent 4 }}
      {{- end }}

      {{- with .Values.loki.frontend_worker }}
      frontend_worker:
        {{- tpl (. | toYaml) $ | nindent 4 }}
      {{- end }}

      {{- with .Values.loki.distributor }}
      distributor:
        {{- tpl (. | toYaml) $ | nindent 4 }}
      {{- end }}

      tracing:
        enabled: {{ .Values.loki.tracing.enabled }}

      {{- with .Values.loki.bloom_build }}
      bloom_build:
        {{- tpl (. | toYaml) $ | nindent 4 }}
      {{- end }}

      {{- with .Values.loki.bloom_gateway }}
      bloom_gateway:
        {{- tpl (. | toYaml) $ | nindent 4 }}
      {{- end }}
    # Should authentication be enabled
    auth_enabled: false
    # -- memberlist configuration (overrides embedded default)
    memberlistConfig: {}
    # -- Extra memberlist configuration
    extraMemberlistConfig: {}
    # -- Tenants list to be created on nginx htpasswd file, with name and password keys
    tenants: []
    # -- Check https://grafana.com/docs/loki/latest/configuration/#server for more info on the server configuration.
    server:
      http_listen_port: 3100
      grpc_listen_port: 9095
      http_server_read_timeout: 600s
      http_server_write_timeout: 600s
    # -- Limits config
    limits_config:
      reject_old_samples: true
      reject_old_samples_max_age: 168h
      max_cache_freshness_per_query: 10m
      split_queries_by_interval: 15m
      query_timeout: 300s
      volume_enabled: true
    # -- Provides a reloadable runtime configuration file for some specific configuration
    runtimeConfig: {}
    # -- Check https://grafana.com/docs/loki/latest/configuration/#common_config for more info on how to provide a common configuration
    commonConfig:
      path_prefix: /var/loki
      replication_factor: 1
      compactor_address: '{{ include "loki.compactorAddress" . }}'
    # -- Storage config. Providing this will automatically populate all necessary storage configs in the templated config.
    storage:
      # Loki requires a bucket for chunks and the ruler. GEL requires a third bucket for the admin API.
      # Please provide these values if you are using object storage.
      # bucketNames:
      #   chunks: FIXME
      #   ruler: FIXME
      #   admin: FIXME
      type: filesystem
      s3:
        s3: null
        endpoint: null
        region: null
        secretAccessKey: null
        accessKeyId: null
        signatureVersion: null
        s3ForcePathStyle: false
        insecure: false
        http_config: {}
        # -- Check https://grafana.com/docs/loki/latest/configure/#s3_storage_config for more info on how to provide a backoff_config
        backoff_config: {}
        disable_dualstack: false
      gcs:
        chunkBufferSize: 0
        requestTimeout: "0s"
        enableHttp2: true
      azure:
        accountName: null
        accountKey: null
        connectionString: null
        useManagedIdentity: false
        useFederatedToken: false
        userAssignedId: null
        requestTimeout: null
        endpointSuffix: null
        chunkDelimiter: null
      swift:
        auth_version: null
        auth_url: null
        internal: null
        username: null
        user_domain_name: null
        user_domain_id: null
        user_id: null
        password: null
        domain_id: null
        domain_name: null
        project_id: null
        project_name: null
        project_domain_id: null
        project_domain_name: null
        region_name: null
        container_name: null
        max_retries: null
        connect_timeout: null
        request_timeout: null
      filesystem:
        chunks_directory: /var/loki/chunks
        rules_directory: /var/loki/rules
        admin_api_directory: /var/loki/admin
    # -- Configure memcached as an external cache for chunk and results cache. Disabled by default
    # must enable and specify a host for each cache you would like to use.
    memcached:
      chunk_cache:
        enabled: false
        host: ""
        service: "memcached-client"
        batch_size: 256
        parallelism: 10
      results_cache:
        enabled: false
        host: ""
        service: "memcached-client"
        timeout: "500ms"
        default_validity: "12h"
    # -- Check https://grafana.com/docs/loki/latest/configuration/#schema_config for more info on how to configure schemas
    schemaConfig:
      configs:
        - from: 2024-01-01
          store: tsdb
          object_store: filesystem
          schema: v13
          index:
            prefix: index_
            period: 24h
    # -- a real Loki install requires a proper schemaConfig defined above this, however for testing or playing around
    # you can enable useTestSchema
    useTestSchema: false
    testSchemaConfig:
      configs:
        - from: 2024-04-01
          store: tsdb
          object_store: '{{ include "loki.testSchemaObjectStore" . }}'
          schema: v13
          index:
            prefix: index_
            period: 24h
    # -- Check https://grafana.com/docs/loki/latest/configuration/#ruler for more info on configuring ruler
    rulerConfig:
      wal:
        dir: /var/loki/ruler-wal
    # -- Structured loki configuration, takes precedence over `loki.config`, `loki.schemaConfig`, `loki.storageConfig`
    structuredConfig: {}
    # -- Additional query scheduler config
    query_scheduler: {}
    # -- Additional storage config
    storage_config:
      boltdb_shipper:
        index_gateway_client:
          server_address: '{{ include "loki.indexGatewayAddress" . }}'
      tsdb_shipper:
        index_gateway_client:
          server_address: '{{ include "loki.indexGatewayAddress" . }}'
      bloom_shipper:
        working_directory: /var/loki/data/bloomshipper
      hedging:
        at: "250ms"
        max_per_second: 20
        up_to: 3
    # --  Optional compactor configuration
    compactor: {}
    # --  Optional pattern ingester configuration
    pattern_ingester:
      enabled: false
    # --  Optional analytics configuration
    analytics: {}
    # --  Optional querier configuration
    query_range: {}
    # --  Optional querier configuration
    querier: {}
    # --  Optional ingester configuration
    ingester: {}
    # --  Optional index gateway configuration
    index_gateway:
      mode: simple
    frontend:
      scheduler_address: '{{ include "loki.querySchedulerAddress" . }}'
      tail_proxy_url: '{{ include "loki.querierAddress" . }}'
    frontend_worker:
      scheduler_address: '{{ include "loki.querySchedulerAddress" . }}'
    # -- Optional distributor configuration
    distributor: {}
    # -- Enable tracing
    tracing:
      enabled: false
    bloom_build:
      enabled: false
      builder:
        planner_address: '{{ include "loki.bloomPlannerAddress" . }}'
    bloom_gateway:
      enabled: false
      client:
        addresses: '{{ include "loki.bloomGatewayAddresses" . }}'
  ######################################################################################################################
  #
  # Enterprise Loki Configs
  #
  ######################################################################################################################

  # -- Configuration for running Enterprise Loki
  enterprise:
    # Enable enterprise features, license must be provided
    enabled: false
    # Default verion of GEL to deploy
    version: 3.1.1
    # -- Optional name of the GEL cluster, otherwise will use .Release.Name
    # The cluster name must match what is in your GEL license
    cluster_name: null
    # -- Grafana Enterprise Logs license
    # In order to use Grafana Enterprise Logs features, you will need to provide
    # the contents of your Grafana Enterprise Logs license, either by providing the
    # contents of the license.jwt, or the name Kubernetes Secret that contains your
    # license.jwt.
    # To set the license contents, use the flag `--set-file 'enterprise.license.contents=./license.jwt'`
    license:
      contents: "NOTAVALIDLICENSE"
    # -- Set to true when providing an external license
    useExternalLicense: false
    # -- Name of external license secret to use
    externalLicenseName: null
    # -- Name of the external config secret to use
    externalConfigName: ""
    # -- Use GEL gateway, if false will use the default nginx gateway
    gelGateway: true
    # -- If enabled, the correct admin_client storage will be configured. If disabled while running enterprise,
    # make sure auth is set to `type: trust`, or that `auth_enabled` is set to `false`.
    adminApi:
      enabled: true
    # enterprise specific sections of the config.yaml file
    config: |
      {{- if .Values.enterprise.adminApi.enabled }}
      admin_client:
        {{ include "enterprise-logs.adminAPIStorageConfig" . | nindent 2 }}
      {{ end }}
      auth:
        type: {{ .Values.enterprise.adminApi.enabled | ternary "enterprise" "trust" }}
      auth_enabled: {{ .Values.loki.auth_enabled }}
      cluster_name: {{ include "loki.clusterName" . }}
      license:
        path: /etc/loki/license/license.jwt
    image:
      # -- The Docker registry
      registry: docker.io
      # -- Docker image repository
      repository: grafana/enterprise-logs
      # -- Docker image tag
      tag: 3.3.0
      # -- Overrides the image tag with an image digest
      digest: null
      # -- Docker image pull policy
      pullPolicy: IfNotPresent
    adminToken:
      # -- Alternative name for admin token secret, needed by tokengen and provisioner jobs
      secret: null
      # -- Additional namespace to also create the token in. Useful if your Grafana instance
      # is in a different namespace
      additionalNamespaces: []
    # -- Alternative name of the secret to store token for the canary
    canarySecret: null
    # -- Configuration for `tokengen` target
    tokengen:
      # -- Whether the job should be part of the deployment
      enabled: true
      # -- Comma-separated list of Loki modules to load for tokengen
      targetModule: "tokengen"
      # -- Additional CLI arguments for the `tokengen` target
      extraArgs: []
      # -- Additional Kubernetes environment
      env: []
      # -- Additional labels for the `tokengen` Job
      labels: {}
      # -- Additional annotations for the `tokengen` Job
      annotations: {}
      # -- Affinity for tokengen Pods
      affinity: {}
      # -- Node selector for tokengen Pods
      nodeSelector: {}
      # -- Tolerations for tokengen Job
      tolerations: []
      # -- Additional volumes for Pods
      extraVolumes: []
      # -- Additional volume mounts for Pods
      extraVolumeMounts: []
      # -- Run containers as user `enterprise-logs(uid=10001)`
      securityContext:
        runAsNonRoot: true
        runAsGroup: 10001
        runAsUser: 10001
        fsGroup: 10001
      # -- Environment variables from secrets or configmaps to add to the tokengen pods
      extraEnvFrom: []
      # -- The name of the PriorityClass for tokengen Pods
      priorityClassName: ""
    # -- Configuration for `provisioner` target
    provisioner:
      # -- Whether the job should be part of the deployment
      enabled: true
      # -- Name of the secret to store provisioned tokens in
      provisionedSecretPrefix: null
      # -- Additional tenants to be created. Each tenant will get a read and write policy
      # and associated token. Tenant must have a name and a namespace for the secret containting
      # the token to be created in. For example
      # additionalTenants:
      #   - name: loki
      #     secretNamespace: grafana
      additionalTenants: []
      # -- Additional Kubernetes environment
      env: []
      # -- Additional labels for the `provisioner` Job
      labels: {}
      # -- Additional annotations for the `provisioner` Job
      annotations: {}
      # -- Affinity for tokengen Pods
      affinity: {}
      # -- Node selector for tokengen Pods
      nodeSelector: {}
      # -- Tolerations for tokengen Pods
      tolerations: []
      # -- The name of the PriorityClass for provisioner Job
      priorityClassName: null
      # -- Run containers as user `enterprise-logs(uid=10001)`
      securityContext:
        runAsNonRoot: true
        runAsGroup: 10001
        runAsUser: 10001
        fsGroup: 10001
      # -- Provisioner image to Utilize
      image:
        # -- The Docker registry
        registry: docker.io
        # -- Docker image repository
        repository: grafana/enterprise-logs-provisioner
        # -- Overrides the image tag whose default is the chart's appVersion
        tag: null
        # -- Overrides the image tag with an image digest
        digest: null
        # -- Docker image pull policy
        pullPolicy: IfNotPresent
      # -- Volume mounts to add to the provisioner pods
      extraVolumeMounts: []
  # -- kubetclImage is used in the enterprise provisioner and tokengen jobs
  kubectlImage:
    # -- The Docker registry
    registry: docker.io
    # -- Docker image repository
    repository: bitnami/kubectl
    # -- Overrides the image tag whose default is the chart's appVersion
    tag: null
    # -- Overrides the image tag with an image digest
    digest: null
    # -- Docker image pull policy
    pullPolicy: IfNotPresent
  ######################################################################################################################
  #
  # Chart Testing
  #
  ######################################################################################################################

  # -- Section for configuring optional Helm test
  test:
    enabled: false
    # -- Used to directly query the metrics endpoint of the canary for testing, this approach avoids needing prometheus for testing.
    # This in a newer approach to using prometheusAddress such that tests do not have a dependency on prometheus
    canaryServiceAddress: "http://loki-canary:3500/metrics"
    # -- Address of the prometheus server to query for the test. This overrides any value set for canaryServiceAddress.
    # This is kept for backward compatibility and may be removed in future releases. Previous value was 'http://prometheus:9090'
    prometheusAddress: ""
    # -- Number of times to retry the test before failing
    timeout: 1m
    # -- Additional labels for the test pods
    labels: {}
    # -- Additional annotations for test pods
    annotations: {}
    # -- Image to use for loki canary
    image:
      # -- The Docker registry
      registry: docker.io
      # -- Docker image repository
      repository: grafana/loki-helm-test
      # -- Overrides the image tag whose default is the chart's appVersion
      tag: "ewelch-distributed-helm-chart-17db5ee"
      # -- Overrides the image tag with an image digest
      digest: null
      # -- Docker image pull policy
      pullPolicy: IfNotPresent
  # The Loki canary pushes logs to and queries from this loki installation to test
  # that it's working correctly
  lokiCanary:
    enabled: false
    # -- If true, the canary will send directly to Loki via the address configured for verification --
    # -- If false, it will write to stdout and an Agent will be needed to scrape and send the logs --
    push: true
    # -- The name of the label to look for at loki when doing the checks.
    labelname: pod
    # -- Additional annotations for the `loki-canary` Daemonset
    annotations: {}
    # -- Additional labels for each `loki-canary` pod
    podLabels: {}
    service:
      # -- Annotations for loki-canary Service
      annotations: {}
      # -- Additional labels for loki-canary Service
      labels: {}
    # -- Additional CLI arguments for the `loki-canary' command
    extraArgs: []
    # -- Environment variables to add to the canary pods
    extraEnv: []
    # -- Environment variables from secrets or configmaps to add to the canary pods
    extraEnvFrom: []
    # -- Volume mounts to add to the canary pods
    extraVolumeMounts: []
    # -- Volumes to add to the canary pods
    extraVolumes: []
    # -- Resource requests and limits for the canary
    resources: {}
    # -- DNS config for canary pods
    dnsConfig: {}
    # -- Node selector for canary pods
    nodeSelector: {}
    # -- Tolerations for canary pods
    tolerations: []
    # -- The name of the PriorityClass for loki-canary pods
    priorityClassName: null
    # -- Image to use for loki canary
    image:
      # -- The Docker registry
      registry: docker.io
      # -- Docker image repository
      repository: grafana/loki-canary
      # -- Overrides the image tag whose default is the chart's appVersion
      tag: null
      # -- Overrides the image tag with an image digest
      digest: null
      # -- Docker image pull policy
      pullPolicy: IfNotPresent
    # -- Update strategy for the `loki-canary` Daemonset pods
    updateStrategy:
      type: RollingUpdate
      rollingUpdate:
        maxUnavailable: 1
  ######################################################################################################################
  #
  # Service Accounts and Kubernetes RBAC
  #
  ######################################################################################################################
  serviceAccount:
    # -- Specifies whether a ServiceAccount should be created
    create: true
    # -- The name of the ServiceAccount to use.
    # If not set and create is true, a name is generated using the fullname template
    name: null
    # -- Image pull secrets for the service account
    imagePullSecrets: []
    # -- Annotations for the service account
    annotations: {}
    # -- Labels for the service account
    labels: {}
    # -- Set this toggle to false to opt out of automounting API credentials for the service account
    automountServiceAccountToken: true
  # RBAC configuration
  rbac:
    # -- If pspEnabled true, a PodSecurityPolicy is created for K8s that use psp.
    pspEnabled: false
    # -- For OpenShift set pspEnabled to 'false' and sccEnabled to 'true' to use the SecurityContextConstraints.
    sccEnabled: false
    # -- Specify PSP annotations
    # Ref: https://kubernetes.io/docs/reference/access-authn-authz/psp-to-pod-security-standards/#podsecuritypolicy-annotations
    pspAnnotations: {}
    # seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'
    # seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default'
    # apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default'
    # -- Whether to install RBAC in the namespace only or cluster-wide. Useful if you want to watch ConfigMap globally.
    namespaced: false
  ######################################################################################################################
  #
  # Network Policy configuration
  #
  ######################################################################################################################
  networkPolicy:
    # -- Specifies whether Network Policies should be created
    enabled: false
    # -- Specifies whether the policies created will be standard Network Policies (flavor: kubernetes)
    # or Cilium Network Policies (flavor: cilium)
    flavor: kubernetes
    metrics:
      # -- Specifies the Pods which are allowed to access the metrics port.
      # As this is cross-namespace communication, you also need the namespaceSelector.
      podSelector: {}
      # -- Specifies the namespaces which are allowed to access the metrics port
      namespaceSelector: {}
      # -- Specifies specific network CIDRs which are allowed to access the metrics port.
      # In case you use namespaceSelector, you also have to specify your kubelet networks here.
      # The metrics ports are also used for probes.
      cidrs: []
    ingress:
      # -- Specifies the Pods which are allowed to access the http port.
      # As this is cross-namespace communication, you also need the namespaceSelector.
      podSelector: {}
      # -- Specifies the namespaces which are allowed to access the http port
      namespaceSelector: {}
    alertmanager:
      # -- Specify the alertmanager port used for alerting
      port: 9093
      # -- Specifies the alertmanager Pods.
      # As this is cross-namespace communication, you also need the namespaceSelector.
      podSelector: {}
      # -- Specifies the namespace the alertmanager is running in
      namespaceSelector: {}
    externalStorage:
      # -- Specify the port used for external storage, e.g. AWS S3
      ports: []
      # -- Specifies specific network CIDRs you want to limit access to
      cidrs: []
    discovery:
      # -- (int) Specify the port used for discovery
      port: null
      # -- Specifies the Pods labels used for discovery.
      # As this is cross-namespace communication, you also need the namespaceSelector.
      podSelector: {}
      # -- Specifies the namespace the discovery Pods are running in
      namespaceSelector: {}
    egressWorld:
      # -- Enable additional cilium egress rules to external world for write, read and backend.
      enabled: false
    egressKubeApiserver:
      # -- Enable additional cilium egress rules to kube-apiserver for backend.
      enabled: false
  ######################################################################################################################
  #
  # Global memberlist configuration
  #
  ######################################################################################################################

  # Configuration for the memberlist service
  memberlist:
    service:
      publishNotReadyAddresses: false
      annotations: {}
  ######################################################################################################################
  #
  # adminAPI configuration, enterprise only.
  #
  ######################################################################################################################

  # -- Configuration for the `admin-api` target
  adminApi:
    # -- Define the amount of instances
    replicas: 1
    # -- hostAliases to add
    hostAliases: []
    #  - ip: 1.2.3.4
    #    hostnames:
    #      - domain.tld
    # -- Additional CLI arguments for the `admin-api` target
    extraArgs: {}
    # -- Environment variables from secrets or configmaps to add to the admin-api pods
    extraEnvFrom: []
    # -- Additional labels for the `admin-api` Deployment
    labels: {}
    # -- Additional annotations for the `admin-api` Deployment
    annotations: {}
    # -- Additional labels and annotations for the `admin-api` Service
    service:
      labels: {}
      annotations: {}
    # -- Run container as user `enterprise-logs(uid=10001)`
    # `fsGroup` must not be specified, because these security options are applied
    # on container level not on Pod level.
    podSecurityContext:
      runAsNonRoot: true
      runAsGroup: 10001
      runAsUser: 10001
    containerSecurityContext:
      readOnlyRootFilesystem: true
      capabilities:
        drop:
          - ALL
      allowPrivilegeEscalation: false
    # -- Update strategy
    strategy:
      type: RollingUpdate
    # -- Readiness probe
    readinessProbe:
      httpGet:
        path: /ready
        port: http-metrics
      initialDelaySeconds: 45
    # -- Request and limit Kubernetes resources
    # -- Values are defined in small.yaml and large.yaml
    resources: {}
    # -- Configure optional environment variables
    env: []
    # -- Configure optional initContainers
    initContainers: []
    # -- Conifgure optional extraContainers
    extraContainers: []
    # -- Additional volumes for Pods
    extraVolumes: []
    # -- Additional volume mounts for Pods
    extraVolumeMounts: []
    # -- Affinity for admin-api Pods
    affinity: {}
    # -- Node selector for admin-api Pods
    nodeSelector: {}
    # -- Topology Spread Constraints for admin-api pods
    topologySpreadConstraints: []
    # -- Tolerations for admin-api Pods
    tolerations: []
    # -- Grace period to allow the admin-api to shutdown before it is killed
    terminationGracePeriodSeconds: 60
  ######################################################################################################################
  #
  # Gateway and Ingress
  #
  # By default this chart will deploy a Nginx container to act as a gateway which handles routing of traffic
  # and can also do auth.
  #
  # If you would prefer you can optionally disable this and enable using k8s ingress to do the incoming routing.
  #
  ######################################################################################################################

  # Configuration for the gateway
  gateway:
    # -- Specifies whether the gateway should be enabled
    enabled: true
    # -- Number of replicas for the gateway
    replicas: 1
    # -- Default container port
    containerPort: 8080
    # -- Enable logging of 2xx and 3xx HTTP requests
    verboseLogging: true
    autoscaling:
      # -- Enable autoscaling for the gateway
      enabled: false
      # -- Minimum autoscaling replicas for the gateway
      minReplicas: 1
      # -- Maximum autoscaling replicas for the gateway
      maxReplicas: 3
      # -- Target CPU utilisation percentage for the gateway
      targetCPUUtilizationPercentage: 60
      # -- Target memory utilisation percentage for the gateway
      targetMemoryUtilizationPercentage:
      # -- See `kubectl explain deployment.spec.strategy` for more
      # -- ref: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy
      # -- Behavior policies while scaling.
      behavior: {}
      #    scaleUp:
      #     stabilizationWindowSeconds: 300
      #     policies:
      #     - type: Pods
      #       value: 1
      #       periodSeconds: 60
      #    scaleDown:
      #     stabilizationWindowSeconds: 300
      #     policies:
      #     - type: Pods
      #       value: 1
      #       periodSeconds: 180
    deploymentStrategy:
      type: RollingUpdate
    image:
      # -- The Docker registry for the gateway image
      registry: docker.io
      # -- The gateway image repository
      repository: nginxinc/nginx-unprivileged
      # -- The gateway image tag
      tag: 1.27-alpine
      # -- Overrides the gateway image tag with an image digest
      digest: null
      # -- The gateway image pull policy
      pullPolicy: IfNotPresent
    # -- The name of the PriorityClass for gateway pods
    priorityClassName: null
    # -- Annotations for gateway deployment
    annotations: {}
    # -- Annotations for gateway pods
    podAnnotations: {}
    # -- Additional labels for gateway pods
    podLabels: {}
    # -- Additional CLI args for the gateway
    extraArgs: []
    # -- Environment variables to add to the gateway pods
    extraEnv: []
    # -- Environment variables from secrets or configmaps to add to the gateway pods
    extraEnvFrom: []
    # -- Lifecycle for the gateway container
    lifecycle: {}
    # -- Volumes to add to the gateway pods
    extraVolumes: []
    # -- Volume mounts to add to the gateway pods
    extraVolumeMounts: []
    # -- The SecurityContext for gateway containers
    podSecurityContext:
      fsGroup: 101
      runAsGroup: 101
      runAsNonRoot: true
      runAsUser: 101
    # -- The SecurityContext for gateway containers
    containerSecurityContext:
      readOnlyRootFilesystem: true
      capabilities:
        drop:
          - ALL
      allowPrivilegeEscalation: false
    # -- Resource requests and limits for the gateway
    resources: {}
    # -- Containers to add to the gateway pods
    extraContainers: []
    # -- Grace period to allow the gateway to shutdown before it is killed
    terminationGracePeriodSeconds: 30
    # -- Affinity for gateway pods.
    # @default -- Hard node anti-affinity
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app.kubernetes.io/component: gateway
            topologyKey: kubernetes.io/hostname
    # -- DNS config for gateway pods
    dnsConfig: {}
    # -- Node selector for gateway pods
    nodeSelector: {}
    # -- Topology Spread Constraints for gateway pods
    topologySpreadConstraints: []
    # -- Tolerations for gateway pods
    tolerations: []
    # Gateway service configuration
    service:
      # -- Port of the gateway service
      port: 80
      # -- Type of the gateway service
      type: ClusterIP
      # -- ClusterIP of the gateway service
      clusterIP: null
      # -- (int) Node port if service type is NodePort
      nodePort: null
      # -- Load balancer IPO address if service type is LoadBalancer
      loadBalancerIP: null
      # -- Annotations for the gateway service
      annotations: {}
      # -- Labels for gateway service
      labels: {}
    # Gateway ingress configuration
    ingress:
      # -- Specifies whether an ingress for the gateway should be created
      enabled: false
      # -- Ingress Class Name. MAY be required for Kubernetes versions >= 1.18
      ingressClassName: ""
      # -- Annotations for the gateway ingress
      annotations: {}
      # -- Labels for the gateway ingress
      labels: {}
      # -- Hosts configuration for the gateway ingress, passed through the `tpl` function to allow templating
      hosts:
        - host: gateway.loki.example.com
          paths:
            - path: /
              # -- pathType (e.g. ImplementationSpecific, Prefix, .. etc.) might also be required by some Ingress Controllers
              # pathType: Prefix
      # -- TLS configuration for the gateway ingress. Hosts passed through the `tpl` function to allow templating
      tls:
        - secretName: loki-gateway-tls
          hosts:
            - gateway.loki.example.com
    # Basic auth configuration
    basicAuth:
      # -- Enables basic authentication for the gateway
      enabled: false
      # -- The basic auth username for the gateway
      username: null
      # -- The basic auth password for the gateway
      password: null
      # -- Uses the specified users from the `loki.tenants` list to create the htpasswd file.
      # if `loki.tenants` is not set, the `gateway.basicAuth.username` and `gateway.basicAuth.password` are used.
      # The value is templated using `tpl`. Override this to use a custom htpasswd, e.g. in case the default causes
      # high CPU load.
      # @default -- Either `loki.tenants` or `gateway.basicAuth.username` and `gateway.basicAuth.password`.
      htpasswd: >-
        {{ if .Values.loki.tenants }}


          {{- range $t := .Values.loki.tenants }}
        {{ htpasswd (required "All tenants must have a 'name' set" $t.name) (required "All tenants must have a 'password' set" $t.password) }}


          {{- end }}
        {{ else }} {{ htpasswd (required "'gateway.basicAuth.username' is required" .Values.gateway.basicAuth.username) (required "'gateway.basicAuth.password' is required" .Values.gateway.basicAuth.password) }} {{ end }}
      # -- Existing basic auth secret to use. Must contain '.htpasswd'
      existingSecret: null
    # Configures the readiness probe for the gateway
    readinessProbe:
      httpGet:
        path: /
        port: http-metrics
      initialDelaySeconds: 15
      timeoutSeconds: 1
    nginxConfig:
      # -- Which schema to be used when building URLs. Can be 'http' or 'https'.
      schema: http
      # -- Enable listener for IPv6, disable on IPv4-only systems
      enableIPv6: false
      # -- NGINX log format
      logFormat: |-
        main '$remote_addr - $remote_user [$time_local]  $status '
                '"$request" $body_bytes_sent "$http_referer" '
                '"$http_user_agent" "$http_x_forwarded_for"';
      # -- Allows appending custom configuration to the server block
      serverSnippet: ""
      # -- Allows appending custom configuration to the http block, passed through the `tpl` function to allow templating
      httpSnippet: >-
        {{ if .Values.loki.tenants }}proxy_set_header X-Scope-OrgID $remote_user;{{ end }}
      # -- Allows customizing the `client_max_body_size` directive
      clientMaxBodySize: 4M
      # -- Whether ssl should be appended to the listen directive of the server block or not.
      ssl: false
      # -- Override Read URL
      customReadUrl: null
      # -- Override Write URL
      customWriteUrl: null
      # -- Override Backend URL
      customBackendUrl: null
      # -- Allows overriding the DNS resolver address nginx will use.
      resolver: ""
      # -- Config file contents for Nginx. Passed through the `tpl` function to allow templating
      # @default -- See values.yaml
      file: |
        {{- include "loki.nginxFile" . | indent 2 -}}
  # -- If running enterprise and using the default enterprise gateway, configs go here.
  enterpriseGateway:
    # -- Define the amount of instances
    replicas: 1
    # -- hostAliases to add
    hostAliases: []
    #  - ip: 1.2.3.4
    #    hostnames:
    #      - domain.tld
    # -- Additional CLI arguments for the `gateway` target
    extraArgs: {}
    # -- Environment variables from secrets or configmaps to add to the enterprise gateway pods
    extraEnvFrom: []
    # -- Additional labels for the `gateway` Pod
    labels: {}
    # -- Additional annotations for the `gateway` Pod
    annotations: {}
    # -- Additional labels and annotations for the `gateway` Service
    # -- Service overriding service type
    service:
      type: ClusterIP
      labels: {}
      annotations: {}
    # -- Run container as user `enterprise-logs(uid=10001)`
    podSecurityContext:
      runAsNonRoot: true
      runAsGroup: 10001
      runAsUser: 10001
      fsGroup: 10001
    containerSecurityContext:
      readOnlyRootFilesystem: true
      capabilities:
        drop:
          - ALL
      allowPrivilegeEscalation: false
    # -- If you want to use your own proxy URLs, set this to false.
    useDefaultProxyURLs: true
    # -- update strategy
    strategy:
      type: RollingUpdate
    # -- Readiness probe
    readinessProbe:
      httpGet:
        path: /ready
        port: http-metrics
      initialDelaySeconds: 45
    # -- Request and limit Kubernetes resources
    # -- Values are defined in small.yaml and large.yaml
    resources: {}
    # -- Configure optional environment variables
    env: []
    # -- Configure optional initContainers
    initContainers: []
    # -- Conifgure optional extraContainers
    extraContainers: []
    # -- Additional volumes for Pods
    extraVolumes: []
    # -- Additional volume mounts for Pods
    extraVolumeMounts: []
    # -- Affinity for gateway Pods
    affinity: {}
    # -- Node selector for gateway Pods
    nodeSelector: {}
    # -- Topology Spread Constraints for enterprise-gateway pods
    topologySpreadConstraints: []
    # -- Tolerations for gateway Pods
    tolerations: []
    # -- Grace period to allow the gateway to shutdown before it is killed
    terminationGracePeriodSeconds: 60
  # -- Ingress configuration Use either this ingress or the gateway, but not both at once.
  # If you enable this, make sure to disable the gateway.
  # You'll need to supply authn configuration for your ingress controller.
  ingress:
    enabled: false
    ingressClassName: ""
    annotations: {}
    #    nginx.ingress.kubernetes.io/auth-type: basic
    #    nginx.ingress.kubernetes.io/auth-secret: loki-distributed-basic-auth
    #    nginx.ingress.kubernetes.io/auth-secret-type: auth-map
    #    nginx.ingress.kubernetes.io/configuration-snippet: |
    #      proxy_set_header X-Scope-OrgID $remote_user;
    labels: {}
    #    blackbox.monitoring.exclude: "true"
    paths:
      # -- Paths that are exposed by Loki Distributor.
      # If deployment mode is Distributed, the requests are forwarded to the service: `{{"loki.distributorFullname"}}`.
      # If deployment mode is SimpleScalable, the requests are forwarded to write k8s service: `{{"loki.writeFullname"}}`.
      # If deployment mode is SingleBinary, the requests are forwarded to the central/single k8s service: `{{"loki.singleBinaryFullname"}}`
      distributor:
        - /api/prom/push
        - /loki/api/v1/push
        - /otlp/v1/logs
      # -- Paths that are exposed by Loki Query Frontend.
      # If deployment mode is Distributed, the requests are forwarded to the service: `{{"loki.queryFrontendFullname"}}`.
      # If deployment mode is SimpleScalable, the requests are forwarded to write k8s service: `{{"loki.readFullname"}}`.
      # If deployment mode is SingleBinary, the requests are forwarded to the central/single k8s service: `{{"loki.singleBinaryFullname"}}`
      queryFrontend:
        - /api/prom/query
        # this path covers labels and labelValues endpoints
        - /api/prom/label
        - /api/prom/series
        - /api/prom/tail
        - /loki/api/v1/query
        - /loki/api/v1/query_range
        - /loki/api/v1/tail
        # this path covers labels and labelValues endpoints
        - /loki/api/v1/label
        - /loki/api/v1/labels
        - /loki/api/v1/series
        - /loki/api/v1/index/stats
        - /loki/api/v1/index/volume
        - /loki/api/v1/index/volume_range
        - /loki/api/v1/format_query
        - /loki/api/v1/detected_field
        - /loki/api/v1/detected_fields
        - /loki/api/v1/detected_labels
        - /loki/api/v1/patterns
      # -- Paths that are exposed by Loki Ruler.
      # If deployment mode is Distributed, the requests are forwarded to the service: `{{"loki.rulerFullname"}}`.
      # If deployment mode is SimpleScalable, the requests are forwarded to k8s service: `{{"loki.backendFullname"}}`.
      # If deployment mode is SimpleScalable but `read.legacyReadTarget` is `true`, the requests are forwarded to k8s service: `{{"loki.readFullname"}}`.
      # If deployment mode is SingleBinary, the requests are forwarded to the central/single k8s service: `{{"loki.singleBinaryFullname"}}`
      ruler:
        - /api/prom/rules
        - /api/prom/api/v1/rules
        - /api/prom/api/v1/alerts
        - /loki/api/v1/rules
        - /prometheus/api/v1/rules
        - /prometheus/api/v1/alerts
    # -- Hosts configuration for the ingress, passed through the `tpl` function to allow templating
    hosts:
      - loki.example.com
    # -- TLS configuration for the ingress. Hosts passed through the `tpl` function to allow templating
    tls: []
  #    - hosts:
  #       - loki.example.com
  #      secretName: loki-distributed-tls

  ######################################################################################################################
  #
  # Migration
  #
  ######################################################################################################################

  # -- Options that may be necessary when performing a migration from another helm chart
  migrate:
    # -- When migrating from a distributed chart like loki-distributed or enterprise-logs
    fromDistributed:
      # -- Set to true if migrating from a distributed helm chart
      enabled: false
      # -- If migrating from a distributed service, provide the distributed deployment's
      # memberlist service DNS so the new deployment can join its ring.
      memberlistService: ""
  ######################################################################################################################
  #
  # Single Binary Deployment
  #
  # For small Loki installations up to a few 10's of GB per day, or for testing and development.
  #
  ######################################################################################################################

  # Configuration for the single binary node(s)
  singleBinary:
    # -- Number of replicas for the single binary
    replicas: 1
    autoscaling:
      # -- Enable autoscaling
      enabled: false
      # -- Minimum autoscaling replicas for the single binary
      minReplicas: 1
      # -- Maximum autoscaling replicas for the single binary
      maxReplicas: 3
      # -- Target CPU utilisation percentage for the single binary
      targetCPUUtilizationPercentage: 60
      # -- Target memory utilisation percentage for the single binary
      targetMemoryUtilizationPercentage:
    image:
      # -- The Docker registry for the single binary image. Overrides `loki.image.registry`
      registry: null
      # -- Docker image repository for the single binary image. Overrides `loki.image.repository`
      repository: null
      # -- Docker image tag for the single binary image. Overrides `loki.image.tag`
      tag: null
    # -- The name of the PriorityClass for single binary pods
    priorityClassName: null
    # -- Annotations for single binary StatefulSet
    annotations: {}
    # -- Annotations for single binary pods
    podAnnotations: {}
    # -- Additional labels for each `single binary` pod
    podLabels: {}
    # -- Additional selector labels for each `single binary` pod
    selectorLabels: {}
    service:
      # -- Annotations for single binary Service
      annotations: {}
      # -- Additional labels for single binary Service
      labels: {}
    # -- Comma-separated list of Loki modules to load for the single binary
    targetModule: "all"
    # -- Labels for single binary service
    extraArgs: []
    # -- Environment variables to add to the single binary pods
    extraEnv: []
    # -- Environment variables from secrets or configmaps to add to the single binary pods
    extraEnvFrom: []
    # -- Extra containers to add to the single binary loki pod
    extraContainers: []
    # -- Init containers to add to the single binary pods
    initContainers: []
    # -- Volume mounts to add to the single binary pods
    extraVolumeMounts: []
    # -- Volumes to add to the single binary pods
    extraVolumes: []
    # -- Resource requests and limits for the single binary
    resources: {}
    # -- Grace period to allow the single binary to shutdown before it is killed
    terminationGracePeriodSeconds: 30
    # -- Affinity for single binary pods.
    # @default -- Hard node anti-affinity
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app.kubernetes.io/component: single-binary
            topologyKey: kubernetes.io/hostname
    # -- DNS config for single binary pods
    dnsConfig: {}
    # -- Node selector for single binary pods
    nodeSelector: {}
    # -- Tolerations for single binary pods
    tolerations: []
    persistence:
      # -- Enable StatefulSetAutoDeletePVC feature
      enableStatefulSetAutoDeletePVC: true
      # -- Enable persistent disk
      enabled: true
      # -- Size of persistent disk
      size: 10Gi
      # -- Storage class to be used.
      # If defined, storageClassName: <storageClass>.
      # If set to "-", storageClassName: "", which disables dynamic provisioning.
      # If empty or set to null, no storageClassName spec is
      # set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).
      storageClass: null
      # -- Selector for persistent disk
      selector: null
      # -- Annotations for volume claim
      annotations: {}
  ######################################################################################################################
  #
  # Simple Scalable Deployment (SSD) Mode
  #
  # For small to medium size Loki deployments up to around 1 TB/day, this is the default mode for this helm chart
  #
  ######################################################################################################################

  # Configuration for the write pod(s)
  write:
    # -- Number of replicas for the write
    replicas: 0
    autoscaling:
      # -- Enable autoscaling for the write.
      enabled: false
      # -- Minimum autoscaling replicas for the write.
      minReplicas: 2
      # -- Maximum autoscaling replicas for the write.
      maxReplicas: 6
      # -- Target CPU utilisation percentage for the write.
      targetCPUUtilizationPercentage: 60
      # -- Target memory utilization percentage for the write.
      targetMemoryUtilizationPercentage:
      # -- Behavior policies while scaling.
      behavior:
        # -- see https://github.com/grafana/loki/blob/main/docs/sources/operations/storage/wal.md#how-to-scale-updown for scaledown details
        scaleUp:
          policies:
            - type: Pods
              value: 1
              periodSeconds: 900
        scaleDown:
          policies:
            - type: Pods
              value: 1
              periodSeconds: 1800
          stabilizationWindowSeconds: 3600
    image:
      # -- The Docker registry for the write image. Overrides `loki.image.registry`
      registry: null
      # -- Docker image repository for the write image. Overrides `loki.image.repository`
      repository: null
      # -- Docker image tag for the write image. Overrides `loki.image.tag`
      tag: null
    # -- The name of the PriorityClass for write pods
    priorityClassName: null
    # -- Annotations for write StatefulSet
    annotations: {}
    # -- Annotations for write pods
    podAnnotations: {}
    # -- Additional labels for each `write` pod
    podLabels: {}
    # -- Additional selector labels for each `write` pod
    selectorLabels: {}
    service:
      # -- Annotations for write Service
      annotations: {}
      # -- Additional labels for write Service
      labels: {}
    # -- Comma-separated list of Loki modules to load for the write
    targetModule: "write"
    # -- Additional CLI args for the write
    extraArgs: []
    # -- Environment variables to add to the write pods
    extraEnv: []
    # -- Environment variables from secrets or configmaps to add to the write pods
    extraEnvFrom: []
    # -- Lifecycle for the write container
    lifecycle: {}
    # -- The default /flush_shutdown preStop hook is recommended as part of the ingester
    # scaledown process so it's added to the template by default when autoscaling is enabled,
    # but it's disabled to optimize rolling restarts in instances that will never be scaled
    # down or when using chunks storage with WAL disabled.
    # https://github.com/grafana/loki/blob/main/docs/sources/operations/storage/wal.md#how-to-scale-updown
    # -- Init containers to add to the write pods
    initContainers: []
    # -- Containers to add to the write pods
    extraContainers: []
    # -- Volume mounts to add to the write pods
    extraVolumeMounts: []
    # -- Volumes to add to the write pods
    extraVolumes: []
    # -- volumeClaimTemplates to add to StatefulSet
    extraVolumeClaimTemplates: []
    # -- Resource requests and limits for the write
    resources: {}
    # -- Grace period to allow the write to shutdown before it is killed. Especially for the ingester,
    # this must be increased. It must be long enough so writes can be gracefully shutdown flushing/transferring
    # all data and to successfully leave the member ring on shutdown.
    terminationGracePeriodSeconds: 300
    # -- Affinity for write pods.
    # @default -- Hard node anti-affinity
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app.kubernetes.io/component: write
            topologyKey: kubernetes.io/hostname
    # -- DNS config for write pods
    dnsConfig: {}
    # -- Node selector for write pods
    nodeSelector: {}
    # -- Topology Spread Constraints for write pods
    topologySpreadConstraints: []
    # -- Tolerations for write pods
    tolerations: []
    # -- The default is to deploy all pods in parallel.
    podManagementPolicy: "Parallel"
    persistence:
      # -- Enable volume claims in pod spec
      volumeClaimsEnabled: true
      # -- Parameters used for the `data` volume when volumeClaimEnabled if false
      dataVolumeParameters:
        emptyDir: {}
      # -- Enable StatefulSetAutoDeletePVC feature
      enableStatefulSetAutoDeletePVC: false
      # -- Size of persistent disk
      size: 10Gi
      # -- Storage class to be used.
      # If defined, storageClassName: <storageClass>.
      # If set to "-", storageClassName: "", which disables dynamic provisioning.
      # If empty or set to null, no storageClassName spec is
      # set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).
      storageClass: null
      # -- Selector for persistent disk
      selector: null
      # -- Annotations for volume claim
      annotations: {}
  # --  Configuration for the read pod(s)
  read:
    # -- Number of replicas for the read
    replicas: 0
    autoscaling:
      # -- Enable autoscaling for the read, this is only used if `queryIndex.enabled: true`
      enabled: false
      # -- Minimum autoscaling replicas for the read
      minReplicas: 2
      # -- Maximum autoscaling replicas for the read
      maxReplicas: 6
      # -- Target CPU utilisation percentage for the read
      targetCPUUtilizationPercentage: 60
      # -- Target memory utilisation percentage for the read
      targetMemoryUtilizationPercentage:
      # -- Behavior policies while scaling.
      behavior: {}
      #  scaleUp:
      #   stabilizationWindowSeconds: 300
      #   policies:
      #   - type: Pods
      #     value: 1
      #     periodSeconds: 60
      #  scaleDown:
      #   stabilizationWindowSeconds: 300
      #   policies:
      #   - type: Pods
      #     value: 1
      #     periodSeconds: 180
    image:
      # -- The Docker registry for the read image. Overrides `loki.image.registry`
      registry: null
      # -- Docker image repository for the read image. Overrides `loki.image.repository`
      repository: null
      # -- Docker image tag for the read image. Overrides `loki.image.tag`
      tag: null
    # -- The name of the PriorityClass for read pods
    priorityClassName: null
    # -- Annotations for read deployment
    annotations: {}
    # -- Annotations for read pods
    podAnnotations: {}
    # -- Additional labels for each `read` pod
    podLabels: {}
    # -- Additional selector labels for each `read` pod
    selectorLabels: {}
    service:
      # -- Annotations for read Service
      annotations: {}
      # -- Additional labels for read Service
      labels: {}
    # -- Comma-separated list of Loki modules to load for the read
    targetModule: "read"
    # -- Whether or not to use the 2 target type simple scalable mode (read, write) or the
    # 3 target type (read, write, backend). Legacy refers to the 2 target type, so true will
    # run two targets, false will run 3 targets.
    legacyReadTarget: false
    # -- Additional CLI args for the read
    extraArgs: []
    # -- Containers to add to the read pods
    extraContainers: []
    # -- Environment variables to add to the read pods
    extraEnv: []
    # -- Environment variables from secrets or configmaps to add to the read pods
    extraEnvFrom: []
    # -- Lifecycle for the read container
    lifecycle: {}
    # -- Volume mounts to add to the read pods
    extraVolumeMounts: []
    # -- Volumes to add to the read pods
    extraVolumes: []
    # -- Resource requests and limits for the read
    resources: {}
    # -- Grace period to allow the read to shutdown before it is killed
    terminationGracePeriodSeconds: 30
    # -- Affinity for read pods.
    # @default -- Hard node anti-affinity
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app.kubernetes.io/component: read
            topologyKey: kubernetes.io/hostname
    # -- DNS config for read pods
    dnsConfig: {}
    # -- Node selector for read pods
    nodeSelector: {}
    # -- Topology Spread Constraints for read pods
    topologySpreadConstraints: []
    # -- Tolerations for read pods
    tolerations: []
    # -- The default is to deploy all pods in parallel.
    podManagementPolicy: "Parallel"
    persistence:
      # -- Enable StatefulSetAutoDeletePVC feature
      enableStatefulSetAutoDeletePVC: true
      # -- Size of persistent disk
      size: 10Gi
      # -- Storage class to be used.
      # If defined, storageClassName: <storageClass>.
      # If set to "-", storageClassName: "", which disables dynamic provisioning.
      # If empty or set to null, no storageClassName spec is
      # set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).
      storageClass: null
      # -- Selector for persistent disk
      selector: null
      # -- Annotations for volume claim
      annotations: {}
  # --  Configuration for the backend pod(s)
  backend:
    # -- Number of replicas for the backend
    replicas: 0
    autoscaling:
      # -- Enable autoscaling for the backend.
      enabled: false
      # -- Minimum autoscaling replicas for the backend.
      minReplicas: 3
      # -- Maximum autoscaling replicas for the backend.
      maxReplicas: 6
      # -- Target CPU utilization percentage for the backend.
      targetCPUUtilizationPercentage: 60
      # -- Target memory utilization percentage for the backend.
      targetMemoryUtilizationPercentage:
      # -- Behavior policies while scaling.
      behavior: {}
      #    scaleUp:
      #     stabilizationWindowSeconds: 300
      #     policies:
      #     - type: Pods
      #       value: 1
      #       periodSeconds: 60
      #    scaleDown:
      #     stabilizationWindowSeconds: 300
      #     policies:
      #     - type: Pods
      #       value: 1
      #       periodSeconds: 180
    image:
      # -- The Docker registry for the backend image. Overrides `loki.image.registry`
      registry: null
      # -- Docker image repository for the backend image. Overrides `loki.image.repository`
      repository: null
      # -- Docker image tag for the backend image. Overrides `loki.image.tag`
      tag: null
    # -- The name of the PriorityClass for backend pods
    priorityClassName: null
    # -- Annotations for backend StatefulSet
    annotations: {}
    # -- Annotations for backend pods
    podAnnotations: {}
    # -- Additional labels for each `backend` pod
    podLabels: {}
    # -- Additional selector labels for each `backend` pod
    selectorLabels: {}
    service:
      # -- Annotations for backend Service
      annotations: {}
      # -- Additional labels for backend Service
      labels: {}
    # -- Comma-separated list of Loki modules to load for the backend
    targetModule: "backend"
    # -- Additional CLI args for the backend
    extraArgs: []
    # -- Environment variables to add to the backend pods
    extraEnv: []
    # -- Environment variables from secrets or configmaps to add to the backend pods
    extraEnvFrom: []
    # -- Init containers to add to the backend pods
    initContainers: []
    # -- Volume mounts to add to the backend pods
    extraVolumeMounts: []
    # -- Volumes to add to the backend pods
    extraVolumes: []
    # -- Resource requests and limits for the backend
    resources: {}
    # -- Grace period to allow the backend to shutdown before it is killed. Especially for the ingester,
    # this must be increased. It must be long enough so backends can be gracefully shutdown flushing/transferring
    # all data and to successfully leave the member ring on shutdown.
    terminationGracePeriodSeconds: 300
    # -- Affinity for backend pods.
    # @default -- Hard node anti-affinity
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app.kubernetes.io/component: backend
            topologyKey: kubernetes.io/hostname
    # -- DNS config for backend pods
    dnsConfig: {}
    # -- Node selector for backend pods
    nodeSelector: {}
    # -- Topology Spread Constraints for backend pods
    topologySpreadConstraints: []
    # -- Tolerations for backend pods
    tolerations: []
    # -- The default is to deploy all pods in parallel.
    podManagementPolicy: "Parallel"
    persistence:
      # -- Enable volume claims in pod spec
      volumeClaimsEnabled: true
      # -- Parameters used for the `data` volume when volumeClaimEnabled if false
      dataVolumeParameters:
        emptyDir: {}
      # -- Enable StatefulSetAutoDeletePVC feature
      enableStatefulSetAutoDeletePVC: true
      # -- Size of persistent disk
      size: 10Gi
      # -- Storage class to be used.
      # If defined, storageClassName: <storageClass>.
      # If set to "-", storageClassName: "", which disables dynamic provisioning.
      # If empty or set to null, no storageClassName spec is
      # set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).
      storageClass: null
      # -- Selector for persistent disk
      selector: null
      # -- Annotations for volume claim
      annotations: {}
  ######################################################################################################################
  #
  # Microservices Mode
  #
  # For large Loki deployments ingesting more than 1 TB/day
  #
  ######################################################################################################################

  # -- Configuration for the ingester
  ingester:
    # -- Number of replicas for the ingester, when zoneAwareReplication.enabled is true, the total
    # number of replicas will match this value with each zone having 1/3rd of the total replicas.
    replicas: 0
    # -- hostAliases to add
    hostAliases: []
    #  - ip: 1.2.3.4
    #    hostnames:
    #      - domain.tld
    autoscaling:
      # -- Enable autoscaling for the ingester
      enabled: false
      # -- Minimum autoscaling replicas for the ingester
      minReplicas: 1
      # -- Maximum autoscaling replicas for the ingester
      maxReplicas: 3
      # -- Target CPU utilisation percentage for the ingester
      targetCPUUtilizationPercentage: 60
      # -- Target memory utilisation percentage for the ingester
      targetMemoryUtilizationPercentage: null
      # -- Allows one to define custom metrics using the HPA/v2 schema (for example, Pods, Object or External metrics)
      customMetrics: []
      # - type: Pods
      #   pods:
      #     metric:
      #       name: loki_lines_total
      #     target:
      #       type: AverageValue
      #       averageValue: 10k
      behavior:
        # -- Enable autoscaling behaviours
        enabled: false
        # -- define scale down policies, must conform to HPAScalingRules
        scaleDown: {}
        # -- define scale up policies, must conform to HPAScalingRules
        scaleUp: {}
    image:
      # -- The Docker registry for the ingester image. Overrides `loki.image.registry`
      registry: null
      # -- Docker image repository for the ingester image. Overrides `loki.image.repository`
      repository: null
      # -- Docker image tag for the ingester image. Overrides `loki.image.tag`
      tag: null
    # -- Command to execute instead of defined in Docker image
    command: null
    priorityClassName: null
    # -- Labels for ingester pods
    podLabels: {}
    # -- Annotations for ingester pods
    podAnnotations: {}
    # -- The name of the PriorityClass for ingester pods
    # -- Labels for ingestor service
    serviceLabels: {}
    # -- Annotations for ingestor service
    serviceAnnotations: {}
    # -- Additional CLI args for the ingester
    extraArgs: []
    # -- Environment variables to add to the ingester pods
    extraEnv: []
    # -- Environment variables from secrets or configmaps to add to the ingester pods
    extraEnvFrom: []
    # -- Volume mounts to add to the ingester pods
    extraVolumeMounts: []
    # -- Volumes to add to the ingester pods
    extraVolumes: []
    # -- Resource requests and limits for the ingester
    resources: {}
    # -- Containers to add to the ingester pods
    extraContainers: []
    # -- Init containers to add to the ingester pods
    initContainers: []
    # -- Grace period to allow the ingester to shutdown before it is killed. Especially for the ingestor,
    # this must be increased. It must be long enough so ingesters can be gracefully shutdown flushing/transferring
    # all data and to successfully leave the member ring on shutdown.
    terminationGracePeriodSeconds: 300
    # -- Lifecycle for the ingester container
    lifecycle: {}
    # -- topologySpread for ingester pods.
    # @default -- Defaults to allow skew no more than 1 node
    topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            app.kubernetes.io/component: ingester
    # -- Affinity for ingester pods. Ignored if zoneAwareReplication is enabled.
    # @default -- Hard node anti-affinity
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app.kubernetes.io/component: ingester
            topologyKey: kubernetes.io/hostname
    # -- Pod Disruption Budget maxUnavailable
    maxUnavailable: 1
    # -- Node selector for ingester pods
    nodeSelector: {}
    # -- Tolerations for ingester pods
    tolerations: []
    # -- readiness probe settings for ingester pods. If empty, use `loki.readinessProbe`
    readinessProbe: {}
    # -- liveness probe settings for ingester pods. If empty use `loki.livenessProbe`
    livenessProbe: {}
    # -- UpdateStrategy for the ingester StatefulSets.
    updateStrategy:
      # -- One of  'OnDelete' or 'RollingUpdate'
      type: RollingUpdate
      # -- Optional for updateStrategy.type=RollingUpdate. See [Partitioned rolling updates](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#partitions) in the StatefulSet docs for details.
      # rollingUpdate:
      #   partition: 0
    persistence:
      # -- Enable creating PVCs which is required when using boltdb-shipper
      enabled: false
      # -- Use emptyDir with ramdisk for storage. **Please note that all data in ingester will be lost on pod restart**
      inMemory: false
      # -- List of the ingester PVCs
      # @notationType -- list
      claims:
        - name: data
          size: 10Gi
          #   -- Storage class to be used.
          #   If defined, storageClassName: <storageClass>.
          #   If set to "-", storageClassName: "", which disables dynamic provisioning.
          #   If empty or set to null, no storageClassName spec is
          #   set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).
          storageClass: null
          # - name: wal
          #   size: 150Gi
      # -- Enable StatefulSetAutoDeletePVC feature
      enableStatefulSetAutoDeletePVC: false
      whenDeleted: Retain
      whenScaled: Retain
    # -- Adds the appProtocol field to the ingester service. This allows ingester to work with istio protocol selection.
    appProtocol:
      # -- Set the optional grpc service protocol. Ex: "grpc", "http2" or "https"
      grpc: ""
    # -- Enabling zone awareness on ingesters will create 3 statefulests where all writes will send a replica to each zone.
    # This is primarily intended to accelerate rollout operations by allowing for multiple ingesters within a single
    # zone to be shutdown and restart simultaneously (the remaining 2 zones will be guaranteed to have at least one copy
    # of the data).
    # Note: This can be used to run Loki over multiple cloud provider availability zones however this is not currently
    # recommended as Loki is not optimized for this and cross zone network traffic costs can become extremely high
    # extremely quickly. Even with zone awareness enabled, it is recommended to run Loki in a single availability zone.
    zoneAwareReplication:
      # -- Enable zone awareness.
      enabled: true
      # -- The percent of replicas in each zone that will be restarted at once. In a value of 0-100
      maxUnavailablePct: 33
      # -- zoneA configuration
      zoneA:
        # -- optionally define a node selector for this zone
        nodeSelector: null
        # -- optionally define extra affinity rules, by default different zones are not allowed to schedule on the same host
        extraAffinity: {}
        # -- Specific annotations to add to zone A statefulset
        annotations: {}
        # -- Specific annotations to add to zone A pods
        podAnnotations: {}
      zoneB:
        # -- optionally define a node selector for this zone
        nodeSelector: null
        # -- optionally define extra affinity rules, by default different zones are not allowed to schedule on the same host
        extraAffinity: {}
        # -- Specific annotations to add to zone B statefulset
        annotations: {}
        # -- Specific annotations to add to zone B pods
        podAnnotations: {}
      zoneC:
        # -- optionally define a node selector for this zone
        nodeSelector: null
        # -- optionally define extra affinity rules, by default different zones are not allowed to schedule on the same host
        extraAffinity: {}
        # -- Specific annotations to add to zone C statefulset
        annotations: {}
        # -- Specific annotations to add to zone C pods
        podAnnotations: {}
      # -- The migration block allows migrating non zone aware ingesters to zone aware ingesters.
      migration:
        enabled: false
        excludeDefaultZone: false
        readPath: false
        writePath: false

    # optionally allow adding arbitrary prefix to the ingester rollout-group label
    rolloutGroupPrefix: null
    # optionally allow adding 'loki-' prefix to ingester name label
    addIngesterNamePrefix: false

  # --  Configuration for the distributor
  distributor:
    # -- Number of replicas for the distributor
    replicas: 0
    # -- hostAliases to add
    hostAliases: []
    #  - ip: 1.2.3.4
    #    hostnames:
    #      - domain.tld
    autoscaling:
      # -- Enable autoscaling for the distributor
      enabled: false
      # -- Minimum autoscaling replicas for the distributor
      minReplicas: 1
      # -- Maximum autoscaling replicas for the distributor
      maxReplicas: 3
      # -- Target CPU utilisation percentage for the distributor
      targetCPUUtilizationPercentage: 60
      # -- Target memory utilisation percentage for the distributor
      targetMemoryUtilizationPercentage: null
      # -- Allows one to define custom metrics using the HPA/v2 schema (for example, Pods, Object or External metrics)
      customMetrics: []
      # - type: Pods
      #   pods:
      #     metric:
      #       name: loki_lines_total
      #     target:
      #       type: AverageValue
      #       averageValue: 10k
      behavior:
        # -- Enable autoscaling behaviours
        enabled: false
        # -- define scale down policies, must conform to HPAScalingRules
        scaleDown: {}
        # -- define scale up policies, must conform to HPAScalingRules
        scaleUp: {}
    image:
      # -- The Docker registry for the distributor image. Overrides `loki.image.registry`
      registry: null
      # -- Docker image repository for the distributor image. Overrides `loki.image.repository`
      repository: null
      # -- Docker image tag for the distributor image. Overrides `loki.image.tag`
      tag: null
    # -- Command to execute instead of defined in Docker image
    command: null
    # -- The name of the PriorityClass for distributor pods
    priorityClassName: null
    # -- Labels for distributor pods
    podLabels: {}
    # -- Annotations for distributor pods
    podAnnotations: {}
    # -- Labels for distributor service
    serviceLabels: {}
    # -- Annotations for distributor service
    serviceAnnotations: {}
    # -- Additional CLI args for the distributor
    extraArgs: []
    # -- Environment variables to add to the distributor pods
    extraEnv: []
    # -- Environment variables from secrets or configmaps to add to the distributor pods
    extraEnvFrom: []
    # -- Volume mounts to add to the distributor pods
    extraVolumeMounts: []
    # -- Volumes to add to the distributor pods
    extraVolumes: []
    # -- Resource requests and limits for the distributor
    resources: {}
    # -- Containers to add to the distributor pods
    extraContainers: []
    # -- Grace period to allow the distributor to shutdown before it is killed
    terminationGracePeriodSeconds: 30
    # -- Affinity for distributor pods.
    # @default -- Hard node anti-affinity
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app.kubernetes.io/component: distributor
            topologyKey: kubernetes.io/hostname
    # -- Pod Disruption Budget maxUnavailable
    maxUnavailable: null
    # -- Max Surge for distributor pods
    maxSurge: 0
    # -- Node selector for distributor pods
    nodeSelector: {}
    # -- Topology Spread Constraints for distributor pods
    topologySpreadConstraints: []
    # -- Tolerations for distributor pods
    tolerations: []
    # -- Adds the appProtocol field to the distributor service. This allows distributor to work with istio protocol selection.
    appProtocol:
      # -- Set the optional grpc service protocol. Ex: "grpc", "http2" or "https"
      grpc: ""
  # --  Configuration for the querier
  querier:
    # -- Number of replicas for the querier
    replicas: 0
    # -- hostAliases to add
    hostAliases: []
    #  - ip: 1.2.3.4
    #    hostnames:
    #      - domain.tld
    autoscaling:
      # -- Enable autoscaling for the querier, this is only used if `indexGateway.enabled: true`
      enabled: false
      # -- Minimum autoscaling replicas for the querier
      minReplicas: 1
      # -- Maximum autoscaling replicas for the querier
      maxReplicas: 3
      # -- Target CPU utilisation percentage for the querier
      targetCPUUtilizationPercentage: 60
      # -- Target memory utilisation percentage for the querier
      targetMemoryUtilizationPercentage: null
      # -- Allows one to define custom metrics using the HPA/v2 schema (for example, Pods, Object or External metrics)
      customMetrics: []
      # - type: External
      #   external:
      #     metric:
      #       name: loki_inflight_queries
      #     target:
      #       type: AverageValue
      #       averageValue: 12
      behavior:
        # -- Enable autoscaling behaviours
        enabled: false
        # -- define scale down policies, must conform to HPAScalingRules
        scaleDown: {}
        # -- define scale up policies, must conform to HPAScalingRules
        scaleUp: {}
    image:
      # -- The Docker registry for the querier image. Overrides `loki.image.registry`
      registry: null
      # -- Docker image repository for the querier image. Overrides `loki.image.repository`
      repository: null
      # -- Docker image tag for the querier image. Overrides `loki.image.tag`
      tag: null
    # -- Command to execute instead of defined in Docker image
    command: null
    # -- The name of the PriorityClass for querier pods
    priorityClassName: null
    # -- Labels for querier pods
    podLabels: {}
    # -- Annotations for querier pods
    podAnnotations: {}
    # -- Labels for querier service
    serviceLabels: {}
    # -- Annotations for querier service
    serviceAnnotations: {}
    # -- Additional CLI args for the querier
    extraArgs: []
    # -- Environment variables to add to the querier pods
    extraEnv: []
    # -- Environment variables from secrets or configmaps to add to the querier pods
    extraEnvFrom: []
    # -- Volume mounts to add to the querier pods
    extraVolumeMounts: []
    # -- Volumes to add to the querier pods
    extraVolumes: []
    # -- Resource requests and limits for the querier
    resources: {}
    # -- Containers to add to the querier pods
    extraContainers: []
    # -- Init containers to add to the querier pods
    initContainers: []
    # -- Grace period to allow the querier to shutdown before it is killed
    terminationGracePeriodSeconds: 30
    # -- topologySpread for querier pods.
    # @default -- Defaults to allow skew no more then 1 node
    topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            app.kubernetes.io/component: querier
    # -- Affinity for querier pods.
    # @default -- Hard node anti-affinity
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app.kubernetes.io/component: querier
            topologyKey: kubernetes.io/hostname
    # -- Pod Disruption Budget maxUnavailable
    maxUnavailable: null
    # -- Max Surge for querier pods
    maxSurge: 0
    # -- Node selector for querier pods
    nodeSelector: {}
    # -- Tolerations for querier pods
    tolerations: []
    # -- DNSConfig for querier pods
    dnsConfig: {}
    persistence:
      # -- Enable creating PVCs for the querier cache
      enabled: false
      # -- Size of persistent disk
      size: 10Gi
      # -- Storage class to be used.
      # If defined, storageClassName: <storageClass>.
      # If set to "-", storageClassName: "", which disables dynamic provisioning.
      # If empty or set to null, no storageClassName spec is
      # set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).
      storageClass: null
      # -- Annotations for querier PVCs
      annotations: {}
    # -- Adds the appProtocol field to the querier service. This allows querier to work with istio protocol selection.
    appProtocol:
      # -- Set the optional grpc service protocol. Ex: "grpc", "http2" or "https"
      grpc: ""
  # -- Configuration for the query-frontend
  queryFrontend:
    # -- Number of replicas for the query-frontend
    replicas: 0
    # -- hostAliases to add
    hostAliases: []
    #  - ip: 1.2.3.4
    #    hostnames:
    #      - domain.tld
    autoscaling:
      # -- Enable autoscaling for the query-frontend
      enabled: false
      # -- Minimum autoscaling replicas for the query-frontend
      minReplicas: 1
      # -- Maximum autoscaling replicas for the query-frontend
      maxReplicas: 3
      # -- Target CPU utilisation percentage for the query-frontend
      targetCPUUtilizationPercentage: 60
      # -- Target memory utilisation percentage for the query-frontend
      targetMemoryUtilizationPercentage: null
      # -- Allows one to define custom metrics using the HPA/v2 schema (for example, Pods, Object or External metrics)
      customMetrics: []
      # - type: Pods
      #   pods:
      #     metric:
      #       name: loki_query_rate
      #     target:
      #       type: AverageValue
      #       averageValue: 100
      behavior:
        # -- Enable autoscaling behaviours
        enabled: false
        # -- define scale down policies, must conform to HPAScalingRules
        scaleDown: {}
        # -- define scale up policies, must conform to HPAScalingRules
        scaleUp: {}
    image:
      # -- The Docker registry for the query-frontend image. Overrides `loki.image.registry`
      registry: null
      # -- Docker image repository for the query-frontend image. Overrides `loki.image.repository`
      repository: null
      # -- Docker image tag for the query-frontend image. Overrides `loki.image.tag`
      tag: null
    # -- Command to execute instead of defined in Docker image
    command: null
    # -- The name of the PriorityClass for query-frontend pods
    priorityClassName: null
    # -- Labels for query-frontend pods
    podLabels: {}
    # -- Annotations for query-frontend pods
    podAnnotations: {}
    # -- Labels for query-frontend service
    serviceLabels: {}
    # -- Annotations for query-frontend service
    serviceAnnotations: {}
    # -- Additional CLI args for the query-frontend
    extraArgs: []
    # -- Environment variables to add to the query-frontend pods
    extraEnv: []
    # -- Environment variables from secrets or configmaps to add to the query-frontend pods
    extraEnvFrom: []
    # -- Volume mounts to add to the query-frontend pods
    extraVolumeMounts: []
    # -- Volumes to add to the query-frontend pods
    extraVolumes: []
    # -- Resource requests and limits for the query-frontend
    resources: {}
    # -- Containers to add to the query-frontend pods
    extraContainers: []
    # -- Grace period to allow the query-frontend to shutdown before it is killed
    terminationGracePeriodSeconds: 30
    # -- Affinity for query-frontend pods.
    # @default -- Hard node anti-affinity
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app.kubernetes.io/component: query-frontend
            topologyKey: kubernetes.io/hostname
    # -- Pod Disruption Budget maxUnavailable
    maxUnavailable: null
    # -- Node selector for query-frontend pods
    nodeSelector: {}
    # -- Topology Spread Constraints for query-frontend pods
    topologySpreadConstraints: []
    # -- Tolerations for query-frontend pods
    tolerations: []
    # -- Adds the appProtocol field to the queryFrontend service. This allows queryFrontend to work with istio protocol selection.
    appProtocol:
      # -- Set the optional grpc service protocol. Ex: "grpc", "http2" or "https"
      grpc: ""
  # -- Configuration for the query-scheduler
  queryScheduler:
    # -- Number of replicas for the query-scheduler.
    # It should be lower than `-querier.max-concurrent` to avoid generating back-pressure in queriers;
    # it's also recommended that this value evenly divides the latter
    replicas: 0
    # -- hostAliases to add
    hostAliases: []
    #  - ip: 1.2.3.4
    #    hostnames:
    #      - domain.tld
    image:
      # -- The Docker registry for the query-scheduler image. Overrides `loki.image.registry`
      registry: null
      # -- Docker image repository for the query-scheduler image. Overrides `loki.image.repository`
      repository: null
      # -- Docker image tag for the query-scheduler image. Overrides `loki.image.tag`
      tag: null
    # -- The name of the PriorityClass for query-scheduler pods
    priorityClassName: null
    # -- Labels for query-scheduler pods
    podLabels: {}
    # -- Annotations for query-scheduler pods
    podAnnotations: {}
    # -- Labels for query-scheduler service
    serviceLabels: {}
    # -- Annotations for query-scheduler service
    serviceAnnotations: {}
    # -- Additional CLI args for the query-scheduler
    extraArgs: []
    # -- Environment variables to add to the query-scheduler pods
    extraEnv: []
    # -- Environment variables from secrets or configmaps to add to the query-scheduler pods
    extraEnvFrom: []
    # -- Volume mounts to add to the query-scheduler pods
    extraVolumeMounts: []
    # -- Volumes to add to the query-scheduler pods
    extraVolumes: []
    # -- Resource requests and limits for the query-scheduler
    resources: {}
    # -- Containers to add to the query-scheduler pods
    extraContainers: []
    # -- Grace period to allow the query-scheduler to shutdown before it is killed
    terminationGracePeriodSeconds: 30
    # -- Affinity for query-scheduler pods.
    # @default -- Hard node anti-affinity
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app.kubernetes.io/component: query-scheduler
            topologyKey: kubernetes.io/hostname
    # -- Pod Disruption Budget maxUnavailable
    maxUnavailable: 1
    # -- Node selector for query-scheduler pods
    nodeSelector: {}
    # -- Topology Spread Constraints for query-scheduler pods
    topologySpreadConstraints: []
    # -- Tolerations for query-scheduler pods
    tolerations: []
    # -- Set the optional grpc service protocol. Ex: "grpc", "http2" or "https"
    appProtocol:
      grpc: ""
  # -- Configuration for the index-gateway
  indexGateway:
    # -- Number of replicas for the index-gateway
    replicas: 0
    # -- Whether the index gateway should join the memberlist hashring
    joinMemberlist: true
    # -- hostAliases to add
    hostAliases: []
    #  - ip: 1.2.3.4
    #    hostnames:
    #      - domain.tld
    image:
      # -- The Docker registry for the index-gateway image. Overrides `loki.image.registry`
      registry: null
      # -- Docker image repository for the index-gateway image. Overrides `loki.image.repository`
      repository: null
      # -- Docker image tag for the index-gateway image. Overrides `loki.image.tag`
      tag: null
    # -- The name of the PriorityClass for index-gateway pods
    priorityClassName: null
    # -- Labels for index-gateway pods
    podLabels: {}
    # -- Annotations for index-gateway pods
    podAnnotations: {}
    # -- Labels for index-gateway service
    serviceLabels: {}
    # -- Annotations for index-gateway service
    serviceAnnotations: {}
    # -- Additional CLI args for the index-gateway
    extraArgs: []
    # -- Environment variables to add to the index-gateway pods
    extraEnv: []
    # -- Environment variables from secrets or configmaps to add to the index-gateway pods
    extraEnvFrom: []
    # -- Volume mounts to add to the index-gateway pods
    extraVolumeMounts: []
    # -- Volumes to add to the index-gateway pods
    extraVolumes: []
    # -- Resource requests and limits for the index-gateway
    resources: {}
    # -- Containers to add to the index-gateway pods
    extraContainers: []
    # -- Init containers to add to the index-gateway pods
    initContainers: []
    # -- Grace period to allow the index-gateway to shutdown before it is killed.
    terminationGracePeriodSeconds: 300
    # -- Affinity for index-gateway pods.
    # @default -- Hard node anti-affinity
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app.kubernetes.io/component: index-gateway
            topologyKey: kubernetes.io/hostname
    # -- Pod Disruption Budget maxUnavailable
    maxUnavailable: null
    # -- Node selector for index-gateway pods
    nodeSelector: {}
    # -- Topology Spread Constraints for index-gateway pods
    topologySpreadConstraints: []
    # -- Tolerations for index-gateway pods
    tolerations: []
    persistence:
      # -- Enable creating PVCs which is required when using boltdb-shipper
      enabled: false
      # -- Use emptyDir with ramdisk for storage. **Please note that all data in indexGateway will be lost on pod restart**
      inMemory: false
      # -- Size of persistent or memory disk
      size: 10Gi
      # -- Storage class to be used.
      # If defined, storageClassName: <storageClass>.
      # If set to "-", storageClassName: "", which disables dynamic provisioning.
      # If empty or set to null, no storageClassName spec is
      # set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).
      storageClass: null
      # -- Annotations for index gateway PVCs
      annotations: {}
      # -- Enable StatefulSetAutoDeletePVC feature
      enableStatefulSetAutoDeletePVC: false
      whenDeleted: Retain
      whenScaled: Retain
    # -- Set the optional grpc service protocol. Ex: "grpc", "http2" or "https"
    appProtocol:
      grpc: ""
    # -- UpdateStrategy for the indexGateway StatefulSet.
    updateStrategy:
      # -- One of  'OnDelete' or 'RollingUpdate'
      type: RollingUpdate
      # -- Optional for updateStrategy.type=RollingUpdate. See [Partitioned rolling updates](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#partitions) in the StatefulSet docs for details.
      # rollingUpdate:
      #   partition: 0
  # -- Configuration for the compactor
  compactor:
    # -- Number of replicas for the compactor
    replicas: 0
    # -- hostAliases to add
    hostAliases: []
    #  - ip: 1.2.3.4
    #    hostnames:
    #      - domain.tld
    image:
      # -- The Docker registry for the compactor image. Overrides `loki.image.registry`
      registry: null
      # -- Docker image repository for the compactor image. Overrides `loki.image.repository`
      repository: null
      # -- Docker image tag for the compactor image. Overrides `loki.image.tag`
      tag: null
    # -- Command to execute instead of defined in Docker image
    command: null
    # -- The name of the PriorityClass for compactor pods
    priorityClassName: null
    # -- Labels for compactor pods
    podLabels: {}
    # -- Annotations for compactor pods
    podAnnotations: {}
    # -- Affinity for compactor pods.
    # @default -- Hard node anti-affinity
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app.kubernetes.io/component: compactor
            topologyKey: kubernetes.io/hostname
    # -- Labels for compactor service
    serviceLabels: {}
    # -- Annotations for compactor service
    serviceAnnotations: {}
    # -- Additional CLI args for the compactor
    extraArgs: []
    # -- Environment variables to add to the compactor pods
    extraEnv: []
    # -- Environment variables from secrets or configmaps to add to the compactor pods
    extraEnvFrom: []
    # -- Volume mounts to add to the compactor pods
    extraVolumeMounts: []
    # -- Volumes to add to the compactor pods
    extraVolumes: []
    # -- readiness probe settings for ingester pods. If empty, use `loki.readinessProbe`
    readinessProbe: {}
    # -- liveness probe settings for ingester pods. If empty use `loki.livenessProbe`
    livenessProbe: {}
    # -- Resource requests and limits for the compactor
    resources: {}
    # -- Containers to add to the compactor pods
    extraContainers: []
    # -- Init containers to add to the compactor pods
    initContainers: []
    # -- Grace period to allow the compactor to shutdown before it is killed
    terminationGracePeriodSeconds: 30
    # -- Node selector for compactor pods
    nodeSelector: {}
    # -- Tolerations for compactor pods
    tolerations: []
    # -- Set the optional grpc service protocol. Ex: "grpc", "http2" or "https"
    appProtocol:
      grpc: ""
    persistence:
      # -- Enable creating PVCs for the compactor
      enabled: false
      # -- Size of persistent disk
      size: 10Gi
      # -- Storage class to be used.
      # If defined, storageClassName: <storageClass>.
      # If set to "-", storageClassName: "", which disables dynamic provisioning.
      # If empty or set to null, no storageClassName spec is
      # set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).
      storageClass: null
      # -- Annotations for compactor PVCs
      annotations: {}
      # -- List of the compactor PVCs
      # @notationType -- list
      claims:
        - name: data
          size: 10Gi
          #   -- Storage class to be used.
          #   If defined, storageClassName: <storageClass>.
          #   If set to "-", storageClassName: "", which disables dynamic provisioning.
          #   If empty or set to null, no storageClassName spec is
          #   set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).
          storageClass: null
          # - name: wal
          #   size: 150Gi
      # -- Enable StatefulSetAutoDeletePVC feature
      enableStatefulSetAutoDeletePVC: false
      whenDeleted: Retain
      whenScaled: Retain
    serviceAccount:
      create: false
      # -- The name of the ServiceAccount to use for the compactor.
      # If not set and create is true, a name is generated by appending
      # "-compactor" to the common ServiceAccount.
      name: null
      # -- Image pull secrets for the compactor service account
      imagePullSecrets: []
      # -- Annotations for the compactor service account
      annotations: {}
      # -- Set this toggle to false to opt out of automounting API credentials for the service account
      automountServiceAccountToken: true
  # -- Configuration for the bloom-gateway
  bloomGateway:
    # -- Number of replicas for the bloom-gateway
    replicas: 0
    # -- hostAliases to add
    hostAliases: []
    #  - ip: 1.2.3.4
    #    hostnames:
    #      - domain.tld
    image:
      # -- The Docker registry for the bloom-gateway image. Overrides `loki.image.registry`
      registry: null
      # -- Docker image repository for the bloom-gateway image. Overrides `loki.image.repository`
      repository: null
      # -- Docker image tag for the bloom-gateway image. Overrides `loki.image.tag`
      tag: null
    # -- Command to execute instead of defined in Docker image
    command: null
    # -- The name of the PriorityClass for bloom-gateway pods
    priorityClassName: null
    # -- Labels for bloom-gateway pods
    podLabels: {}
    # -- Annotations for bloom-gateway pods
    podAnnotations: {}
    # -- Affinity for bloom-gateway pods.
    # @default -- Hard node anti-affinity
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app.kubernetes.io/component: bloom-gateway
            topologyKey: kubernetes.io/hostname
    # -- Labels for bloom-gateway service
    serviceLabels: {}
    # -- Annotations for bloom-gateway service
    serviceAnnotations: {}
    # -- Additional CLI args for the bloom-gateway
    extraArgs: []
    # -- Environment variables to add to the bloom-gateway pods
    extraEnv: []
    # -- Environment variables from secrets or configmaps to add to the bloom-gateway pods
    extraEnvFrom: []
    # -- Volume mounts to add to the bloom-gateway pods
    extraVolumeMounts: []
    # -- Volumes to add to the bloom-gateway pods
    extraVolumes: []
    # -- readiness probe settings for ingester pods. If empty, use `loki.readinessProbe`
    readinessProbe: {}
    # -- liveness probe settings for ingester pods. If empty use `loki.livenessProbe`
    livenessProbe: {}
    # -- Resource requests and limits for the bloom-gateway
    resources: {}
    # -- Containers to add to the bloom-gateway pods
    extraContainers: []
    # -- Init containers to add to the bloom-gateway pods
    initContainers: []
    # -- Grace period to allow the bloom-gateway to shutdown before it is killed
    terminationGracePeriodSeconds: 30
    # -- Node selector for bloom-gateway pods
    nodeSelector: {}
    # -- Tolerations for bloom-gateway pods
    tolerations: []
    # -- Set the optional grpc service protocol. Ex: "grpc", "http2" or "https"
    appProtocol:
      grpc: ""
    persistence:
      # -- Enable creating PVCs for the bloom-gateway
      enabled: false
      # -- Annotations for bloom-gateway PVCs
      annotations: {}
      # -- List of the bloom-gateway PVCs
      # @notationType -- list
      claims:
        - name: data
          # -- Size of persistent disk
          size: 10Gi
          #   -- Storage class to be used.
          #   If defined, storageClassName: <storageClass>.
          #   If set to "-", storageClassName: "", which disables dynamic provisioning.
          #   If empty or set to null, no storageClassName spec is
          #   set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).
          storageClass: null
      # -- Enable StatefulSetAutoDeletePVC feature
      enableStatefulSetAutoDeletePVC: false
      whenDeleted: Retain
      whenScaled: Retain
    serviceAccount:
      create: false
      # -- The name of the ServiceAccount to use for the bloom-gateway.
      # If not set and create is true, a name is generated by appending
      # "-bloom-gateway" to the common ServiceAccount.
      name: null
      # -- Image pull secrets for the bloom-gateway service account
      imagePullSecrets: []
      # -- Annotations for the bloom-gateway service account
      annotations: {}
      # -- Set this toggle to false to opt out of automounting API credentials for the service account
      automountServiceAccountToken: true
  # -- Configuration for the bloom-planner
  bloomPlanner:
    # -- Number of replicas for the bloom-planner
    replicas: 0
    # -- hostAliases to add
    hostAliases: []
    #  - ip: 1.2.3.4
    #    hostnames:
    #      - domain.tld
    image:
      # -- The Docker registry for the bloom-planner image. Overrides `loki.image.registry`
      registry: null
      # -- Docker image repository for the bloom-planner image. Overrides `loki.image.repository`
      repository: null
      # -- Docker image tag for the bloom-planner image. Overrides `loki.image.tag`
      tag: null
    # -- Command to execute instead of defined in Docker image
    command: null
    # -- The name of the PriorityClass for bloom-planner pods
    priorityClassName: null
    # -- Labels for bloom-planner pods
    podLabels: {}
    # -- Annotations for bloom-planner pods
    podAnnotations: {}
    # -- Affinity for bloom-planner pods.
    # @default -- Hard node anti-affinity
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app.kubernetes.io/component: bloom-planner
            topologyKey: kubernetes.io/hostname
    # -- Labels for bloom-planner service
    serviceLabels: {}
    # -- Annotations for bloom-planner service
    serviceAnnotations: {}
    # -- Additional CLI args for the bloom-planner
    extraArgs: []
    # -- Environment variables to add to the bloom-planner pods
    extraEnv: []
    # -- Environment variables from secrets or configmaps to add to the bloom-planner pods
    extraEnvFrom: []
    # -- Volume mounts to add to the bloom-planner pods
    extraVolumeMounts: []
    # -- Volumes to add to the bloom-planner pods
    extraVolumes: []
    # -- readiness probe settings for ingester pods. If empty, use `loki.readinessProbe`
    readinessProbe: {}
    # -- liveness probe settings for ingester pods. If empty use `loki.livenessProbe`
    livenessProbe: {}
    # -- Resource requests and limits for the bloom-planner
    resources: {}
    # -- Containers to add to the bloom-planner pods
    extraContainers: []
    # -- Init containers to add to the bloom-planner pods
    initContainers: []
    # -- Grace period to allow the bloom-planner to shutdown before it is killed
    terminationGracePeriodSeconds: 30
    # -- Node selector for bloom-planner pods
    nodeSelector: {}
    # -- Tolerations for bloom-planner pods
    tolerations: []
    # -- Set the optional grpc service protocol. Ex: "grpc", "http2" or "https"
    appProtocol:
      grpc: ""
    persistence:
      # -- Enable creating PVCs for the bloom-planner
      enabled: false
      # -- Annotations for bloom-planner PVCs
      annotations: {}
      # -- List of the bloom-planner PVCs
      # @notationType -- list
      claims:
        - name: data
          # -- Size of persistent disk
          size: 10Gi
          #   -- Storage class to be used.
          #   If defined, storageClassName: <storageClass>.
          #   If set to "-", storageClassName: "", which disables dynamic provisioning.
          #   If empty or set to null, no storageClassName spec is
          #   set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).
          storageClass: null
      # -- Enable StatefulSetAutoDeletePVC feature
      enableStatefulSetAutoDeletePVC: false
      whenDeleted: Retain
      whenScaled: Retain
    serviceAccount:
      create: false
      # -- The name of the ServiceAccount to use for the bloom-planner.
      # If not set and create is true, a name is generated by appending
      # "-bloom-planner" to the common ServiceAccount.
      name: null
      # -- Image pull secrets for the bloom-planner service account
      imagePullSecrets: []
      # -- Annotations for the bloom-planner service account
      annotations: {}
      # -- Set this toggle to false to opt out of automounting API credentials for the service account
      automountServiceAccountToken: true
  # -- Configuration for the bloom-builder
  bloomBuilder:
    # -- Number of replicas for the bloom-builder
    replicas: 0
    # -- hostAliases to add
    hostAliases: []
    #  - ip: 1.2.3.4
    #    hostnames:
    #      - domain.tld
    autoscaling:
      # -- Enable autoscaling for the bloom-builder
      enabled: false
      # -- Minimum autoscaling replicas for the bloom-builder
      minReplicas: 1
      # -- Maximum autoscaling replicas for the bloom-builder
      maxReplicas: 3
      # -- Target CPU utilisation percentage for the bloom-builder
      targetCPUUtilizationPercentage: 60
      # -- Target memory utilisation percentage for the bloom-builder
      targetMemoryUtilizationPercentage: null
      # -- Allows one to define custom metrics using the HPA/v2 schema (for example, Pods, Object or External metrics)
      customMetrics: []
      # - type: Pods
      #   pods:
      #     metric:
      #       name: loki_query_rate
      #     target:
      #       type: AverageValue
      #       averageValue: 100
      behavior:
        # -- Enable autoscaling behaviours
        enabled: false
        # -- define scale down policies, must conform to HPAScalingRules
        scaleDown: {}
        # -- define scale up policies, must conform to HPAScalingRules
        scaleUp: {}
    image:
      # -- The Docker registry for the bloom-builder image. Overrides `loki.image.registry`
      registry: null
      # -- Docker image repository for the bloom-builder image. Overrides `loki.image.repository`
      repository: null
      # -- Docker image tag for the bloom-builder image. Overrides `loki.image.tag`
      tag: null
    # -- Command to execute instead of defined in Docker image
    command: null
    # -- The name of the PriorityClass for bloom-builder pods
    priorityClassName: null
    # -- Labels for bloom-builder pods
    podLabels: {}
    # -- Annotations for bloom-builder pods
    podAnnotations: {}
    # -- Labels for bloom-builder service
    serviceLabels: {}
    # -- Annotations for bloom-builder service
    serviceAnnotations: {}
    # -- Additional CLI args for the bloom-builder
    extraArgs: []
    # -- Environment variables to add to the bloom-builder pods
    extraEnv: []
    # -- Environment variables from secrets or configmaps to add to the bloom-builder pods
    extraEnvFrom: []
    # -- Volume mounts to add to the bloom-builder pods
    extraVolumeMounts: []
    # -- Volumes to add to the bloom-builder pods
    extraVolumes: []
    # -- Resource requests and limits for the bloom-builder
    resources: {}
    # -- Containers to add to the bloom-builder pods
    extraContainers: []
    # -- Grace period to allow the bloom-builder to shutdown before it is killed
    terminationGracePeriodSeconds: 30
    # -- Affinity for bloom-builder pods.
    # @default -- Hard node anti-affinity
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app.kubernetes.io/component: bloom-builder
            topologyKey: kubernetes.io/hostname
    # -- Pod Disruption Budget maxUnavailable
    maxUnavailable: null
    # -- Node selector for bloom-builder pods
    nodeSelector: {}
    # -- Tolerations for bloom-builder pods
    tolerations: []
    # -- Adds the appProtocol field to the queryFrontend service. This allows bloomBuilder to work with istio protocol selection.
    appProtocol:
      # -- Set the optional grpc service protocol. Ex: "grpc", "http2" or "https"
      grpc: ""
  # -- Configuration for the pattern ingester
  patternIngester:
    # -- Number of replicas for the pattern ingester
    replicas: 0
    # -- hostAliases to add
    hostAliases: []
    #  - ip: 1.2.3.4
    #    hostnames:
    #      - domain.tld
    image:
      # -- The Docker registry for the pattern ingester image. Overrides `loki.image.registry`
      registry: null
      # -- Docker image repository for the pattern ingester image. Overrides `loki.image.repository`
      repository: null
      # -- Docker image tag for the pattern ingester image. Overrides `loki.image.tag`
      tag: null
    # -- Command to execute instead of defined in Docker image
    command: null
    # -- The name of the PriorityClass for pattern ingester pods
    priorityClassName: null
    # -- Labels for pattern ingester pods
    podLabels: {}
    # -- Annotations for pattern ingester pods
    podAnnotations: {}
    # -- Affinity for pattern ingester pods.
    # @default -- Hard node anti-affinity
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app.kubernetes.io/component: pattern-ingester
            topologyKey: kubernetes.io/hostname
    # -- Labels for pattern ingester service
    serviceLabels: {}
    # -- Annotations for pattern ingester service
    serviceAnnotations: {}
    # -- Additional CLI args for the pattern ingester
    extraArgs: []
    # -- Environment variables to add to the pattern ingester pods
    extraEnv: []
    # -- Environment variables from secrets or configmaps to add to the pattern ingester pods
    extraEnvFrom: []
    # -- Volume mounts to add to the pattern ingester pods
    extraVolumeMounts: []
    # -- Volumes to add to the pattern ingester pods
    extraVolumes: []
    # -- readiness probe settings for ingester pods. If empty, use `loki.readinessProbe`
    readinessProbe: {}
    # -- liveness probe settings for ingester pods. If empty use `loki.livenessProbe`
    livenessProbe: {}
    # -- Resource requests and limits for the pattern ingester
    resources: {}
    # -- Containers to add to the pattern ingester pods
    extraContainers: []
    # -- Init containers to add to the pattern ingester pods
    initContainers: []
    # -- Grace period to allow the pattern ingester to shutdown before it is killed
    terminationGracePeriodSeconds: 30
    # -- Node selector for pattern ingester pods
    nodeSelector: {}
    # -- Topology Spread Constraints for pattern ingester pods
    topologySpreadConstraints: []
    # -- Tolerations for pattern ingester pods
    tolerations: []
    # -- Set the optional grpc service protocol. Ex: "grpc", "http2" or "https"
    appProtocol:
      grpc: ""
    persistence:
      # -- Enable creating PVCs for the pattern ingester
      enabled: false
      # -- Size of persistent disk
      size: 10Gi
      # -- Storage class to be used.
      # If defined, storageClassName: <storageClass>.
      # If set to "-", storageClassName: "", which disables dynamic provisioning.
      # If empty or set to null, no storageClassName spec is
      # set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).
      storageClass: null
      # -- Annotations for pattern ingester PVCs
      annotations: {}
      # -- List of the pattern ingester PVCs
      # @notationType -- list
      claims:
        - name: data
          size: 10Gi
          #   -- Storage class to be used.
          #   If defined, storageClassName: <storageClass>.
          #   If set to "-", storageClassName: "", which disables dynamic provisioning.
          #   If empty or set to null, no storageClassName spec is
          #   set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).
          storageClass: null
          # - name: wal
          #   size: 150Gi
      # -- Enable StatefulSetAutoDeletePVC feature
      enableStatefulSetAutoDeletePVC: false
      whenDeleted: Retain
      whenScaled: Retain
    serviceAccount:
      create: false
      # -- The name of the ServiceAccount to use for the pattern ingester.
      # If not set and create is true, a name is generated by appending
      # "-pattern-ingester" to the common ServiceAccount.
      name: null
      # -- Image pull secrets for the pattern ingester service account
      imagePullSecrets: []
      # -- Annotations for the pattern ingester service account
      annotations: {}
      # -- Set this toggle to false to opt out of automounting API credentials for the service account
      automountServiceAccountToken: true
  # -- Configuration for the ruler
  ruler:
    # -- The ruler component is optional and can be disabled if desired.
    enabled: true
    # -- Number of replicas for the ruler
    replicas: 0
    # -- hostAliases to add
    hostAliases: []
    #  - ip: 1.2.3.4
    #    hostnames:
    #      - domain.tld
    image:
      # -- The Docker registry for the ruler image. Overrides `loki.image.registry`
      registry: null
      # -- Docker image repository for the ruler image. Overrides `loki.image.repository`
      repository: null
      # -- Docker image tag for the ruler image. Overrides `loki.image.tag`
      tag: null
    # -- Command to execute instead of defined in Docker image
    command: null
    # -- The name of the PriorityClass for ruler pods
    priorityClassName: null
    # -- Labels for compactor pods
    podLabels: {}
    # -- Annotations for ruler pods
    podAnnotations: {}
    # -- Labels for ruler service
    serviceLabels: {}
    # -- Annotations for ruler service
    serviceAnnotations: {}
    # -- Additional CLI args for the ruler
    extraArgs: []
    # -- Environment variables to add to the ruler pods
    extraEnv: []
    # -- Environment variables from secrets or configmaps to add to the ruler pods
    extraEnvFrom: []
    # -- Volume mounts to add to the ruler pods
    extraVolumeMounts: []
    # -- Volumes to add to the ruler pods
    extraVolumes: []
    # -- Resource requests and limits for the ruler
    resources: {}
    # -- Containers to add to the ruler pods
    extraContainers: []
    # -- Init containers to add to the ruler pods
    initContainers: []
    # -- Grace period to allow the ruler to shutdown before it is killed
    terminationGracePeriodSeconds: 300
    # -- Affinity for ruler pods.
    # @default -- Hard node anti-affinity
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app.kubernetes.io/component: ruler
            topologyKey: kubernetes.io/hostname
    # -- Pod Disruption Budget maxUnavailable
    maxUnavailable: null
    # -- Node selector for ruler pods
    nodeSelector: {}
    # -- Topology Spread Constraints for ruler pods
    topologySpreadConstraints: []
    # -- Tolerations for ruler pods
    tolerations: []
    # -- DNSConfig for ruler pods
    dnsConfig: {}
    persistence:
      # -- Enable creating PVCs which is required when using recording rules
      enabled: false
      # -- Size of persistent disk
      size: 10Gi
      # -- Storage class to be used.
      # If defined, storageClassName: <storageClass>.
      # If set to "-", storageClassName: "", which disables dynamic provisioning.
      # If empty or set to null, no storageClassName spec is
      # set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).
      storageClass: null
      # -- Annotations for ruler PVCs
      annotations: {}
    # -- Set the optional grpc service protocol. Ex: "grpc", "http2" or "https"
    appProtocol:
      grpc: ""
    # -- Directories containing rules files
    directories: {}
    # tenant_foo:
    #   rules1.txt: |
    #     groups:
    #       - name: should_fire
    #         rules:
    #           - alert: HighPercentageError
    #             expr: |
    #               sum(rate({app="foo", env="production"} |= "error" [5m])) by (job)
    #                 /
    #               sum(rate({app="foo", env="production"}[5m])) by (job)
    #                 > 0.05
    #             for: 10m
    #             labels:
    #               severity: warning
    #             annotations:
    #               summary: High error rate
    #       - name: credentials_leak
    #         rules:
    #           - alert: http-credentials-leaked
    #             annotations:
    #               message: "{{ $labels.job }} is leaking http basic auth credentials."
    #             expr: 'sum by (cluster, job, pod) (count_over_time({namespace="prod"} |~ "http(s?)://(\\w+):(\\w+)@" [5m]) > 0)'
    #             for: 10m
    #             labels:
    #               severity: critical
    #   rules2.txt: |
    #     groups:
    #       - name: example
    #         rules:
    #         - alert: HighThroughputLogStreams
    #           expr: sum by(container) (rate({job=~"loki-dev/.*"}[1m])) > 1000
    #           for: 2m
    # tenant_bar:
    #   rules1.txt: |
    #     groups:
    #       - name: should_fire
    #         rules:
    #           - alert: HighPercentageError
    #             expr: |
    #               sum(rate({app="foo", env="production"} |= "error" [5m])) by (job)
    #                 /
    #               sum(rate({app="foo", env="production"}[5m])) by (job)
    #                 > 0.05
    #             for: 10m
    #             labels:
    #               severity: warning
    #             annotations:
    #               summary: High error rate
    #       - name: credentials_leak
    #         rules:
    #           - alert: http-credentials-leaked
    #             annotations:
    #               message: "{{ $labels.job }} is leaking http basic auth credentials."
    #             expr: 'sum by (cluster, job, pod) (count_over_time({namespace="prod"} |~ "http(s?)://(\\w+):(\\w+)@" [5m]) > 0)'
    #             for: 10m
    #             labels:
    #               severity: critical
    #   rules2.txt: |
    #     groups:
    #       - name: example
    #         rules:
    #         - alert: HighThroughputLogStreams
    #           expr: sum by(container) (rate({job=~"loki-dev/.*"}[1m])) > 1000
    #           for: 2m
  memcached:
    image:
      # -- Memcached Docker image repository
      repository: memcached
      # -- Memcached Docker image tag
      tag: 1.6.33-alpine
      # -- Memcached Docker image pull policy
      pullPolicy: IfNotPresent
    # -- The SecurityContext override for memcached pods
    podSecurityContext:
      runAsNonRoot: true
      runAsUser: 11211
      runAsGroup: 11211
      fsGroup: 11211
    # -- The name of the PriorityClass for memcached pods
    priorityClassName: null
    # -- The SecurityContext for memcached containers
    containerSecurityContext:
      readOnlyRootFilesystem: true
      capabilities:
        drop: [ALL]
      allowPrivilegeEscalation: false
  memcachedExporter:
    # -- Whether memcached metrics should be exported
    enabled: true
    image:
      repository: prom/memcached-exporter
      tag: v0.15.0
      pullPolicy: IfNotPresent
    resources:
      requests: {}
      limits: {}
    # -- The SecurityContext for memcached exporter containers
    containerSecurityContext:
      readOnlyRootFilesystem: true
      capabilities:
        drop: [ALL]
      allowPrivilegeEscalation: false
    # -- Extra args to add to the exporter container.
    # Example:
    # extraArgs:
    #   memcached.tls.enable: true
    #   memcached.tls.cert-file: /certs/cert.crt
    #   memcached.tls.key-file: /certs/cert.key
    #   memcached.tls.ca-file: /certs/ca.crt
    #   memcached.tls.insecure-skip-verify: false
    #   memcached.tls.server-name: memcached
    extraArgs: {}
  resultsCache:
    # -- Specifies whether memcached based results-cache should be enabled
    enabled: false
    # -- Specify how long cached results should be stored in the results-cache before being expired
    defaultValidity: 12h
    # -- Memcached operation timeout
    timeout: 500ms
    # -- Total number of results-cache replicas
    replicas: 1
    # -- Port of the results-cache service
    port: 11211
    # -- Amount of memory allocated to results-cache for object storage (in MB).
    allocatedMemory: 1024
    # -- Maximum item results-cache for memcached (in MB).
    maxItemMemory: 5
    # -- Maximum number of connections allowed
    connectionLimit: 16384
    # -- Max memory to use for cache write back
    writebackSizeLimit: 500MB
    # -- Max number of objects to use for cache write back
    writebackBuffer: 500000
    # -- Number of parallel threads for cache write back
    writebackParallelism: 1
    # -- Extra init containers for results-cache pods
    initContainers: []
    # -- Annotations for the results-cache pods
    annotations: {}
    # -- Node selector for results-cache pods
    nodeSelector: {}
    # -- Affinity for results-cache pods
    affinity: {}
    # -- topologySpreadConstraints allows to customize the default topologySpreadConstraints. This can be either a single dict as shown below or a slice of topologySpreadConstraints.
    # labelSelector is taken from the constraint itself (if it exists) or is generated by the chart using the same selectors as for services.
    topologySpreadConstraints: []
    #  maxSkew: 1
    #  topologyKey: kubernetes.io/hostname
    #  whenUnsatisfiable: ScheduleAnyway
    # -- Tolerations for results-cache pods
    tolerations: []
    # -- Pod Disruption Budget
    podDisruptionBudget:
      maxUnavailable: 1
    # -- The name of the PriorityClass for results-cache pods
    priorityClassName: null
    # -- Labels for results-cache pods
    podLabels: {}
    # -- Annotations for results-cache pods
    podAnnotations: {}
    # -- Management policy for results-cache pods
    podManagementPolicy: Parallel
    # -- Grace period to allow the results-cache to shutdown before it is killed
    terminationGracePeriodSeconds: 60
    # -- Stateful results-cache strategy
    statefulStrategy:
      type: RollingUpdate
    # -- Add extended options for results-cache memcached container. The format is the same as for the memcached -o/--extend flag.
    # Example:
    # extraExtendedOptions: 'tls,modern,track_sizes'
    extraExtendedOptions: ""
    # -- Additional CLI args for results-cache
    extraArgs: {}
    # -- Additional containers to be added to the results-cache pod.
    extraContainers: []
    # -- Additional volumes to be added to the results-cache pod (applies to both memcached and exporter containers).
    # Example:
    # extraVolumes:
    # - name: extra-volume
    #   secret:
    #    secretName: extra-volume-secret
    extraVolumes: []
    # -- Additional volume mounts to be added to the results-cache pod (applies to both memcached and exporter containers).
    # Example:
    # extraVolumeMounts:
    # - name: extra-volume
    #   mountPath: /etc/extra-volume
    #   readOnly: true
    extraVolumeMounts: []
    # -- Resource requests and limits for the results-cache
    # By default a safe memory limit will be requested based on allocatedMemory value (floor (* 1.2 allocatedMemory)).
    resources: null
    # -- Service annotations and labels
    service:
      annotations: {}
      labels: {}
    # -- Persistence settings for the results-cache
    persistence:
      # -- Enable creating PVCs for the results-cache
      enabled: false
      # -- Size of persistent disk, must be in G or Gi
      storageSize: 10G
      # -- Storage class to be used.
      # If defined, storageClassName: <storageClass>.
      # If set to "-", storageClassName: "", which disables dynamic provisioning.
      # If empty or set to null, no storageClassName spec is
      # set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).
      storageClass: null
      # -- Volume mount path
      mountPath: /data
  chunksCache:
    # -- Specifies whether memcached based chunks-cache should be enabled
    enabled: false
    # -- Batchsize for sending and receiving chunks from chunks cache
    batchSize: 4
    # -- Parallel threads for sending and receiving chunks from chunks cache
    parallelism: 5
    # -- Memcached operation timeout
    timeout: 2000ms
    # -- Specify how long cached chunks should be stored in the chunks-cache before being expired
    defaultValidity: 0s
    # -- Total number of chunks-cache replicas
    replicas: 1
    # -- Port of the chunks-cache service
    port: 11211
    # -- Amount of memory allocated to chunks-cache for object storage (in MB).
    allocatedMemory: 8192
    # -- Maximum item memory for chunks-cache (in MB).
    maxItemMemory: 5
    # -- Maximum number of connections allowed
    connectionLimit: 16384
    # -- Max memory to use for cache write back
    writebackSizeLimit: 500MB
    # -- Max number of objects to use for cache write back
    writebackBuffer: 500000
    # -- Number of parallel threads for cache write back
    writebackParallelism: 1
    # -- Extra init containers for chunks-cache pods
    initContainers: []
    # -- Annotations for the chunks-cache pods
    annotations: {}
    # -- Node selector for chunks-cache pods
    nodeSelector: {}
    # -- Affinity for chunks-cache pods
    affinity: {}
    # -- topologySpreadConstraints allows to customize the default topologySpreadConstraints. This can be either a single dict as shown below or a slice of topologySpreadConstraints.
    # labelSelector is taken from the constraint itself (if it exists) or is generated by the chart using the same selectors as for services.
    topologySpreadConstraints: []
    #  maxSkew: 1
    #  topologyKey: kubernetes.io/hostname
    #  whenUnsatisfiable: ScheduleAnyway
    # -- Tolerations for chunks-cache pods
    tolerations: []
    # -- Pod Disruption Budget
    podDisruptionBudget:
      maxUnavailable: 1
    # -- The name of the PriorityClass for chunks-cache pods
    priorityClassName: null
    # -- Labels for chunks-cache pods
    podLabels: {}
    # -- Annotations for chunks-cache pods
    podAnnotations: {}
    # -- Management policy for chunks-cache pods
    podManagementPolicy: Parallel
    # -- Grace period to allow the chunks-cache to shutdown before it is killed
    terminationGracePeriodSeconds: 60
    # -- Stateful chunks-cache strategy
    statefulStrategy:
      type: RollingUpdate
    # -- Add extended options for chunks-cache memcached container. The format is the same as for the memcached -o/--extend flag.
    # Example:
    # extraExtendedOptions: 'tls,no_hashexpand'
    extraExtendedOptions: ""
    # -- Additional CLI args for chunks-cache
    extraArgs: {}
    # -- Additional containers to be added to the chunks-cache pod.
    extraContainers: []
    # -- Additional volumes to be added to the chunks-cache pod (applies to both memcached and exporter containers).
    # Example:
    # extraVolumes:
    # - name: extra-volume
    #   secret:
    #    secretName: extra-volume-secret
    extraVolumes: []
    # -- Additional volume mounts to be added to the chunks-cache pod (applies to both memcached and exporter containers).
    # Example:
    # extraVolumeMounts:
    # - name: extra-volume
    #   mountPath: /etc/extra-volume
    #   readOnly: true
    extraVolumeMounts: []
    # -- Resource requests and limits for the chunks-cache
    # By default a safe memory limit will be requested based on allocatedMemory value (floor (* 1.2 allocatedMemory)).
    resources: null
    # -- Service annotations and labels
    service:
      annotations: {}
      labels: {}
    # -- Persistence settings for the chunks-cache
    persistence:
      # -- Enable creating PVCs for the chunks-cache
      enabled: false
      # -- Size of persistent disk, must be in G or Gi
      storageSize: 10G
      # -- Storage class to be used.
      # If defined, storageClassName: <storageClass>.
      # If set to "-", storageClassName: "", which disables dynamic provisioning.
      # If empty or set to null, no storageClassName spec is
      # set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).
      storageClass: null
      # -- Volume mount path
      mountPath: /data
  ######################################################################################################################
  #
  # Subchart configurations
  #
  ######################################################################################################################
  # -- Setting for the Grafana Rollout Operator https://github.com/grafana/helm-charts/tree/main/charts/rollout-operator
  rollout_operator:
    enabled: false
    # -- podSecurityContext is the pod security context for the rollout operator.
    # When installing on OpenShift, override podSecurityContext settings with
    #
    # rollout_operator:
    #   podSecurityContext:
    #     fsGroup: null
    #     runAsGroup: null
    #     runAsUser: null
    podSecurityContext:
      fsGroup: 10001
      runAsGroup: 10001
      runAsNonRoot: true
      runAsUser: 10001
      seccompProfile:
        type: RuntimeDefault
    # Set the container security context
    securityContext:
      readOnlyRootFilesystem: true
      capabilities:
        drop: [ALL]
      allowPrivilegeEscalation: false
  # -- Configuration for the minio subchart
  minio:
    enabled: false
    replicas: 1
    # Minio requires 2 to 16 drives for erasure code (drivesPerNode * replicas)
    # https://docs.min.io/docs/minio-erasure-code-quickstart-guide
    # Since we only have 1 replica, that means 2 drives must be used.
    drivesPerNode: 2
    rootUser: enterprise-logs
    rootPassword: supersecret
    buckets:
      - name: chunks
        policy: none
        purge: false
      - name: ruler
        policy: none
        purge: false
      - name: admin
        policy: none
        purge: false
    persistence:
      size: 5Gi
      annotations: {}
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
    # Allow the address used by Loki to refer to Minio to be overridden
    address: null
  # Create extra manifests via values. Would be passed through `tpl` for templating
  # objects can also be provided as multiline strings, useful for templating field names
  extraObjects: []
  # - apiVersion: v1
  #   kind: ConfigMap
  #   metadata:
  #     name: loki-alerting-rules
  #   data:
  #     loki-alerting-rules.yaml: |-
  #       groups:
  #         - name: example
  #           rules:
  #           - alert: example
  #             expr: |
  #               sum(count_over_time({app="loki"} |~ "error")) > 0
  #             for: 3m
  #             labels:
  #               severity: warning
  #               category: logs
  #             annotations:
  #               message: "loki has encountered errors"
  # - |
  #     apiVersion: v1
  #     kind: Secret
  #     type: Opaque
  #     metadata:
  #       name: loki-distributed-basic-auth
  #     data:
  #       {{- range .Values.loki.tenants }}
  #       {{ .name }}: {{ b64enc .password | quote }}
  #       {{- end }}

  sidecar:
    image:
      # -- The Docker registry and image for the k8s sidecar
      repository: kiwigrid/k8s-sidecar
      # -- Docker image tag
      tag: 1.28.0
      # -- Docker image sha. If empty, no sha will be used
      sha: ""
      # -- Docker image pull policy
      pullPolicy: IfNotPresent
    # -- Resource requests and limits for the sidecar
    resources: {}
    #   limits:
    #     cpu: 100m
    #     memory: 100Mi
    #   requests:
    #     cpu: 50m
    #     memory: 50Mi
    # -- The SecurityContext for the sidecar.
    securityContext: {}
    # -- Set to true to skip tls verification for kube api calls.
    skipTlsVerify: false
    # -- Ensure that rule files aren't conflicting and being overwritten by prefixing their name with the namespace they are defined in.
    enableUniqueFilenames: false
    # -- Readiness probe definition. Probe is disabled on the sidecar by default.
    readinessProbe: {}
    # -- Liveness probe definition. Probe is disabled on the sidecar by default.
    livenessProbe: {}
    rules:
      # -- Whether or not to create a sidecar to ingest rule from specific ConfigMaps and/or Secrets.
      enabled: true
      # -- Label that the configmaps/secrets with rules will be marked with.
      label: loki_rule
      # -- Label value that the configmaps/secrets with rules will be set to.
      labelValue: ""
      # -- Folder into which the rules will be placed.
      folder: /rules
      # -- Comma separated list of namespaces. If specified, the sidecar will search for config-maps/secrets inside these namespaces.
      # Otherwise the namespace in which the sidecar is running will be used.
      # It's also possible to specify 'ALL' to search in all namespaces.
      searchNamespace: null
      # -- Method to use to detect ConfigMap changes. With WATCH the sidecar will do a WATCH request, with SLEEP it will list all ConfigMaps, then sleep for 60 seconds.
      watchMethod: WATCH
      # -- Search in configmap, secret, or both.
      resource: both
      # -- Absolute path to the shell script to execute after a configmap or secret has been reloaded.
      script: null
      # -- WatchServerTimeout: request to the server, asking it to cleanly close the connection after that.
      # defaults to 60sec; much higher values like 3600 seconds (1h) are feasible for non-Azure K8S.
      watchServerTimeout: 60
      #
      # -- WatchClientTimeout: is a client-side timeout, configuring your local socket.
      # If you have a network outage dropping all packets with no RST/FIN,
      # this is how long your client waits before realizing & dropping the connection.
      # Defaults to 66sec.
      watchClientTimeout: 60
      # -- Log level of the sidecar container.
      logLevel: INFO
  ############################################## WARNING ###############################################################
  #
  # DEPRECATED VALUES
  #
  # The following values are deprecated and will be removed in a future version of the helm chart!
  #
  ############################################## WARNING ##############################################################

  # -- DEPRECATED Monitoring section determines which monitoring features to enable, this section is being replaced
  # by https://github.com/grafana/meta-monitoring-chart
  monitoring:
    # Dashboards for monitoring Loki
    dashboards:
      # -- If enabled, create configmap with dashboards for monitoring Loki
      enabled: false
      # -- Alternative namespace to create dashboards ConfigMap in
      namespace: null
      # -- Additional annotations for the dashboards ConfigMap
      annotations: {}
      # -- Labels for the dashboards ConfigMap
      labels:
        grafana_dashboard: "1"
    # -- DEPRECATED Recording rules for monitoring Loki, required for some dashboards
    rules:
      # -- If enabled, create PrometheusRule resource with Loki recording rules
      enabled: false
      # -- Include alerting rules
      alerting: true
      # -- Specify which individual alerts should be disabled
      # -- Instead of turning off each alert one by one, set the .monitoring.rules.alerting value to false instead.
      # -- If you disable all the alerts and keep .monitoring.rules.alerting set to true, the chart will fail to render.
      disabled: {}
      #  LokiRequestErrors: true
      #  LokiRequestPanics: true
      # -- Alternative namespace to create PrometheusRule resources in
      namespace: null
      # -- Additional annotations for the rules PrometheusRule resource
      annotations: {}
      # -- Additional labels for the rules PrometheusRule resource
      labels: {}
      # -- Additional labels for PrometheusRule alerts
      additionalRuleLabels: {}
      # -- Additional groups to add to the rules file
      additionalGroups: []
      # - name: additional-loki-rules
      #   rules:
      #     - record: job:loki_request_duration_seconds_bucket:sum_rate
      #       expr: sum(rate(loki_request_duration_seconds_bucket[1m])) by (le, job)
      #     - record: job_route:loki_request_duration_seconds_bucket:sum_rate
      #       expr: sum(rate(loki_request_duration_seconds_bucket[1m])) by (le, job, route)
      #     - record: node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate
      #       expr: sum(rate(container_cpu_usage_seconds_total[1m])) by (node, namespace, pod, container)
    #  -- DEPRECATED ServiceMonitor configuration
    serviceMonitor:
      # -- If enabled, ServiceMonitor resources for Prometheus Operator are created
      enabled: false
      # -- Namespace selector for ServiceMonitor resources
      namespaceSelector: {}
      # -- ServiceMonitor annotations
      annotations: {}
      # -- Additional ServiceMonitor labels
      labels: {}
      # -- ServiceMonitor scrape interval
      # Default is 15s because included recording rules use a 1m rate, and scrape interval needs to be at
      # least 1/4 rate interval.
      interval: 15s
      # -- ServiceMonitor scrape timeout in Go duration format (e.g. 15s)
      scrapeTimeout: null
      # -- ServiceMonitor relabel configs to apply to samples before scraping
      # https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#relabelconfig
      relabelings: []
      # -- ServiceMonitor metric relabel configs to apply to samples before ingestion
      # https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint
      metricRelabelings: []
      # -- ServiceMonitor will use http by default, but you can pick https as well
      scheme: http
      # -- ServiceMonitor will use these tlsConfig settings to make the health check requests
      tlsConfig: null
      # -- If defined, will create a MetricsInstance for the Grafana Agent Operator.
      metricsInstance:
        # -- If enabled, MetricsInstance resources for Grafana Agent Operator are created
        enabled: true
        # -- MetricsInstance annotations
        annotations: {}
        # -- Additional MetricsInstance labels
        labels: {}
        # -- If defined a MetricsInstance will be created to remote write metrics.
        remoteWrite: null
    # -- DEPRECATED Self monitoring determines whether Loki should scrape its own logs.
    # This feature currently relies on the Grafana Agent Operator being installed,
    # which is installed by default using the grafana-agent-operator sub-chart.
    # It will create custom resources for GrafanaAgent, LogsInstance, and PodLogs to configure
    # scrape configs to scrape its own logs with the labels expected by the included dashboards.
    selfMonitoring:
      enabled: false
      # -- Tenant to use for self monitoring
      tenant:
        # -- Name of the tenant
        name: "self-monitoring"
        # -- Password of the gateway for Basic auth
        password: null
        # -- Namespace to create additional tenant token secret in. Useful if your Grafana instance
        # is in a separate namespace. Token will still be created in the canary namespace.
        secretNamespace: "{{ .Release.Namespace }}"
      # -- DEPRECATED Grafana Agent configuration
      grafanaAgent:
        # -- DEPRECATED Controls whether to install the Grafana Agent Operator and its CRDs.
        # Note that helm will not install CRDs if this flag is enabled during an upgrade.
        # In that case install the CRDs manually from https://github.com/grafana/agent/tree/main/production/operator/crds
        installOperator: false
        # -- Grafana Agent annotations
        annotations: {}
        # -- Additional Grafana Agent labels
        labels: {}
        # -- Enable the config read api on port 8080 of the agent
        enableConfigReadAPI: false
        # -- The name of the PriorityClass for GrafanaAgent pods
        priorityClassName: null
        # -- Resource requests and limits for the grafanaAgent pods
        resources: {}
        #   limits:
        #     memory: 200Mi
        #   requests:
        #     cpu: 50m
        #     memory: 100Mi
        # -- Tolerations for GrafanaAgent pods
        tolerations: []
      # PodLogs configuration
      podLogs:
        # -- PodLogs version
        apiVersion: monitoring.grafana.com/v1alpha1
        # -- PodLogs annotations
        annotations: {}
        # -- Additional PodLogs labels
        labels: {}
        # -- PodLogs relabel configs to apply to samples before scraping
        # https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#relabelconfig
        relabelings: []
        # -- Additional pipeline stages to process logs after scraping
        # https://grafana.com/docs/agent/latest/operator/api/#pipelinestagespec-a-namemonitoringgrafanacomv1alpha1pipelinestagespeca
        additionalPipelineStages: []
      # LogsInstance configuration
      logsInstance:
        # -- LogsInstance annotations
        annotations: {}
        # -- Additional LogsInstance labels
        labels: {}
        # -- Additional clients for remote write
        clients: null
  # -- DEPRECATED Configuration for the table-manager. The table-manager is only necessary when using a deprecated
  # index type such as Cassandra, Bigtable, or DynamoDB, it has not been necessary since loki introduced self-
  # contained index types like 'boltdb-shipper' and 'tsdb'. This will be removed in a future helm chart.
  tableManager:
    # -- Specifies whether the table-manager should be enabled
    enabled: false
    image:
      # -- The Docker registry for the table-manager image. Overrides `loki.image.registry`
      registry: null
      # -- Docker image repository for the table-manager image. Overrides `loki.image.repository`
      repository: null
      # -- Docker image tag for the table-manager image. Overrides `loki.image.tag`
      tag: null
    # -- Command to execute instead of defined in Docker image
    command: null
    # -- The name of the PriorityClass for table-manager pods
    priorityClassName: null
    # -- Labels for table-manager pods
    podLabels: {}
    # -- Annotations for table-manager deployment
    annotations: {}
    # -- Annotations for table-manager pods
    podAnnotations: {}
    service:
      # -- Annotations for table-manager Service
      annotations: {}
      # -- Additional labels for table-manager Service
      labels: {}
    # -- Additional CLI args for the table-manager
    extraArgs: []
    # -- Environment variables to add to the table-manager pods
    extraEnv: []
    # -- Environment variables from secrets or configmaps to add to the table-manager pods
    extraEnvFrom: []
    # -- Volume mounts to add to the table-manager pods
    extraVolumeMounts: []
    # -- Volumes to add to the table-manager pods
    extraVolumes: []
    # -- Resource requests and limits for the table-manager
    resources: {}
    # -- Containers to add to the table-manager pods
    extraContainers: []
    # -- Grace period to allow the table-manager to shutdown before it is killed
    terminationGracePeriodSeconds: 30
    # -- Affinity for table-manager pods.
    # @default -- Hard node and anti-affinity
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app.kubernetes.io/component: table-manager
            topologyKey: kubernetes.io/hostname
    # -- DNS config table-manager pods
    dnsConfig: {}
    # -- Node selector for table-manager pods
    nodeSelector: {}
    # -- Tolerations for table-manager pods
    tolerations: []
    # -- Enable deletes by retention
    retention_deletes_enabled: false
    # -- Set retention period
    retention_period: 0


fluent-bit:
  enabled: false
  # Default values for fluent-bit.

  loki:
    host: 'test'

  # kind -- DaemonSet or Deployment
  kind: DaemonSet

  # replicaCount -- Only applicable if kind=Deployment
  replicaCount: 1

  image:
    repository: cr.fluentbit.io/fluent/fluent-bit
    # Overrides the image tag whose default is {{ .Chart.AppVersion }}
    # Set to "-" to not use the default value
    tag:
    digest:
    pullPolicy: IfNotPresent

  testFramework:
    enabled: false
    namespace:
    image:
      repository: busybox
      pullPolicy: Always
      tag: latest
      digest:

  imagePullSecrets: []
  nameOverride: ""
  fullnameOverride: ""

  serviceAccount:
    create: true
    annotations: {}
    name:

  rbac:
    create: true
    nodeAccess: false
    eventsAccess: false

  # Configure podsecuritypolicy
  # Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  # from Kubernetes 1.25, PSP is deprecated
  # See: https://kubernetes.io/blog/2022/08/23/kubernetes-v1-25-release/#pod-security-changes
  # We automatically disable PSP if Kubernetes version is 1.25 or higher
  podSecurityPolicy:
    create: false
    annotations: {}
    runAsUser:
      rule: RunAsAny
    seLinux:
      # This policy assumes the nodes are using AppArmor rather than SELinux.
      rule: RunAsAny

  # OpenShift-specific configuration
  openShift:
    enabled: false
    securityContextConstraints:
      # Create SCC for Fluent-bit and allow use it
      create: true
      name: ""
      annotations: {}
      runAsUser:
        type: RunAsAny
      seLinuxContext:
        type: MustRunAs
      # Use existing SCC in cluster, rather then create new one
      existingName: ""

  podSecurityContext: {}
  #   fsGroup: 2000

  hostNetwork: false
  dnsPolicy: ClusterFirst

  dnsConfig: {}
  #   nameservers:
  #     - 1.2.3.4
  #   searches:
  #     - ns1.svc.cluster-domain.example
  #     - my.dns.search.suffix
  #   options:
  #     - name: ndots
  #       value: "2"
  #     - name: edns0

  hostAliases: []
  #   - ip: "1.2.3.4"
  #     hostnames:
  #     - "foo.local"
  #     - "bar.local"

  securityContext: {}
  #   capabilities:
  #     drop:
  #     - ALL
  #   readOnlyRootFilesystem: true
  #   runAsNonRoot: true
  #   runAsUser: 1000

  service:
    type: ClusterIP
    port: 2020
    internalTrafficPolicy:
    loadBalancerClass:
    loadBalancerSourceRanges: []
    labels: {}
    # nodePort: 30020
    # clusterIP: 172.16.10.1
    annotations: {}
    #   prometheus.io/path: "/api/v1/metrics/prometheus"
    #   prometheus.io/port: "2020"
    #   prometheus.io/scrape: "true"
    externalIPs: []
    # externalIPs:
    #  - 2.2.2.2

  serviceMonitor:
    enabled: false
    #   namespace: monitoring
    #   interval: 10s
    #   scrapeTimeout: 10s
    #   selector:
    #    prometheus: my-prometheus
    #  ## metric relabel configs to apply to samples before ingestion.
    #  ##
    #  metricRelabelings:
    #    - sourceLabels: [__meta_kubernetes_service_label_cluster]
    #      targetLabel: cluster
    #      regex: (.*)
    #      replacement: ${1}
    #      action: replace
    #  ## relabel configs to apply to samples after ingestion.
    #  ##
    #  relabelings:
    #    - sourceLabels: [__meta_kubernetes_pod_node_name]
    #      separator: ;
    #      regex: ^(.*)$
    #      targetLabel: nodename
    #      replacement: $1
    #      action: replace
    #  scheme: ""
    #  tlsConfig: {}

    ## Bear in mind if you want to collect metrics from a different port
    ## you will need to configure the new ports on the extraPorts property.
    additionalEndpoints: []
    # - port: metrics
    #   path: /metrics
    #   interval: 10s
    #   scrapeTimeout: 10s
    #   scheme: ""
    #   tlsConfig: {}
    #   # metric relabel configs to apply to samples before ingestion.
    #   #
    #   metricRelabelings:
    #     - sourceLabels: [__meta_kubernetes_service_label_cluster]
    #       targetLabel: cluster
    #       regex: (.*)
    #       replacement: ${1}
    #       action: replace
    #   # relabel configs to apply to samples after ingestion.
    #   #
    #   relabelings:
    #     - sourceLabels: [__meta_kubernetes_pod_node_name]
    #       separator: ;
    #       regex: ^(.*)$
    #       targetLabel: nodename
    #       replacement: $1
    #       action: replace

  prometheusRule:
    enabled: false
  #   namespace: ""
  #   additionalLabels: {}
  #   rules:
  #   - alert: NoOutputBytesProcessed
  #     expr: rate(fluentbit_output_proc_bytes_total[5m]) == 0
  #     annotations:
  #       message: |
  #         Fluent Bit instance {{ $labels.instance }}'s output plugin {{ $labels.name }} has not processed any
  #         bytes for at least 15 minutes.
  #       summary: No Output Bytes Processed
  #     for: 15m
  #     labels:
  #       severity: critical

  dashboards:
    enabled: false
    labelKey: grafana_dashboard
    labelValue: 1
    annotations: {}
    namespace: ""
    deterministicUid: false

  lifecycle: {}
  #   preStop:
  #     exec:
  #       command: ["/bin/sh", "-c", "sleep 20"]

  livenessProbe:
    httpGet:
      path: /
      port: http

  readinessProbe:
    httpGet:
      path: /api/v1/health
      port: http

  resources: {}
  #   limits:
  #     cpu: 100m
  #     memory: 128Mi
  #   requests:
  #     cpu: 100m
  #     memory: 128Mi

  ## only available if kind is Deployment
  ingress:
    enabled: false
    ingressClassName: ""
    annotations: {}
    #  kubernetes.io/ingress.class: nginx
    #  kubernetes.io/tls-acme: "true"
    hosts: []
    # - host: fluent-bit.example.tld
    extraHosts: []
    # - host: fluent-bit-extra.example.tld
    ## specify extraPort number
    #   port: 5170
    tls: []
    #  - secretName: fluent-bit-example-tld
    #    hosts:
    #      - fluent-bit.example.tld

  ## only available if kind is Deployment
  autoscaling:
    vpa:
      enabled: false

      annotations: {}

      # List of resources that the vertical pod autoscaler can control. Defaults to cpu and memory
      controlledResources: []

      # Define the max allowed resources for the pod
      maxAllowed: {}
      # cpu: 200m
      # memory: 100Mi
      # Define the min allowed resources for the pod
      minAllowed: {}
      # cpu: 200m
      # memory: 100Mi

      updatePolicy:
        # Specifies whether recommended updates are applied when a Pod is started and whether recommended updates
        # are applied during the life of a Pod. Possible values are "Off", "Initial", "Recreate", and "Auto".
        updateMode: Auto

    enabled: false
    minReplicas: 1
    maxReplicas: 3
    targetCPUUtilizationPercentage: 75
    #  targetMemoryUtilizationPercentage: 75
    ## see https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-multiple-metrics-and-custom-metrics
    customRules: []
    #     - type: Pods
    #       pods:
    #         metric:
    #           name: packets-per-second
    #         target:
    #           type: AverageValue
    #           averageValue: 1k
    ## see https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#support-for-configurable-scaling-behavior
    behavior: {}
  #      scaleDown:
  #        policies:
  #          - type: Pods
  #            value: 4
  #            periodSeconds: 60
  #          - type: Percent
  #            value: 10
  #            periodSeconds: 60

  ## only available if kind is Deployment
  podDisruptionBudget:
    enabled: false
    annotations: {}
    maxUnavailable: "30%"

  nodeSelector: {}

  tolerations: []

  affinity: {}

  labels: {}

  annotations: {}

  podAnnotations: {}

  podLabels: {}

  ## How long (in seconds) a pods needs to be stable before progressing the deployment
  ##
  minReadySeconds:

  ## How long (in seconds) a pod may take to exit (useful with lifecycle hooks to ensure lb deregistration is done)
  ##
  terminationGracePeriodSeconds:

  priorityClassName: ""

  env: []
  #  - name: FOO
  #    value: "bar"

  # The envWithTpl array below has the same usage as "env", but is using the tpl function to support templatable string.
  # This can be useful when you want to pass dynamic values to the Chart using the helm argument "--set <variable>=<value>"
  # https://helm.sh/docs/howto/charts_tips_and_tricks/#using-the-tpl-function
  envWithTpl: []
  #  - name: FOO_2
  #    value: "{{ .Values.foo2 }}"
  #
  # foo2: bar2

  envFrom: []

  # This supports either a structured array or a templatable string
  extraContainers: []

  # Array mode
  # extraContainers:
  #   - name: do-something
  #     image: busybox
  #     command: ['do', 'something']

  # String mode
  # extraContainers: |-
  #   - name: do-something
  #     image: bitnami/kubectl:{{ .Capabilities.KubeVersion.Major }}.{{ .Capabilities.KubeVersion.Minor }}
  #     command: ['kubectl', 'version']

  flush: 1

  metricsPort: 2020

  extraPorts: []
  #   - port: 5170
  #     containerPort: 5170
  #     protocol: TCP
  #     name: tcp
  #     nodePort: 30517

  extraVolumes: []

  extraVolumeMounts: []

  updateStrategy: {}
  #   type: RollingUpdate
  #   rollingUpdate:
  #     maxUnavailable: 1

  # Make use of a pre-defined configmap instead of the one templated here
  existingConfigMap: ""

  networkPolicy:
    enabled: false
  #   ingress:
  #     from: []

  luaScripts: {}

  ## https://docs.fluentbit.io/manual/administration/configuring-fluent-bit/classic-mode/configuration-file
  config:
    service: |
      [SERVICE]
          Daemon Off
          Flush {{ .Values.flush }}
          Log_Level {{ .Values.logLevel }}
          Parsers_File /fluent-bit/etc/parsers.conf
          Parsers_File /fluent-bit/etc/conf/custom_parsers.conf
          HTTP_Server On
          HTTP_Listen 0.0.0.0
          HTTP_Port {{ .Values.metricsPort }}
          Health_Check On

    ## https://docs.fluentbit.io/manual/pipeline/inputs
    inputs: |
      [INPUT]
          Name tail
          Path /var/log/containers/*.log
          multiline.parser docker, cri
          Tag kube.*
          Mem_Buf_Limit 5MB
          Skip_Long_Lines On

      [INPUT]
          Name systemd
          Tag host.*
          Systemd_Filter _SYSTEMD_UNIT=kubelet.service
          Read_From_Tail On

    ## https://docs.fluentbit.io/manual/pipeline/filters
    filters: |
      [FILTER]
          Name kubernetes
          Match kube.*
          Merge_Log On
          Keep_Log Off
          K8S-Logging.Parser On
          K8S-Logging.Exclude On

    ## https://docs.fluentbit.io/manual/pipeline/outputs
    outputs: |
      [OUTPUT]
          Name es
          Match kube.*
          Host elasticsearch-master
          Logstash_Format On
          Retry_Limit False

      [OUTPUT]
          Name es
          Match host.*
          Host elasticsearch-master
          Logstash_Format On
          Logstash_Prefix node
          Retry_Limit False

    ## https://docs.fluentbit.io/manual/administration/configuring-fluent-bit/classic-mode/upstream-servers
    ## This configuration is deprecated, please use `extraFiles` instead.
    upstream: {}

    ## https://docs.fluentbit.io/manual/pipeline/parsers
    customParsers: |
      [PARSER]
          Name docker_no_time
          Format json
          Time_Keep Off
          Time_Key time
          Time_Format %Y-%m-%dT%H:%M:%S.%L

    # This allows adding more files with arbitrary filenames to /fluent-bit/etc/conf by providing key/value pairs.
    # The key becomes the filename, the value becomes the file content.
    extraFiles: {}
  #     upstream.conf: |
  #       [UPSTREAM]
  #           upstream1
  #
  #       [NODE]
  #           name       node-1
  #           host       127.0.0.1
  #           port       43000
  #     example.conf: |
  #       [OUTPUT]
  #           Name example
  #           Match foo.*
  #           Host bar

  # The config volume is mounted by default, either to the existingConfigMap value, or the default of "fluent-bit.fullname"
  volumeMounts:
    - name: config
      mountPath: /fluent-bit/etc/conf

  daemonSetVolumes:
    - name: varlog
      hostPath:
        path: /var/log
    - name: varlibdockercontainers
      hostPath:
        path: /var/lib/docker/containers
    - name: etcmachineid
      hostPath:
        path: /etc/machine-id
        type: File

  daemonSetVolumeMounts:
    - name: varlog
      mountPath: /var/log
    - name: varlibdockercontainers
      mountPath: /var/lib/docker/containers
      readOnly: true
    - name: etcmachineid
      mountPath: /etc/machine-id
      readOnly: true

  command:
    - /fluent-bit/bin/fluent-bit

  args:
    - --workdir=/fluent-bit/etc
    - --config=/fluent-bit/etc/conf/fluent-bit.conf

  # This supports either a structured array or a templatable string
  initContainers: []

  # Array mode
  # initContainers:
  #   - name: do-something
  #     image: bitnami/kubectl:1.22
  #     command: ['kubectl', 'version']

  # String mode
  # initContainers: |-
  #   - name: do-something
  #     image: bitnami/kubectl:{{ .Capabilities.KubeVersion.Major }}.{{ .Capabilities.KubeVersion.Minor }}
  #     command: ['kubectl', 'version']

  logLevel: info

  hotReload:
    enabled: false
    image:
      repository: ghcr.io/jimmidyson/configmap-reload
      tag: v0.14.0
      digest:
      pullPolicy: IfNotPresent
    resources: {}

tempo:
  enabled: false
  # -- Overrides the chart's name
  nameOverride: ""

  # -- Overrides the chart's computed fullname
  fullnameOverride: ""

  # -- Define the amount of instances
  replicas: 1

  # -- Number of old history to retain to allow rollback (If not set, default Kubernetes value is set to 10)
  # revisionHistoryLimit: 1

  # -- labels for tempo
  labels: {}

  # -- Annotations for the StatefulSet
  annotations: {}

  tempo:
    repository: grafana/tempo
    tag: ""
    pullPolicy: IfNotPresent
    ## Optionally specify an array of imagePullSecrets.
    ## Secrets must be manually created in the namespace.
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
    ##
    # pullSecrets:
    #   - myRegistryKeySecretName

    updateStrategy: RollingUpdate
    resources: {}
    #  requests:
    #    cpu: 1000m
    #    memory: 4Gi
    #  limits:
    #    cpu: 2000m
    #    memory: 6Gi

    memBallastSizeMbs: 1024
    multitenancyEnabled: false
    # -- If true, Tempo will report anonymous usage data about the shape of a deployment to Grafana Labs
    reportingEnabled: true
    metricsGenerator:
      # -- If true, enables Tempo's metrics generator (https://grafana.com/docs/tempo/next/metrics-generator/)
      enabled: false
      remoteWriteUrl: "http://prometheus.monitoring:9090/api/v1/write"
    # -- Configuration options for the ingester
    ingester: {}
    # -- Configuration options for the querier
    querier: {}
    # -- Configuration options for the query-fronted
    queryFrontend: {}
    retention: 24h
    # Global overrides
    global_overrides:
      per_tenant_override_config: /conf/overrides.yaml
    overrides: {}

    # Tempo server configuration
    # Refers to https://grafana.com/docs/tempo/latest/configuration/#server
    server:
      # -- HTTP server listen port
      http_listen_port: 3200
    storage:
      trace:
        # tempo storage backend
        # refer https://grafana.com/docs/tempo/latest/configuration/
        ## Use s3 for example
        # backend: s3
        # store traces in s3
        # s3:
        #   bucket: <your s3 bucket>                        # store traces in this bucket
        #   endpoint: s3.dualstack.us-east-2.amazonaws.com  # api endpoint
        #   access_key: ...                                 # optional. access key when using static credentials.
        #   secret_key: ...                                 # optional. secret key when using static credentials.
        #   insecure: false                                 # optional. enable if endpoint is http
        backend: local
        local:
          path: /var/tempo/traces
        wal:
          path: /var/tempo/wal
    # this configuration will listen on all ports and protocols that tempo is capable of.
    # the receives all come from the OpenTelemetry collector.  more configuration information can
    # be found there: https://github.com/open-telemetry/opentelemetry-collector/tree/master/receiver
    receivers:
      jaeger:
        protocols:
          grpc:
            endpoint: 0.0.0.0:14250
          thrift_binary:
            endpoint: 0.0.0.0:6832
          thrift_compact:
            endpoint: 0.0.0.0:6831
          thrift_http:
            endpoint: 0.0.0.0:14268
      opencensus:
      otlp:
        protocols:
          grpc:
            endpoint: "0.0.0.0:4317"
          http:
            endpoint: "0.0.0.0:4318"
    securityContext: {}
      # allowPrivilegeEscalation: false
      #  capabilities:
      #    drop:
      #    - ALL
      # readOnlyRootFilesystem: true
    ## Additional container arguments
    extraArgs: {}
    # -- Environment variables to add
    extraEnv: []
    # -- Environment variables from secrets or configmaps to add to the ingester pods
    extraEnvFrom: []
    # -- Volume mounts to add
    extraVolumeMounts: []
    # - name: extra-volume
    #   mountPath: /mnt/volume
    #   readOnly: true
    #   existingClaim: volume-claim

  # -- Tempo configuration file contents
  # @default -- Dynamically generated tempo configmap
  config: |
      multitenancy_enabled: {{ .Values.tempo.multitenancyEnabled }}
      usage_report:
        reporting_enabled: {{ .Values.tempo.reportingEnabled }}
      compactor:
        compaction:
          block_retention: {{ .Values.tempo.retention }}
      distributor:
        receivers:
          {{- toYaml .Values.tempo.receivers | nindent 8 }}
      ingester:
        {{- toYaml .Values.tempo.ingester | nindent 6 }}
      server:
        {{- toYaml .Values.tempo.server | nindent 6 }}
      storage:
        {{- toYaml .Values.tempo.storage | nindent 6 }}
      querier:
        {{- toYaml .Values.tempo.querier | nindent 6 }}
      query_frontend:
        {{- toYaml .Values.tempo.queryFrontend | nindent 6 }}
      overrides:
        {{- toYaml .Values.tempo.global_overrides | nindent 6 }}
        {{- if .Values.tempo.metricsGenerator.enabled }}
            metrics_generator_processors:
            - 'service-graphs'
            - 'span-metrics'
      metrics_generator:
            storage:
              path: "/tmp/tempo"
              remote_write:
                - url: {{ .Values.tempo.metricsGenerator.remoteWriteUrl }}
        {{- end }}

  tempoQuery:
    repository: grafana/tempo-query
    tag: null
    pullPolicy: IfNotPresent
    ## Optionally specify an array of imagePullSecrets.
    ## Secrets must be manually created in the namespace.
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
    ##
    # pullSecrets:
    #   - myRegistryKeySecretName

    # -- if False the tempo-query container is not deployed
    enabled: false

    service:
      port: 16686

    ingress:
      enabled: false
      # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
      # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
      # ingressClassName: nginx
      # Values can be templated
      annotations: {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
      labels: {}
      path: /

      # pathType is only for k8s >= 1.1=
      pathType: Prefix

      hosts:
        - query.tempo.example.com
      ## Extra paths to prepend to every host configuration. This is useful when working with annotation based services.
      extraPaths: []
      # - path: /*
      #   backend:
      #     serviceName: ssl-redirect
      #     servicePort: use-annotation
      ## Or for k8s > 1.19
      # - path: /*
      #   pathType: Prefix
      #   backend:
      #     service:
      #       name: ssl-redirect
      #       port:
      #         name: use-annotation


      tls: []
      #  - secretName: tempo-query-tls
      #    hosts:
      #      - query.tempo.example.com

    resources: {}
    #  requests:
    #    cpu: 1000m
    #    memory: 4Gi
    #  limits:
    #    cpu: 2000m
    #    memory: 6Gi

    ## Additional container arguments
    extraArgs: {}
    # -- Environment variables to add
    extraEnv: []
    # -- Volume mounts to add
    extraVolumeMounts: []
    # - name: extra-volume
    #   mountPath: /mnt/volume
    #   readOnly: true
    #   existingClaim: volume-claim
    securityContext: {}
      # allowPrivilegeEscalation: false
      #  capabilities:
      #    drop:
      #    - ALL
      # readOnlyRootFilesystem: false # fails if true, do not enable

  # -- securityContext for container
  securityContext:
    runAsUser: 10001
    runAsGroup: 10001
    fsGroup: 10001
    runAsNonRoot: false

  serviceAccount:
    # -- Specifies whether a ServiceAccount should be created
    create: true
    # -- The name of the ServiceAccount to use.
    # If not set and create is true, a name is generated using the fullname template
    name: null
    # -- Image pull secrets for the service account
    imagePullSecrets: []
    # -- Annotations for the service account
    annotations: {}
    # -- Labels for the service account
    labels: {}
    automountServiceAccountToken: true

  service:
    type: ClusterIP
    annotations: {}
    labels: {}
    targetPort: ""

  serviceMonitor:
    enabled: false
    interval: ""
    additionalLabels: {}
    annotations: {}
    # scrapeTimeout: 10s

  persistence:
    enabled: false
    # storageClassName: local-path
    accessModes:
      - ReadWriteOnce
    size: 10Gi

  # -- Pod Annotations
  podAnnotations: {}

  # -- Pod (extra) Labels
  podLabels: {}

  # Apply extra labels to common labels.
  extraLabels: {}

  # -- Volumes to add
  extraVolumes: []

  # -- Node labels for pod assignment. See: https://kubernetes.io/docs/user-guide/node-selection/
  nodeSelector: {}

  # -- Tolerations for pod assignment. See: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  tolerations: []

  # -- Affinity for pod assignment. See: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  affinity: {}

  # -- The name of the PriorityClass
  priorityClassName: null

  networkPolicy:
    ## @param networkPolicy.enabled Enable creation of NetworkPolicy resources. Only Ingress traffic is filtered for now.
    ##
    enabled: false
    ## @param networkPolicy.allowExternal Don't require client label for connections
    ## The Policy model to apply. When set to false, only pods with the correct
    ## client label will have network access to  tempo port defined.
    ## When true, tempo will accept connections from any source
    ## (with the correct destination port).
    ##
    ingress: true
    ## @param networkPolicy.ingress When true enables the creation
    ## an ingress network policy
    ##
    allowExternal: true
    ## @param networkPolicy.explicitNamespacesSelector A Kubernetes LabelSelector to explicitly select namespaces from which traffic could be allowed
    ## If explicitNamespacesSelector is missing or set to {}, only client Pods that are in the networkPolicy's namespace
    ## and that match other criteria, the ones that have the good label, can reach the tempo.
    ## But sometimes, we want the tempo to be accessible to clients from other namespaces, in this case, we can use this
    ## LabelSelector to select these namespaces, note that the networkPolicy's namespace should also be explicitly added.
    ##
    ## Example:
    ## explicitNamespacesSelector:
    ##   matchLabels:
    ##     role: frontend
    ##   matchExpressions:
    ##    - {key: role, operator: In, values: [frontend]}
    ##
    explicitNamespacesSelector: {}
    ##
    egress:
      ## @param networkPolicy.egress.enabled When enabled, an egress network policy will be
      ## created allowing tempo to connect to external data sources from kubernetes cluster.
      enabled: false
      ##
      ## @param networkPolicy.egress.blockDNSResolution When enabled, DNS resolution will be blocked
      ## for all pods in the tempo namespace.
      blockDNSResolution: false
      ##
      ## @param networkPolicy.egress.ports Add individual ports to be allowed by the egress
      ports: []
      ## Add ports to the egress by specifying - port: <port number>
      ## E.X.
      ## - port: 80
      ## - port: 443
      ##
      ## @param networkPolicy.egress.to Allow egress traffic to specific destinations
      to: []
      ## Add destinations to the egress by specifying - ipBlock: <CIDR>
      ## E.X.
      ## to:
      ##  - namespaceSelector:
      ##    matchExpressions:
    ##    - {key: role, operator: In, values: [tempo]}

pyroscope:
  enabled: false
# Default values for pyroscope.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.


  pyroscope:
    replicaCount: 1

    # -- Kubernetes cluster domain suffix for DNS discovery
    cluster_domain: .cluster.local

    image:
      repository: grafana/pyroscope
      pullPolicy: IfNotPresent
      # Allows to override the image tag, which defaults to the appVersion in the chart metadata
      tag: ""

    extraArgs:
      log.level: debug

    extraLabels: {}

    extraEnvVars:
      {}
      # The following environment variables are set by the Helm chart.
      # JAEGER_AGENT_HOST: jaeger-agent.jaeger.svc.cluster.local.

    # -- Environment variables from secrets or configmaps to add to the pods
    extraEnvFrom: []

    imagePullSecrets: []
    dnsPolicy: ClusterFirst
    initContainers: []
    nameOverride: ""
    fullnameOverride: ""

    rbac:
      create: true

    serviceAccount:
      # Specifies whether a service account should be created
      create: true
      # Annotations to add to the service account
      annotations: {}
      # The name of the service account to use.
      # If not set and create is true, a name is generated using the fullname template
      name: ""

    podAnnotations:
      # Scrapes itself see https://grafana.com/docs/pyroscope/latest/deploy-kubernetes/helm/#optional-scrape-your-own-workloads-profiles
      profiles.grafana.com/memory.scrape: "true"
      profiles.grafana.com/memory.port_name: http2
      profiles.grafana.com/cpu.scrape: "true"
      profiles.grafana.com/cpu.port_name: http2
      profiles.grafana.com/goroutine.scrape: "true"
      profiles.grafana.com/goroutine.port_name: http2
      # profiles.grafana.com/block.scrape: "true"
      # profiles.grafana.com/mutex.scrape: "true"

    podSecurityContext:
      fsGroup: 10001
      runAsUser: 10001
      runAsNonRoot: true

    podDisruptionBudget:
      enabled: true
      maxUnavailable: 1

    securityContext:
      {}
      # capabilities:
      #   drop:
      #   - ALL
      # readOnlyRootFilesystem: true
      # runAsNonRoot: true
      # runAsUser: 1000

    service:
      type: ClusterIP
      port: 4040
      port_name: http2
      scheme: HTTP
      annotations: {}

    memberlist:
      port: 7946
      port_name: memberlist

    resources:
      {}
      # We usually recommend not to specify default resources and to leave this as a conscious
      # choice for the user. This also increases chances charts run on environments with little
      # resources, such as Minikube. If you do want to specify resources, uncomment the following
      # lines, adjust them as necessary, and remove the curly braces after 'resources'.
      #
      # Note that if memory consumption is higher than you would like, you can decrease the interval
      # that profiles are written into blocks by setting `pyroscopedb.max-block-duration` in the `extraArgs`
      # stanza. By default, it is set to 3h - override it, for example, as below:
      # ```
      # extraArgs:
      #   pyroscopedb.max-block-duration: 30m
      # ```
      #
      # limits:
      #   cpu: 100m
      #   memory: 128Mi
      # requests:
      #   cpu: 100m
      #   memory: 128Mi

    nodeSelector: {}

    # -- Topology Spread Constraints
    topologySpreadConstraints: []

    ## ref: https://kubernetes.io/docs/concepts/storage/persistent-volumes/
    ## If you set enabled as "True", you need :
    ## - create a pv which above 10Gi and has same namespace with phlare
    ## - keep storageClassName same with below setting
    persistence:
      enabled: false
      accessModes:
        - ReadWriteOnce
      size: 10Gi
      annotations: {}
      # selector:
      #   matchLabels:
      #     app.kubernetes.io/name: phlare
      # subPath: ""
      # existingClaim:

    extraVolumes:
      []
      # - name: backup-volume
      #   emptydir: {}

    extraVolumeMounts:
      []
      # - name: testing
      #   mountPath: /var/lib/testing
      #   readOnly: false
      # - name: test-volume
      #   mountPath: /var/tmp/test-volume
      #   existingClaim: test-volume
      #   readOnly: false

    tolerations: []

    affinity: {}

    # run specific components separately
    components: {}

    # -- Allows to override Phlare's configuration using structured format.
    structuredConfig: {}

    # -- Contains Phlare's configuration as a string.
    # @default -- The config depends on other values been set, details can be found in [`values.yaml`](./values.yaml)
    config: |
      {{- if .Values.minio.enabled }}
      storage:
        backend: s3
        s3:
          endpoint: "{{ include "pyroscope.fullname" . }}-minio:9000"
          bucket_name: {{(index .Values.minio.buckets 0).name | quote }}
          access_key_id: {{ .Values.minio.rootUser | quote }}
          secret_access_key: {{ .Values.minio.rootPassword | quote }}
          insecure: true
      {{- end }}

    # -- Allows to add tenant specific overrides to the default limit configuration.
    tenantOverrides:
      {}
      # "foo":
      #   ingestion_rate_mb: 1
      #   ingestion_burst_size_mb: 2
    # -- Grafana Agent Configuration.

  # -------------------------------------
  # Configuration for `alloy` child chart
  # -------------------------------------
  alloy:
    enabled: false
    controller:
      type: "statefulset"
      replicas: 1
      podAnnotations:
        profiles.grafana.com/memory.scrape: "true"
        profiles.grafana.com/memory.port_name: "http-metrics"
        profiles.grafana.com/cpu.scrape: "true"
        profiles.grafana.com/cpu.port_name: "http-metrics"
        profiles.grafana.com/goroutine.scrape: "true"
        profiles.grafana.com/goroutine.port_name: "http-metrics"
    alloy:
      stabilityLevel: "public-preview"  # This needs to be set for some of our resources until verison v1.2 is released
      configMap:
        create: false
        name: alloy-config-pyroscope
      clustering:
        enabled: true

  # -------------------------------------
  # Configuration for `grafana-agent` child chart
  # -------------------------------------
  agent:
    enabled: false
    controller:
      type: "statefulset"
      replicas: 1
      podAnnotations:
        profiles.grafana.com/memory.scrape: "true"
        profiles.grafana.com/memory.port_name: "http-metrics"
        profiles.grafana.com/cpu.scrape: "true"
        profiles.grafana.com/cpu.port_name: "http-metrics"
        profiles.grafana.com/goroutine.scrape: "true"
        profiles.grafana.com/goroutine.port_name: "http-metrics"
    agent:
      configMap:
        create: false
        name: grafana-agent-config-pyroscope
      clustering:
        enabled: true

  # -------------------------------------
  # Configuration for `minio` child chart
  # -------------------------------------
  minio:
    enabled: false
    replicas: 1
    # Minio requires 2 to 16 drives for erasure code (drivesPerNode * replicas)
    # https://docs.min.io/docs/minio-erasure-code-quickstart-guide
    # Since we only have 1 replica, that means 2 drives must be used.
    drivesPerNode: 2
    rootUser: grafana-pyroscope
    rootPassword: supersecret
    buckets:
      - name: grafana-pyroscope-data
        policy: none
        purge: false
    persistence:
      size: 5Gi
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
    podAnnotations:
      phlare.grafana.com/scrape: "true"
      phlare.grafana.com/port: "9000"

  ingress:
    enabled: false
    className: ""
    # hosts:
    #   - localhost
    # tls:
    #   - secretName: certificate

  # ServiceMonitor configuration
  serviceMonitor:
    # -- If enabled, ServiceMonitor resources for Prometheus Operator are created
    enabled: false
    # -- Namespace selector for ServiceMonitor resources
    namespaceSelector: {}
    # -- Optional expressions to match on
    matchExpressions: []
      # - key: prometheus.io/service-monitor
      #   operator: NotIn
      #   values:
      #     - "false"
    # -- ServiceMonitor annotations
    annotations: {}
    # -- Additional ServiceMonitor labels
    labels: {}
    # -- ServiceMonitor scrape interval
    interval: null
    # -- ServiceMonitor scrape timeout in Go duration format (e.g. 15s)
    scrapeTimeout: null
    # -- ServiceMonitor relabel configs to apply to samples before scraping
    # https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#relabelconfig
    relabelings: []
    # -- ServiceMonitor metric relabel configs to apply to samples before ingestion
    # https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint
    metricRelabelings: []
    # --ServiceMonitor will add labels from the service to the Prometheus metric
    # https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#servicemonitorspec
    targetLabels: []
    # -- ServiceMonitor will use http by default, but you can pick https as well
    scheme: http
    # -- ServiceMonitor will use these tlsConfig settings to make the health check requests
    tlsConfig: null
